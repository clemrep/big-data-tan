{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c15192",
   "metadata": {},
   "source": [
    "# 02 - Unified Streaming Pipeline\n",
    "\n",
    "**Single SparkSession** orchestrating all data layers:\n",
    "\n",
    "- **Bronze**: Raw Kafka → Delta (append-only log)\n",
    "- **Silver**: Cleaning, normalization, type coercion\n",
    "- **Silver_ML**: Feature engineering with rolling windows\n",
    "- **Gold**: Aggregations, KPIs, metrics\n",
    "\n",
    "Performance optimizations:\n",
    "- Micro-batch streaming (30s trigger)\n",
    "- Minimal shuffle operations\n",
    "- Type-hinting + production logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec48ae0",
   "metadata": {},
   "source": [
    "## Initialize MinIO Bucket Structure\n",
    "Run this cell first to create the S3 bucket with bronze/silver/gold directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5533a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Bronze: s3a://datalake/bronze/flights | Silver: s3a://datalake/silver/flights | Gold: s3a://datalake/gold/traffic_by_country\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration chargée depuis .env\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, from_json, from_unixtime, to_timestamp, round as spark_round,\n",
    "    lag, avg, stddev, row_number, when, sqrt, pow, lit,\n",
    "    window, sum as spark_sum, max as spark_max, min as spark_min,\n",
    "    count, broadcast\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, FloatType, IntegerType, BooleanType, LongType\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jovyan/work')\n",
    "\n",
    "from config import get_s3_path, create_spark_session\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "BRONZE_PATH: str = get_s3_path(\"bronze\", \"flights\")\n",
    "SILVER_PATH: str = get_s3_path(\"silver\", \"flights\")\n",
    "SILVER_ML_PATH: str = get_s3_path(\"silver\", \"flights_ml\")\n",
    "GOLD_TRAFFIC_PATH: str = get_s3_path(\"gold\", \"traffic_by_country\")\n",
    "GOLD_METRICS_PATH: str = get_s3_path(\"gold\", \"metrics_by_category\")\n",
    "\n",
    "CHECKPOINT_BRONZE: str = get_s3_path(\"checkpoints\", \"bronze\")\n",
    "CHECKPOINT_SILVER: str = get_s3_path(\"checkpoints\", \"silver\")\n",
    "CHECKPOINT_SILVER_ML: str = get_s3_path(\"checkpoints\", \"silver_ml\")\n",
    "CHECKPOINT_GOLD_TRAFFIC: str = get_s3_path(\"checkpoints\", \"gold_traffic\")\n",
    "CHECKPOINT_GOLD_METRICS: str = get_s3_path(\"checkpoints\", \"gold_metrics\")\n",
    "\n",
    "KAFKA_BOOTSTRAP: str = os.getenv(\"KAFKA_BOOTSTRAP\", \"kafka1:9092\")\n",
    "TOPIC_NAME: str = os.getenv(\"TOPIC_NAME\", \"opensky-data\")\n",
    "AIRPORTS_CSV: str = \"./data/airports.csv\"\n",
    "\n",
    "PROCESSING_TIME: str = \"120 seconds\"\n",
    "\n",
    "logger.info(f\"Bronze: {BRONZE_PATH} | Silver: {SILVER_PATH} | Gold: {GOLD_TRAFFIC_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "825f2253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.spark#spark-hadoop-cloud_2.12 added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4fc5fd4a-36d0-469c-a276-2ab57ecda2cf;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.spark#spark-hadoop-cloud_2.12;3.5.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound com.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.14 in central\n",
      "\tfound joda-time#joda-time;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.15.2 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.14 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.16 in central\n",
      "\tfound commons-codec#commons-codec;1.16.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.4 in central\n",
      "\tfound com.microsoft.azure#azure-storage;7.0.1 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.0.0 in central\n",
      "\tfound com.google.guava#guava;14.0.1 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-cloud-storage;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-aliyun;3.3.4 in central\n",
      "\tfound com.aliyun.oss#aliyun-sdk-oss;3.13.0 in central\n",
      "\tfound org.jdom#jdom2;2.0.6 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound stax#stax-api;1.0.1 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-core;4.5.10 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.ini4j#ini4j;0.5.4 in central\n",
      "\tfound io.opentracing#opentracing-api;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-util;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-noop;0.33.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-ram;3.1.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-kms;2.11.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure-datalake;3.3.4 in central\n",
      "\tfound com.microsoft.azure#azure-data-lake-store-sdk;2.3.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-openstack;3.3.4 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.54.v20240208 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.54.v20240208 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.3 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.3 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.4!hadoop-aws.jar (469ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.262!aws-java-sdk-bundle.jar (59518ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-hadoop-cloud_2.12/3.5.3/spark-hadoop-cloud_2.12-3.5.3.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-hadoop-cloud_2.12;3.5.3!spark-hadoop-cloud_2.12.jar (71ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.0.0/delta-spark_2.12-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.12;3.0.0!delta-spark_2.12.jar (2166ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.3/spark-sql-kafka-0-10_2.12-3.5.3.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.3!spark-sql-kafka-0-10_2.12.jar (289ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (276ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (13424ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.14/gcs-connector-hadoop3-2.2.14-shaded.jar ...\n",
      "\t[SUCCESSFUL ] com.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.14!gcs-connector.jar (20339ms)\n",
      "downloading https://repo1.maven.org/maven2/joda-time/joda-time/2.12.5/joda-time-2.12.5.jar ...\n",
      "\t[SUCCESSFUL ] joda-time#joda-time;2.12.5!joda-time.jar (296ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.15.2/jackson-databind-2.15.2.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-databind;2.15.2!jackson-databind.jar (743ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.15.2/jackson-annotations-2.15.2.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-annotations;2.15.2!jackson-annotations.jar (82ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.15.2/jackson-dataformat-cbor-2.15.2.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.15.2!jackson-dataformat-cbor.jar (88ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.14/httpclient-4.5.14.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpclient;4.5.14!httpclient.jar (390ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.16/httpcore-4.4.16.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpcore;4.4.16!httpcore.jar (185ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.4/hadoop-azure-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-azure;3.3.4!hadoop-azure.jar (378ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-cloud-storage/3.3.4/hadoop-cloud-storage-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-cloud-storage;3.3.4!hadoop-cloud-storage.jar (68ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.4.54.v20240208/jetty-util-9.4.54.v20240208.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-util;9.4.54.v20240208!jetty-util.jar (312ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util-ajax/9.4.54.v20240208/jetty-util-ajax-9.4.54.v20240208.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-util-ajax;9.4.54.v20240208!jetty-util-ajax.jar (83ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (8338ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.5/snappy-java-1.1.10.5.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.5!snappy-java.jar(bundle) (1030ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (85ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (98ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (100ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.15.2/jackson-core-2.15.2.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-core;2.15.2!jackson-core.jar (295ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-codec/commons-codec/1.16.1/commons-codec-1.16.1.jar ...\n",
      "\t[SUCCESSFUL ] commons-codec#commons-codec;1.16.1!commons-codec.jar (273ms)\n",
      "downloading https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/7.0.1/azure-storage-7.0.1.jar ...\n",
      "\t[SUCCESSFUL ] com.microsoft.azure#azure-storage;7.0.1!azure-storage.jar (405ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/thirdparty/hadoop-shaded-guava/1.1.1/hadoop-shaded-guava-1.1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1!hadoop-shaded-guava.jar (5359ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-mapper-asl;1.9.13!jackson-mapper-asl.jar (605ms)\n",
      "downloading https://repo1.maven.org/maven2/com/microsoft/azure/azure-keyvault-core/1.0.0/azure-keyvault-core-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.microsoft.azure#azure-keyvault-core;1.0.0!azure-keyvault-core.jar (62ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/guava/guava/14.0.1/guava-14.0.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.guava#guava;14.0.1!guava.jar(bundle) (1177ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-annotations/3.3.4/hadoop-annotations-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-annotations;3.3.4!hadoop-annotations.jar (69ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aliyun/3.3.4/hadoop-aliyun-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aliyun;3.3.4!hadoop-aliyun.jar (89ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure-datalake/3.3.4/hadoop-azure-datalake-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-azure-datalake;3.3.4!hadoop-azure-datalake.jar (74ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-openstack/3.3.4/hadoop-openstack-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-openstack;3.3.4!hadoop-openstack.jar (121ms)\n",
      "downloading https://repo1.maven.org/maven2/com/aliyun/oss/aliyun-sdk-oss/3.13.0/aliyun-sdk-oss-3.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.aliyun.oss#aliyun-sdk-oss;3.13.0!aliyun-sdk-oss.jar (370ms)\n",
      "downloading https://repo1.maven.org/maven2/org/jdom/jdom2/2.0.6/jdom2-2.0.6.jar ...\n",
      "\t[SUCCESSFUL ] org.jdom#jdom2;2.0.6!jdom2.jar (203ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jettison#jettison;1.1!jettison.jar(bundle) (80ms)\n",
      "downloading https://repo1.maven.org/maven2/com/aliyun/aliyun-java-sdk-core/4.5.10/aliyun-java-sdk-core-4.5.10.jar ...\n",
      "\t[SUCCESSFUL ] com.aliyun#aliyun-java-sdk-core;4.5.10!aliyun-java-sdk-core.jar (149ms)\n",
      "downloading https://repo1.maven.org/maven2/com/aliyun/aliyun-java-sdk-ram/3.1.0/aliyun-java-sdk-ram-3.1.0.jar ...\n",
      "\t[SUCCESSFUL ] com.aliyun#aliyun-java-sdk-ram;3.1.0!aliyun-java-sdk-ram.jar (143ms)\n",
      "downloading https://repo1.maven.org/maven2/com/aliyun/aliyun-java-sdk-kms/2.11.0/aliyun-java-sdk-kms-2.11.0.jar ...\n",
      "\t[SUCCESSFUL ] com.aliyun#aliyun-java-sdk-kms;2.11.0!aliyun-java-sdk-kms.jar (134ms)\n",
      "downloading https://repo1.maven.org/maven2/stax/stax-api/1.0.1/stax-api-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] stax#stax-api;1.0.1!stax-api.jar (66ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/gson/gson/2.8.9/gson-2.8.9.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.gson#gson;2.8.9!gson.jar (207ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar ...\n",
      "\t[SUCCESSFUL ] javax.xml.bind#jaxb-api;2.2.11!jaxb-api.jar (107ms)\n",
      "downloading https://repo1.maven.org/maven2/org/ini4j/ini4j/0.5.4/ini4j-0.5.4.jar ...\n",
      "\t[SUCCESSFUL ] org.ini4j#ini4j;0.5.4!ini4j.jar (103ms)\n",
      "downloading https://repo1.maven.org/maven2/io/opentracing/opentracing-api/0.33.0/opentracing-api-0.33.0.jar ...\n",
      "\t[SUCCESSFUL ] io.opentracing#opentracing-api;0.33.0!opentracing-api.jar (69ms)\n",
      "downloading https://repo1.maven.org/maven2/io/opentracing/opentracing-util/0.33.0/opentracing-util-0.33.0.jar ...\n",
      "\t[SUCCESSFUL ] io.opentracing#opentracing-util;0.33.0!opentracing-util.jar (60ms)\n",
      "downloading https://repo1.maven.org/maven2/io/opentracing/opentracing-noop/0.33.0/opentracing-noop-0.33.0.jar ...\n",
      "\t[SUCCESSFUL ] io.opentracing#opentracing-noop;0.33.0!opentracing-noop.jar (67ms)\n",
      "downloading https://repo1.maven.org/maven2/com/microsoft/azure/azure-data-lake-store-sdk/2.3.9/azure-data-lake-store-sdk-2.3.9.jar ...\n",
      "\t[SUCCESSFUL ] com.microsoft.azure#azure-data-lake-store-sdk;2.3.9!azure-data-lake-store-sdk.jar (110ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/3.0.0/delta-storage-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;3.0.0!delta-storage.jar (77ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (209ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.3/spark-token-provider-kafka-0-10_2.12-3.5.3.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.3!spark-token-provider-kafka-0-10_2.12.jar (83ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (2304ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (163ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (470ms)\n",
      ":: resolution report :: resolve 68780ms :: artifacts dl 122584ms\n",
      "\t:: modules in use:\n",
      "\tcom.aliyun#aliyun-java-sdk-core;4.5.10 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-kms;2.11.0 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-ram;3.1.0 from central in [default]\n",
      "\tcom.aliyun.oss#aliyun-sdk-oss;3.13.0 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.15.2 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.14 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.guava#guava;14.0.1 from central in [default]\n",
      "\tcom.microsoft.azure#azure-data-lake-store-sdk;2.3.9 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.16.1 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\tio.opentracing#opentracing-api;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-noop;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-util;0.33.0 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tjoda-time#joda-time;2.12.5 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aliyun;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure-datalake;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-cloud-storage;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-openstack;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.14 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.16 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-hadoop-cloud_2.12;3.5.3 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.3 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.3 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.54.v20240208 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.54.v20240208 from central in [default]\n",
      "\torg.ini4j#ini4j;0.5.4 from central in [default]\n",
      "\torg.jdom#jdom2;2.0.6 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\tstax#stax-api;1.0.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 by [org.eclipse.jetty#jetty-util-ajax;9.4.54.v20240208] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   55  |   54  |   54  |   1   ||   54  |   54  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4fc5fd4a-36d0-469c-a276-2ab57ecda2cf\n",
      "\tconfs: [default]\n",
      "\t54 artifacts copied, 0 already retrieved (390169kB/874ms)\n",
      "26/01/23 18:12:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "INFO:__main__:Spark session initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Session 'UnifiedPipeline' configurée\n"
     ]
    }
   ],
   "source": [
    "spark: SparkSession = create_spark_session(\n",
    "    \"UnifiedPipeline\",\n",
    "    extra_packages=[\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\"],\n",
    "    shuffle_partitions=6\n",
    ")\n",
    "\n",
    "logger.info(\"Spark session initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec929e",
   "metadata": {},
   "source": [
    "## Bronze Stream - Kafka to Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a8c466c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 18:12:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "26/01/23 18:12:56 WARN StreamingQueryManager: Stopping existing streaming query [id=5de058b5-78d5-4071-b99e-c51668b4ed50, runId=aa53aab8-4d29-49c2-875d-817a3c62e370], as a new run is being started.\n",
      "INFO:__main__:Bronze stream started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 18:12:56 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "schema_bronze: StructType = StructType([\n",
    "    StructField(\"time\", LongType(), True),\n",
    "    StructField(\"icao24\", StringType(), True),\n",
    "    StructField(\"callsign\", StringType(), True),\n",
    "    StructField(\"origin_country\", StringType(), True),\n",
    "    StructField(\"time_position\", LongType(), True),\n",
    "    StructField(\"last_contact\", LongType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"baro_altitude\", FloatType(), True),\n",
    "    StructField(\"on_ground\", BooleanType(), True),\n",
    "    StructField(\"velocity\", FloatType(), True),\n",
    "    StructField(\"true_track\", FloatType(), True),\n",
    "    StructField(\"vertical_rate\", FloatType(), True),\n",
    "    StructField(\"geo_altitude\", FloatType(), True),\n",
    "    StructField(\"squawk\", StringType(), True),\n",
    "    StructField(\"spi\", BooleanType(), True),\n",
    "    StructField(\"position_source\", IntegerType(), True),\n",
    "    StructField(\"category\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "kafka_stream: DataFrame = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP) \\\n",
    "    .option(\"subscribe\", TOPIC_NAME) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "df_bronze: DataFrame = kafka_stream.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema_bronze).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "query_bronze = df_bronze.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime=PROCESSING_TIME) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_BRONZE) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(BRONZE_PATH)\n",
    "\n",
    "logger.info(\"Bronze stream started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be87dd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 18:13:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "INFO:__main__:Silver stream started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 18:13:04 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_silver: DataFrame = spark.readStream.format(\"delta\").load(BRONZE_PATH) \\\n",
    "    .filter(col(\"icao24\").isNotNull()) \\\n",
    "    .filter(col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull()) \\\n",
    "    .withColumn(\"event_timestamp\", to_timestamp(from_unixtime(col(\"time\")))) \\\n",
    "    .withColumn(\"velocity_kmh\", spark_round(col(\"velocity\") * 3.6, 2)) \\\n",
    "    .withColumn(\"altitude_meters\", col(\"baro_altitude\")) \\\n",
    "    .select(\n",
    "        \"event_timestamp\", \"icao24\", \"callsign\", \"origin_country\",\n",
    "        \"longitude\", \"latitude\", \"velocity_kmh\", \"altitude_meters\",\n",
    "        \"on_ground\", \"category\"\n",
    "    )\n",
    "\n",
    "query_silver = df_silver.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime=PROCESSING_TIME) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_SILVER) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(SILVER_PATH)\n",
    "\n",
    "logger.info(\"Silver stream started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1ed658",
   "metadata": {},
   "source": [
    "## Airports Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53a40d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 5211 airports\n"
     ]
    }
   ],
   "source": [
    "df_airports: DataFrame = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(AIRPORTS_CSV) \\\n",
    "    .select(\n",
    "        col(\"ident\").alias(\"airport_icao\"),\n",
    "        col(\"name\").alias(\"airport_name\"),\n",
    "        col(\"iso_country\").alias(\"airport_country\"),\n",
    "        col(\"latitude_deg\").cast(\"double\").alias(\"airport_lat\"),\n",
    "        col(\"longitude_deg\").cast(\"double\").alias(\"airport_lon\")\n",
    "    ) \\\n",
    "    .filter(col(\"type\").isin(\"large_airport\", \"medium_airport\"))\n",
    "\n",
    "logger.info(f\"Loaded {df_airports.count()} airports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82616bde",
   "metadata": {},
   "source": [
    "## Silver_ML Stream - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "717e8877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.java_gateway:Callback Server Starting\n",
      "INFO:py4j.java_gateway:Socket listening on ('127.0.0.1', 37581)\n",
      "26/01/23 17:51:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "INFO:__main__:Silver_ML stream started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Python Server ready to receive messages\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "26/01/23 17:51:18 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n"
     ]
    }
   ],
   "source": [
    "def process_ml_batch(batch_df: DataFrame, batch_id: int) -> None:\n",
    "    \"\"\"Micro-batch feature engineering for ML layer.\"\"\"\n",
    "    \n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "    \n",
    "    df_base = batch_df \\\n",
    "        .filter(col(\"icao24\").isNotNull()) \\\n",
    "        .filter(col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull()) \\\n",
    "        .withColumn(\"event_timestamp\", to_timestamp(from_unixtime(col(\"time\")))) \\\n",
    "        .withColumn(\"velocity_kmh\", spark_round(col(\"velocity\") * 3.6, 2)) \\\n",
    "        .withColumn(\"altitude_meters\", col(\"baro_altitude\"))\n",
    "    \n",
    "    df_clean = df_base \\\n",
    "        .filter(col(\"altitude_meters\").between(-500, 15000)) \\\n",
    "        .filter(col(\"velocity_kmh\").between(0, 1200))\n",
    "    \n",
    "    if df_clean.isEmpty():\n",
    "        return\n",
    "    \n",
    "    w_aircraft = Window.partitionBy(\"icao24\").orderBy(\"event_timestamp\")\n",
    "    \n",
    "    df_temporal = df_clean \\\n",
    "        .withColumn(\"prev_altitude\", lag(\"altitude_meters\", 1).over(w_aircraft)) \\\n",
    "        .withColumn(\"prev_velocity\", lag(\"velocity_kmh\", 1).over(w_aircraft)) \\\n",
    "        .withColumn(\"altitude_change\", col(\"altitude_meters\") - col(\"prev_altitude\")) \\\n",
    "        .withColumn(\"velocity_change\", col(\"velocity_kmh\") - col(\"prev_velocity\")) \\\n",
    "        .withColumn(\"observation_rank\", row_number().over(w_aircraft))\n",
    "    \n",
    "    df_on_ground = df_temporal.filter(col(\"on_ground\") == True)\n",
    "    df_in_flight = df_temporal.filter(col(\"on_ground\") == False)\n",
    "    \n",
    "    if df_on_ground.count() > 0:\n",
    "        df_with_dist = df_on_ground.crossJoin(broadcast(df_airports)) \\\n",
    "            .withColumn(\n",
    "                \"dist\",\n",
    "                sqrt(pow(col(\"latitude\") - col(\"airport_lat\"), 2) +\n",
    "                     pow(col(\"longitude\") - col(\"airport_lon\"), 2))\n",
    "            )\n",
    "        \n",
    "        w_dist = Window.partitionBy(\"icao24\", \"event_timestamp\")\n",
    "        df_closest = df_with_dist.withColumn(\"min_dist\", spark_min(\"dist\").over(w_dist)) \\\n",
    "            .filter(col(\"dist\") == col(\"min_dist\")) \\\n",
    "            .drop(\"dist\", \"min_dist\", \"airport_lat\", \"airport_lon\")\n",
    "        \n",
    "        df_enriched = df_closest.unionByName(\n",
    "            df_in_flight.withColumn(\"airport_icao\", lit(None))\n",
    "                        .withColumn(\"airport_name\", lit(None))\n",
    "                        .withColumn(\"airport_country\", lit(None)),\n",
    "            allowMissingColumns=True\n",
    "        )\n",
    "    else:\n",
    "        df_enriched = df_in_flight \\\n",
    "            .withColumn(\"airport_icao\", lit(None)) \\\n",
    "            .withColumn(\"airport_name\", lit(None)) \\\n",
    "            .withColumn(\"airport_country\", lit(None))\n",
    "    \n",
    "    w_rolling = Window.partitionBy(\"icao24\").orderBy(\"event_timestamp\").rowsBetween(-5, 0)\n",
    "    \n",
    "    df_rolling = df_enriched \\\n",
    "        .withColumn(\"rolling_avg_altitude\", avg(\"altitude_meters\").over(w_rolling)) \\\n",
    "        .withColumn(\"rolling_std_altitude\", stddev(\"altitude_meters\").over(w_rolling)) \\\n",
    "        .withColumn(\"rolling_avg_velocity\", avg(\"velocity_kmh\").over(w_rolling))\n",
    "    \n",
    "    df_ml = df_rolling.withColumn(\n",
    "        \"flight_phase\",\n",
    "        when(col(\"on_ground\") == True, \"GROUND\")\n",
    "            .when((col(\"altitude_change\") > 50) & (col(\"altitude_meters\") < 3000), \"TAKEOFF\")\n",
    "            .when(col(\"altitude_change\") > 20, \"CLIMB\")\n",
    "            .when(col(\"altitude_change\").between(-20, 20) & (col(\"altitude_meters\") > 8000), \"CRUISE\")\n",
    "            .when(col(\"altitude_change\") < -20, \"DESCENT\")\n",
    "            .otherwise(\"TRANSITION\")\n",
    "    )\n",
    "    \n",
    "    df_final = df_ml.select(\n",
    "        \"event_timestamp\", \"icao24\", \"callsign\", \"origin_country\",\n",
    "        \"longitude\", \"latitude\", \"velocity_kmh\", \"altitude_meters\",\n",
    "        \"on_ground\", \"category\",\n",
    "        \"prev_altitude\", \"prev_velocity\", \"altitude_change\", \"velocity_change\",\n",
    "        \"observation_rank\",\n",
    "        \"airport_icao\", \"airport_name\", \"airport_country\",\n",
    "        \"rolling_avg_altitude\", \"rolling_std_altitude\", \"rolling_avg_velocity\",\n",
    "        \"flight_phase\"\n",
    "    )\n",
    "    \n",
    "    df_final.write.format(\"delta\").mode(\"append\").save(SILVER_ML_PATH)\n",
    "\n",
    "\n",
    "df_bronze_ml = spark.readStream.format(\"delta\").load(BRONZE_PATH)\n",
    "\n",
    "query_silver_ml = df_bronze_ml.writeStream \\\n",
    "    .foreachBatch(process_ml_batch) \\\n",
    "    .trigger(processingTime=PROCESSING_TIME) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_SILVER_ML) \\\n",
    "    .start()\n",
    "\n",
    "logger.info(\"Silver_ML stream started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735ce319",
   "metadata": {},
   "source": [
    "## Gold Streams - Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f318d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 18:13:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "INFO:__main__:Gold (Traffic) stream started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 18:13:14 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/23 18:13:15 WARN HDFSBackedStateStoreProvider: The state for version 21 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 18:13:15 WARN HDFSBackedStateStoreProvider: The state for version 21 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 18:13:15 WARN HDFSBackedStateStoreProvider: The state for version 21 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 18:13:15 WARN HDFSBackedStateStoreProvider: The state for version 21 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 18:13:15 WARN HDFSBackedStateStoreProvider: The state for version 21 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 18:13:15 WARN HDFSBackedStateStoreProvider: The state for version 21 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    }
   ],
   "source": [
    "df_gold_traffic = spark.readStream.format(\"delta\").load(SILVER_PATH) \\\n",
    "    .filter(col(\"origin_country\").isNotNull()) \\\n",
    "    .withColumn(\"window\", window(col(\"event_timestamp\"), \"5 minutes\")) \\\n",
    "    .groupBy(\"window\", \"origin_country\") \\\n",
    "    .agg(\n",
    "        count(\"icao24\").alias(\"aircraft_count\"),\n",
    "        spark_round(avg(\"velocity_kmh\"), 2).alias(\"avg_velocity_kmh\"),\n",
    "        spark_round(avg(\"altitude_meters\"), 0).alias(\"avg_altitude_m\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window\").alias(\"time_window\"), \"origin_country\", \"aircraft_count\",\n",
    "        \"avg_velocity_kmh\", \"avg_altitude_m\"\n",
    "    )\n",
    "\n",
    "query_gold_traffic = df_gold_traffic.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(processingTime=PROCESSING_TIME) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_GOLD_TRAFFIC) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(GOLD_TRAFFIC_PATH)\n",
    "\n",
    "logger.info(\"Gold (Traffic) stream started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93ecfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 18:13:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "INFO:__main__:Gold (Metrics) stream started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 18:16:00 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/23 18:16:00 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_gold_metrics = spark.readStream.format(\"delta\").load(SILVER_ML_PATH) \\\n",
    "    .filter(col(\"category\").isNotNull()) \\\n",
    "    .withColumn(\"window\", window(col(\"event_timestamp\"), \"5 minutes\")) \\\n",
    "    .groupBy(\"window\", \"category\", \"flight_phase\") \\\n",
    "    .agg(\n",
    "        count(\"icao24\").alias(\"aircraft_count\"),\n",
    "        spark_round(avg(\"velocity_kmh\"), 2).alias(\"avg_velocity_kmh\"),\n",
    "        spark_round(avg(\"altitude_meters\"), 0).alias(\"avg_altitude_m\"),\n",
    "        spark_round(avg(\"rolling_avg_altitude\"), 0).alias(\"rolling_altitude_m\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"window\", \"category\", \"flight_phase\", \"aircraft_count\",\n",
    "        \"avg_velocity_kmh\", \"avg_altitude_m\", \"rolling_altitude_m\"\n",
    "    )\n",
    "\n",
    "query_gold_metrics = df_gold_metrics.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(processingTime=PROCESSING_TIME) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_GOLD_METRICS) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(GOLD_METRICS_PATH)\n",
    "\n",
    "logger.info(\"Gold (Metrics) stream started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a2a7a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Unified Pipeline Started\n",
      "INFO:__main__:Time: 17:46:37 UTC\n",
      "INFO:__main__:Active Streams: Bronze, Silver, Silver_ML, Gold_Traffic, Gold_Metrics\n",
      "INFO:__main__:Processing Time: 30 seconds | Spark UI: http://localhost:4040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "logger.info(\"Unified Pipeline Started\")\n",
    "logger.info(f\"Time: {datetime.now().strftime('%H:%M:%S UTC')}\")\n",
    "logger.info(\"Active Streams: Bronze, Silver, Silver_ML, Gold_Traffic, Gold_Metrics\")\n",
    "logger.info(\"Processing Time: 30 seconds | Spark UI: http://localhost:4040\")\n",
    "print()\n",
    "\n",
    "# Monitoring loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf6371",
   "metadata": {},
   "source": [
    "## Pipeline Status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce26ba2d",
   "metadata": {},
   "source": [
    "## Shutdown Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0493a328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Shutting down pipeline\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'query_silver_ml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m     spark.stop()\n\u001b[32m     21\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mSpark session stopped\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mshutdown_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mshutdown_all\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Graceful shutdown of all streams and Spark session.\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mShutting down pipeline\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m streams = [\n\u001b[32m      6\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mBronze\u001b[39m\u001b[33m\"\u001b[39m, query_bronze),\n\u001b[32m      7\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mSilver\u001b[39m\u001b[33m\"\u001b[39m, query_silver),\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mSilver_ML\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mquery_silver_ml\u001b[49m),\n\u001b[32m      9\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mGold_Traffic\u001b[39m\u001b[33m\"\u001b[39m, query_gold_traffic),\n\u001b[32m     10\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mGold_Metrics\u001b[39m\u001b[33m\"\u001b[39m, query_gold_metrics)\n\u001b[32m     11\u001b[39m ]\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, stream \u001b[38;5;129;01min\u001b[39;00m streams:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'query_silver_ml' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def shutdown_all() -> None:\n",
    "    \"\"\"Graceful shutdown of all streams and Spark session.\"\"\"\n",
    "    logger.info(\"Shutting down pipeline\")\n",
    "    \n",
    "    streams = [\n",
    "        (\"Bronze\", query_bronze),\n",
    "        (\"Silver\", query_silver),\n",
    "        (\"Silver_ML\", query_silver_ml),\n",
    "        (\"Gold_Traffic\", query_gold_traffic),\n",
    "        (\"Gold_Metrics\", query_gold_metrics)\n",
    "    ]\n",
    "    \n",
    "    for name, stream in streams:\n",
    "        try:\n",
    "            stream.stop()\n",
    "            logger.info(f\"{name} stopped\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error stopping {name}: {str(e)}\")\n",
    "    \n",
    "    spark.stop()\n",
    "    logger.info(\"Spark session stopped\")\n",
    "\n",
    "shutdown_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
