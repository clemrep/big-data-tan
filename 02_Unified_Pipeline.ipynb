{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c15192",
   "metadata": {},
   "source": [
    "# 02 - Unified Streaming Pipeline\n",
    "\n",
    "**Single SparkSession** orchestrating all data layers:\n",
    "\n",
    "- **Bronze**: Raw Kafka → Delta (append-only log)\n",
    "- **Silver**: Cleaning, normalization, type coercion\n",
    "- **Silver_ML**: Feature engineering with rolling windows\n",
    "- **Gold**: Aggregations, KPIs, metrics\n",
    "\n",
    "Performance optimizations:\n",
    "- Micro-batch streaming (30s trigger)\n",
    "- Minimal shuffle operations\n",
    "- Type-hinting + production logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5533a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Bronze: s3a://datalake/bronze/flights | Silver: s3a://datalake/silver/flights | Gold: s3a://datalake/gold/traffic_by_country\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration chargée depuis .env\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, from_json, from_unixtime, to_timestamp, round as spark_round,\n",
    "    lag, avg, stddev, row_number, when, sqrt, pow, lit,\n",
    "    window, sum as spark_sum, max as spark_max, min as spark_min,\n",
    "    count, broadcast\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, FloatType, IntegerType, BooleanType, LongType\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jovyan/work')\n",
    "\n",
    "from config import get_s3_path, create_spark_session\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "BRONZE_PATH: str = get_s3_path(\"bronze\", \"flights\")\n",
    "SILVER_PATH: str = get_s3_path(\"silver\", \"flights\")\n",
    "SILVER_ML_PATH: str = get_s3_path(\"silver\", \"flights_ml\")\n",
    "GOLD_TRAFFIC_PATH: str = get_s3_path(\"gold\", \"traffic_by_country\")\n",
    "GOLD_METRICS_PATH: str = get_s3_path(\"gold\", \"metrics_by_category\")\n",
    "\n",
    "CHECKPOINT_BRONZE: str = get_s3_path(\"checkpoints\", \"bronze\")\n",
    "CHECKPOINT_SILVER: str = get_s3_path(\"checkpoints\", \"silver\")\n",
    "CHECKPOINT_SILVER_ML: str = get_s3_path(\"checkpoints\", \"silver_ml\")\n",
    "CHECKPOINT_GOLD_TRAFFIC: str = get_s3_path(\"checkpoints\", \"gold_traffic\")\n",
    "CHECKPOINT_GOLD_METRICS: str = get_s3_path(\"checkpoints\", \"gold_metrics\")\n",
    "\n",
    "KAFKA_BOOTSTRAP: str = os.getenv(\"KAFKA_BOOTSTRAP\", \"kafka1:9092\")\n",
    "TOPIC_NAME: str = os.getenv(\"TOPIC_NAME\", \"opensky-data\")\n",
    "AIRPORTS_CSV: str = \"./data/airports.csv\"\n",
    "\n",
    "PROCESSING_TIME: str = \"120 seconds\"\n",
    "\n",
    "logger.info(f\"Bronze: {BRONZE_PATH} | Silver: {SILVER_PATH} | Gold: {GOLD_TRAFFIC_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "825f2253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Spark session initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Session 'UnifiedPipeline' configurée\n"
     ]
    }
   ],
   "source": [
    "spark: SparkSession = create_spark_session(\n",
    "    \"UnifiedPipeline\",\n",
    "    extra_packages=[\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\"],\n",
    "    shuffle_partitions=6\n",
    ")\n",
    "\n",
    "logger.info(\"Spark session initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec929e",
   "metadata": {},
   "source": [
    "## Bronze Stream - Kafka to Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c466c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 20:24:04 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "26/01/23 20:24:04 WARN StreamingQueryManager: Stopping existing streaming query [id=5de058b5-78d5-4071-b99e-c51668b4ed50, runId=3e4c3d84-fb09-49e5-bd7a-ed5a12cda30a], as a new run is being started.\n",
      "INFO:__main__:Bronze stream started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 20:24:04 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "26/01/23 20:24:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "schema_bronze: StructType = StructType([\n",
    "    StructField(\"time\", LongType(), True),\n",
    "    StructField(\"icao24\", StringType(), True),\n",
    "    StructField(\"callsign\", StringType(), True),\n",
    "    StructField(\"origin_country\", StringType(), True),\n",
    "    StructField(\"time_position\", LongType(), True),\n",
    "    StructField(\"last_contact\", LongType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"baro_altitude\", FloatType(), True),\n",
    "    StructField(\"on_ground\", BooleanType(), True),\n",
    "    StructField(\"velocity\", FloatType(), True),\n",
    "    StructField(\"true_track\", FloatType(), True),\n",
    "    StructField(\"vertical_rate\", FloatType(), True),\n",
    "    StructField(\"geo_altitude\", FloatType(), True),\n",
    "    StructField(\"squawk\", StringType(), True),\n",
    "    StructField(\"spi\", BooleanType(), True),\n",
    "    StructField(\"position_source\", IntegerType(), True),\n",
    "    StructField(\"category\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "kafka_stream: DataFrame = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP) \\\n",
    "    .option(\"subscribe\", TOPIC_NAME) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "df_bronze: DataFrame = kafka_stream.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema_bronze).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "query_bronze = df_bronze.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime=PROCESSING_TIME) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_BRONZE) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(BRONZE_PATH)\n",
    "\n",
    "logger.info(\"Bronze stream started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be87dd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 20:24:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "26/01/23 20:24:08 WARN StreamingQueryManager: Stopping existing streaming query [id=fb8e3cba-f4b7-401f-bbcf-060035ff7f36, runId=c6c67ff9-ce00-41ed-ae1e-6aac57cafd3a], as a new run is being started.\n",
      "INFO:__main__:Silver stream started\n"
     ]
    }
   ],
   "source": [
    "df_silver: DataFrame = spark.readStream.format(\"delta\").load(BRONZE_PATH) \\\n",
    "    .filter(col(\"icao24\").isNotNull()) \\\n",
    "    .filter(col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull()) \\\n",
    "    .withColumn(\"event_timestamp\", to_timestamp(from_unixtime(col(\"time\")))) \\\n",
    "    .withColumn(\"velocity_kmh\", spark_round(col(\"velocity\") * 3.6, 2)) \\\n",
    "    .withColumn(\"altitude_meters\", col(\"baro_altitude\")) \\\n",
    "    .select(\n",
    "        \"event_timestamp\", \"icao24\", \"callsign\", \"origin_country\",\n",
    "        \"longitude\", \"latitude\", \"velocity_kmh\", \"altitude_meters\",\n",
    "        \"on_ground\", \"category\"\n",
    "    )\n",
    "\n",
    "query_silver = df_silver.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime=PROCESSING_TIME) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_SILVER) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(SILVER_PATH)\n",
    "\n",
    "logger.info(\"Silver stream started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1ed658",
   "metadata": {},
   "source": [
    "## Airports Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53a40d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 5211 airports\n"
     ]
    }
   ],
   "source": [
    "df_airports: DataFrame = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(AIRPORTS_CSV) \\\n",
    "    .select(\n",
    "        col(\"ident\").alias(\"airport_icao\"),\n",
    "        col(\"name\").alias(\"airport_name\"),\n",
    "        col(\"iso_country\").alias(\"airport_country\"),\n",
    "        col(\"latitude_deg\").cast(\"double\").alias(\"airport_lat\"),\n",
    "        col(\"longitude_deg\").cast(\"double\").alias(\"airport_lon\")\n",
    "    ) \\\n",
    "    .filter(col(\"type\").isin(\"large_airport\", \"medium_airport\"))\n",
    "\n",
    "logger.info(f\"Loaded {df_airports.count()} airports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82616bde",
   "metadata": {},
   "source": [
    "## Silver_ML Stream - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "717e8877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 20:24:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "26/01/23 20:24:12 WARN StreamingQueryManager: Stopping existing streaming query [id=f29f4bbb-96a5-4979-9c67-8e2cbd8eeec8, runId=ec693f22-1502-44c0-b262-24c85667063d], as a new run is being started.\n",
      "INFO:__main__:Silver_ML stream started\n"
     ]
    }
   ],
   "source": [
    "def process_ml_batch(batch_df: DataFrame, batch_id: int) -> None:\n",
    "    \"\"\"Micro-batch feature engineering for ML layer.\"\"\"\n",
    "    \n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "    \n",
    "    df_base = batch_df \\\n",
    "        .filter(col(\"icao24\").isNotNull()) \\\n",
    "        .filter(col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull()) \\\n",
    "        .withColumn(\"event_timestamp\", to_timestamp(from_unixtime(col(\"time\")))) \\\n",
    "        .withColumn(\"velocity_kmh\", spark_round(col(\"velocity\") * 3.6, 2)) \\\n",
    "        .withColumn(\"altitude_meters\", col(\"baro_altitude\"))\n",
    "    \n",
    "    df_clean = df_base \\\n",
    "        .filter(col(\"altitude_meters\").between(-500, 15000)) \\\n",
    "        .filter(col(\"velocity_kmh\").between(0, 1200))\n",
    "    \n",
    "    if df_clean.isEmpty():\n",
    "        return\n",
    "    \n",
    "    w_aircraft = Window.partitionBy(\"icao24\").orderBy(\"event_timestamp\")\n",
    "    \n",
    "    df_temporal = df_clean \\\n",
    "        .withColumn(\"prev_altitude\", lag(\"altitude_meters\", 1).over(w_aircraft)) \\\n",
    "        .withColumn(\"prev_velocity\", lag(\"velocity_kmh\", 1).over(w_aircraft)) \\\n",
    "        .withColumn(\"altitude_change\", col(\"altitude_meters\") - col(\"prev_altitude\")) \\\n",
    "        .withColumn(\"velocity_change\", col(\"velocity_kmh\") - col(\"prev_velocity\")) \\\n",
    "        .withColumn(\"observation_rank\", row_number().over(w_aircraft))\n",
    "    \n",
    "    df_on_ground = df_temporal.filter(col(\"on_ground\") == True)\n",
    "    df_in_flight = df_temporal.filter(col(\"on_ground\") == False)\n",
    "    \n",
    "    if df_on_ground.count() > 0:\n",
    "        df_with_dist = df_on_ground.crossJoin(broadcast(df_airports)) \\\n",
    "            .withColumn(\n",
    "                \"dist\",\n",
    "                sqrt(pow(col(\"latitude\") - col(\"airport_lat\"), 2) +\n",
    "                     pow(col(\"longitude\") - col(\"airport_lon\"), 2))\n",
    "            )\n",
    "        \n",
    "        w_dist = Window.partitionBy(\"icao24\", \"event_timestamp\")\n",
    "        df_closest = df_with_dist.withColumn(\"min_dist\", spark_min(\"dist\").over(w_dist)) \\\n",
    "            .filter(col(\"dist\") == col(\"min_dist\")) \\\n",
    "            .drop(\"dist\", \"min_dist\", \"airport_lat\", \"airport_lon\")\n",
    "        \n",
    "        df_enriched = df_closest.unionByName(\n",
    "            df_in_flight.withColumn(\"airport_icao\", lit(None))\n",
    "                        .withColumn(\"airport_name\", lit(None))\n",
    "                        .withColumn(\"airport_country\", lit(None)),\n",
    "            allowMissingColumns=True\n",
    "        )\n",
    "    else:\n",
    "        df_enriched = df_in_flight \\\n",
    "            .withColumn(\"airport_icao\", lit(None)) \\\n",
    "            .withColumn(\"airport_name\", lit(None)) \\\n",
    "            .withColumn(\"airport_country\", lit(None))\n",
    "    \n",
    "    w_rolling = Window.partitionBy(\"icao24\").orderBy(\"event_timestamp\").rowsBetween(-5, 0)\n",
    "    \n",
    "    df_rolling = df_enriched \\\n",
    "        .withColumn(\"rolling_avg_altitude\", avg(\"altitude_meters\").over(w_rolling)) \\\n",
    "        .withColumn(\"rolling_std_altitude\", stddev(\"altitude_meters\").over(w_rolling)) \\\n",
    "        .withColumn(\"rolling_avg_velocity\", avg(\"velocity_kmh\").over(w_rolling))\n",
    "    \n",
    "    df_ml = df_rolling.withColumn(\n",
    "        \"flight_phase\",\n",
    "        when(col(\"on_ground\") == True, \"GROUND\")\n",
    "            .when((col(\"altitude_change\") > 50) & (col(\"altitude_meters\") < 3000), \"TAKEOFF\")\n",
    "            .when(col(\"altitude_change\") > 20, \"CLIMB\")\n",
    "            .when(col(\"altitude_change\").between(-20, 20) & (col(\"altitude_meters\") > 8000), \"CRUISE\")\n",
    "            .when(col(\"altitude_change\") < -20, \"DESCENT\")\n",
    "            .otherwise(\"TRANSITION\")\n",
    "    )\n",
    "    \n",
    "    df_final = df_ml.select(\n",
    "        \"event_timestamp\", \"icao24\", \"callsign\", \"origin_country\",\n",
    "        \"longitude\", \"latitude\", \"velocity_kmh\", \"altitude_meters\",\n",
    "        \"on_ground\", \"category\",\n",
    "        \"prev_altitude\", \"prev_velocity\", \"altitude_change\", \"velocity_change\",\n",
    "        \"observation_rank\",\n",
    "        \"airport_icao\", \"airport_name\", \"airport_country\",\n",
    "        \"rolling_avg_altitude\", \"rolling_std_altitude\", \"rolling_avg_velocity\",\n",
    "        \"flight_phase\"\n",
    "    )\n",
    "    \n",
    "    df_final.write.format(\"delta\").mode(\"append\").save(SILVER_ML_PATH)\n",
    "\n",
    "\n",
    "df_bronze_ml = spark.readStream.format(\"delta\").load(BRONZE_PATH)\n",
    "\n",
    "query_silver_ml = df_bronze_ml.writeStream \\\n",
    "    .foreachBatch(process_ml_batch) \\\n",
    "    .trigger(processingTime=PROCESSING_TIME) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_SILVER_ML) \\\n",
    "    .start()\n",
    "\n",
    "logger.info(\"Silver_ML stream started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735ce319",
   "metadata": {},
   "source": [
    "## Gold Streams - Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c69f318d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 20:24:15 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "26/01/23 20:24:15 WARN StreamingQueryManager: Stopping existing streaming query [id=49f34648-087e-4710-a6a8-d5e90d5f62dd, runId=68ceddd5-1678-48eb-abc5-f893d6be5cc5], as a new run is being started.\n",
      "INFO:__main__:Gold (Traffic) stream started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 20:24:16 WARN HDFSBackedStateStoreProvider: The state for version 49 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 20:24:16 WARN HDFSBackedStateStoreProvider: The state for version 49 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    }
   ],
   "source": [
    "df_gold_traffic = spark.readStream.format(\"delta\").load(SILVER_PATH) \\\n",
    "    .filter(col(\"origin_country\").isNotNull()) \\\n",
    "    .withColumn(\"window\", window(col(\"event_timestamp\"), \"5 minutes\")) \\\n",
    "    .groupBy(\"window\", \"origin_country\") \\\n",
    "    .agg(\n",
    "        count(\"icao24\").alias(\"aircraft_count\"),\n",
    "        spark_round(avg(\"velocity_kmh\"), 2).alias(\"avg_velocity_kmh\"),\n",
    "        spark_round(avg(\"altitude_meters\"), 0).alias(\"avg_altitude_m\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window\").alias(\"time_window\"), \"origin_country\", \"aircraft_count\",\n",
    "        \"avg_velocity_kmh\", \"avg_altitude_m\"\n",
    "    )\n",
    "\n",
    "query_gold_traffic = df_gold_traffic.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(processingTime=PROCESSING_TIME) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_GOLD_TRAFFIC) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(GOLD_TRAFFIC_PATH)\n",
    "\n",
    "logger.info(\"Gold (Traffic) stream started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93ecfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 20:24:16 WARN HDFSBackedStateStoreProvider: The state for version 49 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 20:24:16 WARN HDFSBackedStateStoreProvider: The state for version 49 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 20:24:16 WARN HDFSBackedStateStoreProvider: The state for version 49 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 20:24:16 WARN HDFSBackedStateStoreProvider: The state for version 49 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 20:24:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "26/01/23 20:24:17 WARN StreamingQueryManager: Stopping existing streaming query [id=96b42017-ab15-4a5c-8a24-61ff93d3ffa3, runId=dfe5f9c1-d0bb-4824-be0f-80b74af85988], as a new run is being started.\n",
      "INFO:__main__:Gold (Metrics) stream started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 20:24:18 WARN HDFSBackedStateStoreProvider: The state for version 24 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 20:24:18 WARN HDFSBackedStateStoreProvider: The state for version 24 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 20:24:18 WARN HDFSBackedStateStoreProvider: The state for version 24 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 20:24:18 WARN HDFSBackedStateStoreProvider: The state for version 24 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 20:24:18 WARN HDFSBackedStateStoreProvider: The state for version 24 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 20:24:18 WARN HDFSBackedStateStoreProvider: The state for version 24 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 20:30:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/23 20:32:00 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/23 20:34:00 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/23 20:36:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/23 20:38:00 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/23 20:42:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/23 20:42:03 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/23 20:46:00 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/23 20:50:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/23 20:52:00 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/23 20:52:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/23 20:54:00 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/23 20:56:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/23 20:58:00 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/23 21:00:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/23 21:02:04 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/23 21:04:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/23 21:06:00 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/23 21:10:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/23 21:12:00 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/23 21:14:00 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/23 21:16:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_gold_metrics = spark.readStream.format(\"delta\").load(SILVER_ML_PATH) \\\n",
    "    .filter(col(\"category\").isNotNull()) \\\n",
    "    .withColumn(\"window\", window(col(\"event_timestamp\"), \"5 minutes\")) \\\n",
    "    .groupBy(\"window\", \"category\", \"flight_phase\") \\\n",
    "    .agg(\n",
    "        count(\"icao24\").alias(\"aircraft_count\"),\n",
    "        spark_round(avg(\"velocity_kmh\"), 2).alias(\"avg_velocity_kmh\"),\n",
    "        spark_round(avg(\"altitude_meters\"), 0).alias(\"avg_altitude_m\"),\n",
    "        spark_round(avg(\"rolling_avg_altitude\"), 0).alias(\"rolling_altitude_m\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"window\", \"category\", \"flight_phase\", \"aircraft_count\",\n",
    "        \"avg_velocity_kmh\", \"avg_altitude_m\", \"rolling_altitude_m\"\n",
    "    )\n",
    "\n",
    "query_gold_metrics = df_gold_metrics.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(processingTime=PROCESSING_TIME) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_GOLD_METRICS) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(GOLD_METRICS_PATH)\n",
    "\n",
    "logger.info(\"Gold (Metrics) stream started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "logger.info(\"Unified Pipeline Started\")\n",
    "logger.info(f\"Time: {datetime.now().strftime('%H:%M:%S UTC')}\")\n",
    "logger.info(\"Active Streams: Bronze, Silver, Silver_ML, Gold_Traffic, Gold_Metrics\")\n",
    "logger.info(\"Processing Time: 30 seconds | Spark UI: http://localhost:4040\")\n",
    "print()\n",
    "\n",
    "# Monitoring loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce26ba2d",
   "metadata": {},
   "source": [
    "## Shutdown Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0493a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shutdown_all() -> None:\n",
    "    \"\"\"Graceful shutdown of all streams and Spark session.\"\"\"\n",
    "    logger.info(\"Shutting down pipeline\")\n",
    "    \n",
    "    streams = [\n",
    "        (\"Bronze\", query_bronze),\n",
    "        (\"Silver\", query_silver),\n",
    "        (\"Silver_ML\", query_silver_ml),\n",
    "        (\"Gold_Traffic\", query_gold_traffic),\n",
    "        (\"Gold_Metrics\", query_gold_metrics)\n",
    "    ]\n",
    "    \n",
    "    for name, stream in streams:\n",
    "        try:\n",
    "            stream.stop()\n",
    "            logger.info(f\"{name} stopped\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error stopping {name}: {str(e)}\")\n",
    "    \n",
    "    spark.stop()\n",
    "    logger.info(\"Spark session stopped\")\n",
    "\n",
    "shutdown_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
