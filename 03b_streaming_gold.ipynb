{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8efa999",
   "metadata": {},
   "source": [
    "# 03b - Streaming Gold & AgrÃ©gations Temps RÃ©el\n",
    "\n",
    "Pipeline d'agrÃ©gation temps rÃ©el :\n",
    "- **Silver â†’ Gold** : AgrÃ©gations avec fenÃªtres temporelles (Tumbling & Sliding Windows)\n",
    "- **2 RequÃªtes mÃ©tier imposÃ©es :**\n",
    "  1. **Tumbling Window** : Trafic par pays (fenÃªtre fixe 30s)\n",
    "  2. **Sliding Window** : Vitesse/Altitude moyenne par catÃ©gorie (fenÃªtre 1m, glissement 10s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb0c2f",
   "metadata": {},
   "source": [
    "## Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d914f417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration chargÃ©e depuis .env\n",
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.spark#spark-hadoop-cloud_2.12 added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0e2213de-7443-4108-8680-476fc8d9b270;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.spark#spark-hadoop-cloud_2.12;3.5.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound com.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.14 in central\n",
      "\tfound joda-time#joda-time;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.15.2 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.14 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.16 in central\n",
      "\tfound commons-codec#commons-codec;1.16.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.4 in central\n",
      "\tfound com.microsoft.azure#azure-storage;7.0.1 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.0.0 in central\n",
      "\tfound com.google.guava#guava;14.0.1 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-cloud-storage;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-aliyun;3.3.4 in central\n",
      "\tfound com.aliyun.oss#aliyun-sdk-oss;3.13.0 in central\n",
      "\tfound org.jdom#jdom2;2.0.6 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound stax#stax-api;1.0.1 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-core;4.5.10 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.ini4j#ini4j;0.5.4 in central\n",
      "\tfound io.opentracing#opentracing-api;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-util;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-noop;0.33.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-ram;3.1.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-kms;2.11.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure-datalake;3.3.4 in central\n",
      "\tfound com.microsoft.azure#azure-data-lake-store-sdk;2.3.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-openstack;3.3.4 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.54.v20240208 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.54.v20240208 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 1548ms :: artifacts dl 25ms\n",
      "\t:: modules in use:\n",
      "\tcom.aliyun#aliyun-java-sdk-core;4.5.10 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-kms;2.11.0 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-ram;3.1.0 from central in [default]\n",
      "\tcom.aliyun.oss#aliyun-sdk-oss;3.13.0 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.15.2 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.14 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.guava#guava;14.0.1 from central in [default]\n",
      "\tcom.microsoft.azure#azure-data-lake-store-sdk;2.3.9 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.16.1 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\tio.opentracing#opentracing-api;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-noop;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-util;0.33.0 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tjoda-time#joda-time;2.12.5 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aliyun;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure-datalake;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-cloud-storage;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-openstack;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.14 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.16 from central in [default]\n",
      "\torg.apache.spark#spark-hadoop-cloud_2.12;3.5.3 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.54.v20240208 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.54.v20240208 from central in [default]\n",
      "\torg.ini4j#ini4j;0.5.4 from central in [default]\n",
      "\torg.jdom#jdom2;2.0.6 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\tstax#stax-api;1.0.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 by [org.eclipse.jetty#jetty-util-ajax;9.4.54.v20240208] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   50  |   0   |   0   |   1   ||   49  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0e2213de-7443-4108-8680-476fc8d9b270\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 49 already retrieved (0kB/17ms)\n",
      "26/01/22 20:20:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/22 20:20:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "26/01/22 20:20:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark Session 'StreamingGold' configurÃ©e\n",
      "âœ… Input Silver:        s3a://datalake/silver/flights\n",
      "âœ… Gold Traffic:        s3a://datalake/gold/traffic_by_country\n",
      "âœ… Gold Metrics:        s3a://datalake/gold/metrics_by_category\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, window, count, avg, max as spark_max, min as spark_min,\n",
    "    sum as spark_sum, stddev, round, desc\n",
    ")\n",
    "from config import get_s3_path, create_spark_session\n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max as spark_max\n",
    "import os\n",
    "\n",
    "# Chemins S3\n",
    "SILVER_PATH = get_s3_path(\"silver\", \"flights\")\n",
    "GOLD_TRAFFIC_PATH = get_s3_path(\"gold\", \"traffic_by_country\")\n",
    "GOLD_METRICS_PATH = get_s3_path(\"gold\", \"metrics_by_category\")\n",
    "CHECKPOINT_TRAFFIC = get_s3_path(\"checkpoints\", \"gold_traffic_by_country\")\n",
    "CHECKPOINT_METRICS = get_s3_path(\"checkpoints\", \"gold_metrics_by_category\")\n",
    "\n",
    "spark = create_spark_session(\"StreamingGold\")\n",
    "\n",
    "print(f\"âœ… Input Silver:        {SILVER_PATH}\")\n",
    "print(f\"âœ… Gold Traffic:        {GOLD_TRAFFIC_PATH}\")\n",
    "print(f\"âœ… Gold Metrics:        {GOLD_METRICS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d0b0e",
   "metadata": {},
   "source": [
    "## RequÃªte 1 : Tumbling Window - Trafic par Pays\n",
    "\n",
    "**Objectif :** Compter les avions uniques par pays d'origine sur des fenÃªtres fixes de 30 secondes.\n",
    "\n",
    "**Cas d'usage mÃ©tier :** Monitoring du trafic par pays en temps rÃ©el pour dÃ©tecter les pics de congestion.\n",
    "\n",
    "**Window Type :** Tumbling (fenÃªtres qui ne se chevauchent pas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2382d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/22 20:21:04 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ RequÃªte 1: Tumbling Window (30s) - Trafic par Pays\n",
      "   Structure: [window_start, window_end, origin_country, flight_count]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/22 20:21:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Query1 ID: 42b870ab-898d-41bb-a5ec-c1f8392bc9dc\n",
      "   Destination: s3a://datalake/gold/traffic_by_country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/22 20:21:16 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/22 20:21:17 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df_silver_stream = spark.readStream.format(\"delta\").load(SILVER_PATH)\n",
    "\n",
    "# RequÃªte 1: Tumbling Window (30s) - Trafic par pays\n",
    "query_traffic = df_silver_stream \\\n",
    "    .filter(col(\"origin_country\").isNotNull()) \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_timestamp\"), \"30 seconds\"),\n",
    "        col(\"origin_country\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"icao24\").alias(\"flight_count\")\n",
    "    )\n",
    "\n",
    "print(\"ðŸš€ RequÃªte 1: Tumbling Window (30s) - Trafic par Pays\")\n",
    "print(\"   Structure: [window_start, window_end, origin_country, flight_count]\")\n",
    "\n",
    "query1 = query_traffic \\\n",
    "    .writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_TRAFFIC) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(GOLD_TRAFFIC_PATH)\n",
    "\n",
    "print(f\"âœ… Query1 ID: {query1.id}\")\n",
    "print(f\"   Destination: {GOLD_TRAFFIC_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f636f51",
   "metadata": {},
   "source": [
    "## RequÃªte 2 : Sliding Window - Vitesse & Altitude par CatÃ©gorie\n",
    "\n",
    "**Objectif :** Calculer la vitesse moyenne et l'altitude moyenne par catÃ©gorie d'avion sur une fenÃªtre glissante.\n",
    "\n",
    "**FenÃªtre :** 1 minute de durÃ©e, glissant toutes les 10 secondes.\n",
    "\n",
    "**Cas d'usage mÃ©tier :** Analyser les profils de vol par catÃ©gorie (Light vs Heavy) pour calibrer les modÃ¨les ML.\n",
    "\n",
    "**Window Type :** Sliding (fenÃªtres qui se chevauchent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e036dd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ RequÃªte 2: Sliding Window (1m duration, 10s slide) - Vitesse/Altitude par CatÃ©gorie\n",
      "   Structure: [window_start, window_end, category, aircraft_count, avg_velocity, ...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/22 20:21:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Query2 ID: 940df0b5-0c6e-414c-a994-2cc8104942f6\n",
      "   Destination: s3a://datalake/gold/metrics_by_category\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/22 20:21:23 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/22 20:21:23 WARN HDFSBackedStateStoreProvider: The state for version 89 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:23 WARN HDFSBackedStateStoreProvider: The state for version 89 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:23 WARN HDFSBackedStateStoreProvider: The state for version 89 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:23 WARN HDFSBackedStateStoreProvider: The state for version 89 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:23 WARN HDFSBackedStateStoreProvider: The state for version 89 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:23 WARN HDFSBackedStateStoreProvider: The state for version 89 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:23 WARN HDFSBackedStateStoreProvider: The state for version 89 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:24 WARN HDFSBackedStateStoreProvider: The state for version 89 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:24 WARN HDFSBackedStateStoreProvider: The state for version 89 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:24 WARN HDFSBackedStateStoreProvider: The state for version 89 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/22 20:21:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "26/01/22 20:21:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "26/01/22 20:21:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "26/01/22 20:21:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/22 20:21:30 WARN HDFSBackedStateStoreProvider: The state for version 88 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:30 WARN HDFSBackedStateStoreProvider: The state for version 88 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:30 WARN HDFSBackedStateStoreProvider: The state for version 88 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:30 WARN HDFSBackedStateStoreProvider: The state for version 88 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:30 WARN HDFSBackedStateStoreProvider: The state for version 88 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:30 WARN HDFSBackedStateStoreProvider: The state for version 88 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:30 WARN HDFSBackedStateStoreProvider: The state for version 88 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:30 WARN HDFSBackedStateStoreProvider: The state for version 88 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:30 WARN HDFSBackedStateStoreProvider: The state for version 88 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:30 WARN HDFSBackedStateStoreProvider: The state for version 88 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 20:21:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/22 20:21:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "26/01/22 20:21:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "26/01/22 20:21:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "26/01/22 20:21:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/22 20:21:42 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/22 20:21:48 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/22 20:22:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/22 20:22:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "26/01/22 20:22:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/22 20:22:47 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/22 20:22:47 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "26/01/22 20:22:47 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n"
     ]
    }
   ],
   "source": [
    "# RequÃªte 2: Sliding Window (1m duration, 10s slide) - MÃ©triques par CatÃ©gorie\n",
    "query_metrics = df_silver_stream \\\n",
    "    .filter(col(\"category\").isNotNull()) \\\n",
    "    .filter(col(\"velocity_kmh\") > 0) \\\n",
    "    .filter(col(\"altitude_meters\").isNotNull()) \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_timestamp\"), \"1 minute\", \"10 seconds\"),\n",
    "        col(\"category\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"icao24\").alias(\"aircraft_count\"),\n",
    "        round(avg(col(\"velocity_kmh\")), 2).alias(\"avg_velocity_kmh\"),\n",
    "        round(spark_max(col(\"velocity_kmh\")), 2).alias(\"max_velocity_kmh\"),\n",
    "        round(spark_min(col(\"velocity_kmh\")), 2).alias(\"min_velocity_kmh\"),\n",
    "        round(stddev(col(\"velocity_kmh\")), 2).alias(\"stddev_velocity_kmh\"),\n",
    "        round(avg(col(\"altitude_meters\")), 0).alias(\"avg_altitude_m\"),\n",
    "        round(spark_max(col(\"altitude_meters\")), 0).alias(\"max_altitude_m\"),\n",
    "        round(spark_min(col(\"altitude_meters\")), 0).alias(\"min_altitude_m\")\n",
    "    )\n",
    "\n",
    "print(\"ðŸš€ RequÃªte 2: Sliding Window (1m duration, 10s slide) - Vitesse/Altitude par CatÃ©gorie\")\n",
    "print(\"   Structure: [window_start, window_end, category, aircraft_count, avg_velocity, ...]\")\n",
    "\n",
    "query2 = query_metrics \\\n",
    "    .writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_METRICS) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(GOLD_METRICS_PATH)\n",
    "\n",
    "print(f\"âœ… Query2 ID: {query2.id}\")\n",
    "print(f\"   Destination: {GOLD_METRICS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99732df4",
   "metadata": {},
   "source": [
    "## Monitoring des Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcceeb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Monitoring des streams (Ctrl+C pour arrÃªter)\n",
      "======================================================================\n",
      "\n",
      "â±ï¸  19:51:49\n",
      "  Query 1 (Traffic):  {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Query 2 (Metrics):  {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/22 19:51:51 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/22 19:51:51 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/22 19:51:52 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â¹ï¸  ArrÃªt demandÃ©...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/22 19:51:57 WARN HDFSBackedStateStoreProvider: The state for version 53 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 19:51:57 WARN HDFSBackedStateStoreProvider: The state for version 53 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 19:51:57 WARN HDFSBackedStateStoreProvider: The state for version 52 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 19:51:57 WARN HDFSBackedStateStoreProvider: The state for version 52 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 19:51:57 WARN HDFSBackedStateStoreProvider: The state for version 52 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 19:51:57 WARN HDFSBackedStateStoreProvider: The state for version 52 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 19:51:57 WARN HDFSBackedStateStoreProvider: The state for version 52 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 19:51:58 WARN HDFSBackedStateStoreProvider: The state for version 53 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 19:51:58 WARN HDFSBackedStateStoreProvider: The state for version 52 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 19:51:58 WARN HDFSBackedStateStoreProvider: The state for version 52 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 19:51:58 WARN HDFSBackedStateStoreProvider: The state for version 53 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 19:51:58 WARN HDFSBackedStateStoreProvider: The state for version 52 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 19:51:58 WARN HDFSBackedStateStoreProvider: The state for version 53 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 19:51:58 WARN HDFSBackedStateStoreProvider: The state for version 52 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 19:51:58 WARN HDFSBackedStateStoreProvider: The state for version 53 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/22 19:51:58 WARN HDFSBackedStateStoreProvider: The state for version 52 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"ðŸ“Š Monitoring des streams (Ctrl+C pour arrÃªter)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        print(f\"\\nâ±ï¸  {time.strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Query 1 (Traffic):  {query1.status}\")\n",
    "        print(f\"  Query 2 (Metrics):  {query2.status}\")\n",
    "        time.sleep(30)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâ¹ï¸  ArrÃªt demandÃ©...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f127b6",
   "metadata": {},
   "source": [
    "## ArrÃªt des Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3053d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tous les streams Gold arrÃªtÃ©s\n"
     ]
    }
   ],
   "source": [
    "query1.stop()\n",
    "query2.stop()\n",
    "print(\"âœ… Tous les streams Gold arrÃªtÃ©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547177de",
   "metadata": {},
   "source": [
    "## VÃ©rification des RÃ©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5072f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š VÃ©rification du Gold Traffic (3 derniÃ¨res fenÃªtres):\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/22 18:04:07 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Lire les derniers agrÃ©gats pour vÃ©rifier\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ“Š VÃ©rification du Gold Traffic (3 derniÃ¨res fenÃªtres):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdelta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGOLD_TRAFFIC_PATH\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[32m      4\u001b[39m     .orderBy(desc(\u001b[33m\"\u001b[39m\u001b[33mwindow\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m      5\u001b[39m     .limit(\u001b[32m15\u001b[39m) \\\n\u001b[32m      6\u001b[39m     .show(truncate=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ“Š VÃ©rification du Gold Metrics (3 derniÃ¨res fenÃªtres):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m spark.read.format(\u001b[33m\"\u001b[39m\u001b[33mdelta\u001b[39m\u001b[33m\"\u001b[39m).load(GOLD_METRICS_PATH) \\\n\u001b[32m     10\u001b[39m     .orderBy(desc(\u001b[33m\"\u001b[39m\u001b[33mwindow\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     11\u001b[39m     .limit(\u001b[32m15\u001b[39m) \\\n\u001b[32m     12\u001b[39m     .show(truncate=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyspark/sql/readwriter.py:307\u001b[39m, in \u001b[36mDataFrameReader.load\u001b[39m\u001b[34m(self, path, format, schema, **options)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28mself\u001b[39m.options(**options)\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) != \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lire les derniers agrÃ©gats pour vÃ©rifier\n",
    "print(\"ðŸ“Š VÃ©rification du Gold Traffic (3 derniÃ¨res fenÃªtres):\\n\")\n",
    "spark.read.format(\"delta\").load(GOLD_TRAFFIC_PATH) \\\n",
    "    .orderBy(desc(\"window\")) \\\n",
    "    .limit(15) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "print(\"\\nðŸ“Š VÃ©rification du Gold Metrics (3 derniÃ¨res fenÃªtres):\\n\")\n",
    "spark.read.format(\"delta\").load(GOLD_METRICS_PATH) \\\n",
    "    .orderBy(desc(\"window\")) \\\n",
    "    .limit(15) \\\n",
    "    .show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
