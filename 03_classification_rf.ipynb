{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Classification Random Forest - Silver_ML to Gold\n",
    "\n",
    "Ce notebook impl√©mente un pipeline MLlib complet pour classifier les phases de vol :\n",
    "- Lecture des donn√©es Silver_ML\n",
    "- Pipeline de pr√©paration et classification\n",
    "- Random Forest Classifier\n",
    "- √âvaluation et m√©triques\n",
    "- Sauvegarde dans la couche Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 1 : Configuration\n",
    "%pip install python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "GARAGE_ENDPOINT = os.getenv(\"GARAGE_ENDPOINT\", \"http://garage:3900\")\n",
    "ACCESS_KEY = os.getenv(\"ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\", \"datalake\")\n",
    "\n",
    "if not ACCESS_KEY or not SECRET_KEY:\n",
    "    raise ValueError(\"‚ùå ACCESS_KEY et SECRET_KEY doivent √™tre d√©finis dans le fichier .env\")\n",
    "\n",
    "# D√©finir les chemins\n",
    "SILVER_ML_PATH = f\"s3a://{BUCKET_NAME}/silver/flights_ml\"\n",
    "GOLD_MODEL_PATH = f\"s3a://{BUCKET_NAME}/gold/models/rf_flight_phase\"\n",
    "GOLD_PREDICTIONS_PATH = f\"s3a://{BUCKET_NAME}/gold/predictions/flight_phase\"\n",
    "\n",
    "print(\"‚úÖ Configuration charg√©e depuis .env\")\n",
    "print(f\"üìÇ Silver ML Path: {SILVER_ML_PATH}\")\n",
    "print(f\"üìÇ Gold Model Path: {GOLD_MODEL_PATH}\")\n",
    "print(f\"üìÇ Gold Predictions Path: {GOLD_PREDICTIONS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de Spark avec configuration S3/Delta\n",
    "\n",
    "# 1. Packages\n",
    "packages = [\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    \"com.amazonaws:aws-java-sdk-bundle:1.12.262\",\n",
    "    \"org.apache.spark:spark-hadoop-cloud_2.12:3.5.3\",\n",
    "    \"io.delta:delta-spark_2.12:3.0.0\"\n",
    "]\n",
    "\n",
    "# 2. Configuration Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RandomForestClassification_Gold\") \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", GARAGE_ENDPOINT) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint.region\", \"garage\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.committer.name\", \"filesystem\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\", \"false\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"‚úÖ Spark Session configur√©e pour Random Forest Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6386c7",
   "metadata": {},
   "outputs": [],
   "source": "# Cellule 2 : Lecture et pr√©paration des donn√©es\n\nfrom pyspark.sql.functions import col\n\nprint(f\"üìñ Lecture des donn√©es Silver_ML depuis {SILVER_ML_PATH}...\")\ndf_ml = spark.read.format(\"delta\").load(SILVER_ML_PATH)\n\n# Afficher le sch√©ma\nprint(\"\\nüìã Sch√©ma des donn√©es :\")\ndf_ml.printSchema()\n\n# Compter avant filtrage\ncount_before = df_ml.count()\nprint(f\"\\nüìä Nombre de lignes avant filtrage : {count_before:,}\")\n\n# D√©finir les colonnes de features\nfeature_columns = [\n    \"altitude_meters\",\n    \"velocity_kmh\",\n    \"altitude_change\",\n    \"velocity_change\",\n    \"rolling_avg_altitude\",\n    \"rolling_std_altitude\",\n    \"rolling_avg_velocity\"\n]\n\n# Filtrer uniquement les lignes avec label NULL\nprint(\"\\nüßπ Filtrage des lignes avec label NULL...\")\ndf_clean = df_ml.filter(col(\"flight_phase\").isNotNull())\n\n# Remplacer les NULL dans les features par 0 au lieu de supprimer les lignes\nprint(\"üîß Remplacement des NULL dans les features par 0...\")\ndf_clean = df_clean.fillna(0, subset=feature_columns)\n\ncount_after = df_clean.count()\nremoved = count_before - count_after\nprint(f\"Lignes apr√®s nettoyage : {count_after:,}\")\nprint(f\"Lignes supprim√©es : {removed:,} ({100 * removed / count_before:.2f}% if count_before > 0 else 0}%)\")\n\n# Afficher la distribution du label\nprint(\"\\nüìä Distribution du label 'flight_phase' :\")\ndf_clean.groupBy(\"flight_phase\").count().orderBy(\"count\", ascending=False).show()\n\nprint(\"\\n‚úÖ Donn√©es pr√™tes pour l'entra√Ænement\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 3 : Construction du Pipeline MLlib\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, IndexToString\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "print(\"üî® Construction du Pipeline MLlib...\")\n",
    "\n",
    "# Stage 1 : StringIndexer - Convertir flight_phase en label num√©rique\n",
    "print(\"\\n1Ô∏è‚É£ StringIndexer : flight_phase -> label\")\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"flight_phase\",\n",
    "    outputCol=\"label\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# Stage 2 : VectorAssembler - Assembler les features en vecteur\n",
    "print(\"2Ô∏è‚É£ VectorAssembler : features num√©riques -> features_raw\")\n",
    "print(f\"   Features utilis√©es : {', '.join(feature_columns)}\")\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# Stage 3 : StandardScaler - Normaliser les features\n",
    "print(\"3Ô∏è‚É£ StandardScaler : features_raw -> features\")\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=False\n",
    ")\n",
    "\n",
    "# Stage 4 : RandomForestClassifier\n",
    "print(\"4Ô∏è‚É£ RandomForestClassifier : 100 arbres, maxDepth=10\")\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"prediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Stage 5 : IndexToString - Reconvertir les pr√©dictions en labels lisibles\n",
    "print(\"5Ô∏è‚É£ IndexToString : prediction -> predicted_label\")\n",
    "label_converter = IndexToString(\n",
    "    inputCol=\"prediction\",\n",
    "    outputCol=\"predicted_label\",\n",
    "    labels=label_indexer.fit(df_clean).labels\n",
    ")\n",
    "\n",
    "# Cr√©er le pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    label_indexer,\n",
    "    vector_assembler,\n",
    "    scaler,\n",
    "    rf,\n",
    "    label_converter\n",
    "])\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline cr√©√© avec 5 stages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9aedbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 4 : Split et entra√Ænement\n",
    "\n",
    "print(\"üìä Split des donn√©es en train/test (80/20)...\")\n",
    "train_df, test_df = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_count = train_df.count()\n",
    "test_count = test_df.count()\n",
    "print(f\"üìä Train : {train_count:,} lignes ({100 * train_count / (train_count + test_count):.1f}%)\")\n",
    "print(f\"üìä Test  : {test_count:,} lignes ({100 * test_count / (train_count + test_count):.1f}%)\")\n",
    "\n",
    "# Afficher la distribution des labels dans train et test\n",
    "print(\"\\nüìä Distribution du label dans le set d'entra√Ænement :\")\n",
    "train_df.groupBy(\"flight_phase\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "print(\"\\nüöÄ Entra√Ænement du mod√®le Random Forest...\")\n",
    "print(\"‚è≥ Cela peut prendre quelques minutes...\")\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "print(\"\\n‚úÖ Mod√®le entra√Æn√© avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a4230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 5 : Pr√©dictions et √©valuation\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "print(\"üîÆ G√©n√©ration des pr√©dictions sur le set de test...\")\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Afficher quelques pr√©dictions\n",
    "print(\"\\nüîç Aper√ßu des pr√©dictions :\")\n",
    "predictions.select(\n",
    "    \"icao24\", \"event_timestamp\", \"altitude_meters\", \"velocity_kmh\",\n",
    "    \"flight_phase\", \"predicted_label\", \"probability\"\n",
    ").show(10, truncate=False)\n",
    "\n",
    "# Calculer les m√©triques\n",
    "print(\"\\nüìä Calcul des m√©triques d'√©valuation...\")\n",
    "\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator_acc.evaluate(predictions)\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"           M√âTRIQUES D'√âVALUATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üéØ Accuracy  : {accuracy:.4f} ({100*accuracy:.2f}%)\")\n",
    "print(f\"üéØ F1 Score  : {f1:.4f}\")\n",
    "print(f\"üéØ Precision : {precision:.4f}\")\n",
    "print(f\"üéØ Recall    : {recall:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 6 : Matrice de confusion\n",
    "\n",
    "print(\"üìä Matrice de confusion :\")\n",
    "print(\"\\nLignes = Label r√©el | Colonnes = Label pr√©dit\\n\")\n",
    "\n",
    "confusion_matrix = predictions.groupBy(\"flight_phase\", \"predicted_label\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"flight_phase\", \"predicted_label\")\n",
    "\n",
    "confusion_matrix.show(100, truncate=False)\n",
    "\n",
    "# Calculer l'accuracy par classe\n",
    "print(\"\\nüìä Accuracy par classe :\")\n",
    "from pyspark.sql.functions import sum as _sum, col, when\n",
    "\n",
    "class_accuracy = predictions.groupBy(\"flight_phase\").agg(\n",
    "    _sum(when(col(\"flight_phase\") == col(\"predicted_label\"), 1).otherwise(0)).alias(\"correct\"),\n",
    "    _sum(when(col(\"flight_phase\") != col(\"predicted_label\"), 1).otherwise(0)).alias(\"incorrect\")\n",
    ").withColumn(\n",
    "    \"accuracy\",\n",
    "    col(\"correct\") / (col(\"correct\") + col(\"incorrect\"))\n",
    ").orderBy(\"accuracy\", ascending=False)\n",
    "\n",
    "class_accuracy.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6869d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 7 : Feature Importance\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üìä Analyse de l'importance des features...\")\n",
    "\n",
    "# Extraire le Random Forest du pipeline (stage 3, car index√© √† partir de 0)\n",
    "# Stages: 0=StringIndexer, 1=VectorAssembler, 2=StandardScaler, 3=RandomForest, 4=IndexToString\n",
    "rf_model = model.stages[3]\n",
    "\n",
    "# Extraire les importances\n",
    "importances = rf_model.featureImportances.toArray()\n",
    "\n",
    "# Cr√©er un DataFrame pandas pour un affichage plus lisible\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_columns,\n",
    "    \"importance\": importances,\n",
    "    \"importance_pct\": [f\"{100*imp:.2f}%\" for imp in importances]\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"           IMPORTANCE DES FEATURES\")\n",
    "print(\"=\"*60)\n",
    "print(feature_importance_df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Statistiques suppl√©mentaires sur le mod√®le\n",
    "print(f\"\\nüå≥ Nombre d'arbres : {rf_model.getNumTrees}\")\n",
    "print(f\"üå≥ Profondeur max  : {rf_model.getMaxDepth()}\")\n",
    "print(f\"üå≥ Nombre de features : {len(feature_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 8 : Sauvegarder le mod√®le et les pr√©dictions\n",
    "\n",
    "print(\"üíæ Sauvegarde du mod√®le et des pr√©dictions...\")\n",
    "\n",
    "# Sauvegarder le mod√®le\n",
    "print(f\"\\nüì¶ Sauvegarde du mod√®le dans {GOLD_MODEL_PATH}...\")\n",
    "model.write().overwrite().save(GOLD_MODEL_PATH)\n",
    "print(f\"‚úÖ Mod√®le sauvegard√© dans {GOLD_MODEL_PATH}\")\n",
    "\n",
    "# Sauvegarder les pr√©dictions\n",
    "print(f\"\\nüì¶ Sauvegarde des pr√©dictions dans {GOLD_PREDICTIONS_PATH}...\")\n",
    "predictions_to_save = predictions.select(\n",
    "    \"event_timestamp\",\n",
    "    \"icao24\",\n",
    "    \"callsign\",\n",
    "    \"origin_country\",\n",
    "    \"altitude_meters\",\n",
    "    \"velocity_kmh\",\n",
    "    \"flight_phase\",\n",
    "    \"predicted_label\",\n",
    "    \"probability\"\n",
    ")\n",
    "\n",
    "predictions_to_save.write.format(\"delta\").mode(\"overwrite\").save(GOLD_PREDICTIONS_PATH)\n",
    "print(f\"‚úÖ Pr√©dictions sauvegard√©es dans {GOLD_PREDICTIONS_PATH}\")\n",
    "print(f\"üìä Nombre de pr√©dictions sauvegard√©es : {predictions_to_save.count():,}\")\n",
    "\n",
    "# V√©rification\n",
    "print(\"\\nüîç V√©rification des donn√©es sauvegard√©es :\")\n",
    "saved_predictions = spark.read.format(\"delta\").load(GOLD_PREDICTIONS_PATH)\n",
    "print(f\"‚úÖ {saved_predictions.count():,} lignes lues depuis {GOLD_PREDICTIONS_PATH}\")\n",
    "saved_predictions.show(5, truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"   ‚úÖ PIPELINE COMPLET TERMIN√â AVEC SUCC√àS !\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÇ Mod√®le      : {GOLD_MODEL_PATH}\")\n",
    "print(f\"üìÇ Pr√©dictions : {GOLD_PREDICTIONS_PATH}\")\n",
    "print(f\"üéØ Accuracy    : {accuracy:.4f}\")\n",
    "print(f\"üéØ F1 Score    : {f1:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}