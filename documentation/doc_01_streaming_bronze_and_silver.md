# SchÃ©ma du flux de donnÃ©es - Pipeline OpenSky

![SchÃ©ma du Pipeline OpenSky](image_01_streaming_bronze_and_silver.png)

## Description des Ã©tapes

### 1. Ingestion (Source) ğŸ“¡

- **Source** : OpenSky Network API.
- **Composant** : Producer exÃ©cutÃ© en background thread.
- **FrÃ©quence** : Appel toutes les 15 secondes (respect du rate limiting).
- **Format** : Conversion des vecteurs d'Ã©tat (listes) en objets JSON structurÃ©s avant l'envoi.

### 2. Messaging (Buffer) ğŸ“¨

- **SystÃ¨me** : Apache Kafka.
- **Topic** : opensky-data.
- **RÃ´le** : Tampon de dÃ©couplage pour garantir la rÃ©silience et absorber les pics de charge avant le traitement Spark.

### 3. Bronze Layer (DonnÃ©es Brutes) ğŸŸ¤

- **Traitement** : Apache Spark Streaming (readStream).
- **Action** : Lecture depuis Kafka, parsing du schÃ©ma JSON, et Ã©criture "as-is" (telle quelle).
- **Stockage** : S3 (Garage) au format Delta Lake.
- **Objectif** : Historisation complÃ¨te et immuable de la donnÃ©e brute.
- **Technique** : Utilisation de Checkpoints pour garantir la tolÃ©rance aux pannes (Exactly-once delivery).

### 4. Silver Layer (DonnÃ©es RaffinÃ©es) âšª

- **Traitement** : Apache Spark Streaming.
- **Action** : Lecture du flux Bronze et application des rÃ¨gles mÃ©tier.
- **Transformations appliquÃ©es** :
    - **ğŸ§¹ Filtrage** : Suppression des vols sans identifiant (icao24) ou sans coordonnÃ©es GPS.
    - **ğŸ•’ Typage** : Conversion du timestamp Unix en format Timestamp standard.
    - **ğŸï¸ Enrichissement** : Calcul de la vitesse en km/h (velocity * 3.6).
    - **ğŸ“‹ Projection** : SÃ©lection et renommage des colonnes utiles pour l'analyse.
- **Stockage** : Table Delta optimisÃ©e pour les requÃªtes analytiques.

## Comment utiliser le notebook 01_streaming_bronze_and_silver ?

### 1ï¸âƒ£ Ã‰tape 1 : Configuration et Initialisation (Cellules 0, 1, 2, 3)

**Objectif** : Charger la configuration, initialiser Spark, vÃ©rifier le bucket S3 et lancer le Producer Python.

1. **ExÃ©cute la Cellule 0** (Variables globales)
   - Importe les librairies nÃ©cessaires (threading, kafka, requests, etc.)
   - DÃ©finit les variables de configuration (Kafka, Garage/S3, credentials)
   - ğŸ‘€ Tu dois voir : `âœ… Configuration chargÃ©e.`

2. **ExÃ©cute la Cellule 1** (Initialisation de Spark)
   - Configure la SparkSession avec les packages nÃ©cessaires (Delta Lake, Kafka, S3)
   - Configure la connexion S3 vers Garage (MinIO)
   - ğŸ‘€ Tu dois voir : `âœ… Spark Session configurÃ©e (RÃ©gion 'garage' forcÃ©e).`

3. **ExÃ©cute la Cellule 2** (VÃ©rification du bucket)
   - VÃ©rifie l'existence du bucket `datalake` sur Garage
   - Le crÃ©e s'il n'existe pas
   - ğŸ‘€ Tu dois voir : `âœ… Le bucket 'datalake' existe dÃ©jÃ .` (ou message de crÃ©ation)

4. **ExÃ©cute la Cellule 3** (Thread Producer)
   - Lance un thread en arriÃ¨re-plan qui interroge l'API OpenSky toutes les 15 secondes
   - Publie les donnÃ©es sur le topic Kafka `opensky-data`
   - ğŸ‘€ Tu dois voir : `âœ… Le Producer tourne en arriÃ¨re-plan ! Passez Ã  la suite.`
   - ğŸ‘€ Puis rÃ©guliÃ¨rement : `ğŸ“¡ [THREAD PRODUCER] XX vols envoyÃ©s Ã  HH:MM:SS`

âš ï¸ **Important** : Laisse cette cellule tourner. Ne l'arrÃªte pas. Le Producer tourne en arriÃ¨re-plan et alimente Kafka.

> ğŸ’¡ **Note** : La Cellule 4 est optionnelle. Elle permet d'arrÃªter le Producer si besoin (en dÃ©finissant `stop_producer = True`).

### 2ï¸âƒ£ Ã‰tape 2 : Remplir le Bronze (Cellule 5)

**Objectif** : CrÃ©er la couche Bronze en lisant depuis Kafka et en Ã©crivant les donnÃ©es brutes sur S3.

1. **ExÃ©cute la Cellule 5** (Streaming vers Bronze)
   - DÃ©finit le schÃ©ma des donnÃ©es OpenSky
   - Lit le stream depuis Kafka (topic `opensky-data`)
   - Parse les messages JSON selon le schÃ©ma
   - Ã‰crit en Delta Lake sur S3 : `s3a://datalake/bronze/flights`
   - Utilise un checkpoint pour la tolÃ©rance aux pannes

2. **â³ Patiente 2 Ã  3 minutes**
   - Le stream Spark traite les donnÃ©es en micro-batches
   - Les logs peuvent Ãªtre limitÃ©s en mode cluster, c'est normal

3. **VÃ©rification Visuelle** :
   - Ouvre ton navigateur sur **Garage WebUI** (gÃ©nÃ©ralement `http://localhost:3909` ou le port configurÃ©)
   - Navigue vers `datalake > bronze > flights`
   - Tu DOIS voir :
     - Le dossier `_delta_log` (mÃ©tadonnÃ©es Delta Lake)
     - Des fichiers `.parquet` (donnÃ©es)

4. **ğŸ›‘ Une fois que tu as vu les fichiers : ARRÃŠTE LA CELLULE 5** (Bouton CarrÃ© Noir â¹ï¸)
   - Pourquoi ? Pour libÃ©rer les ressources avant de lancer le Silver
   - Le Producer continue de tourner en arriÃ¨re-plan et alimente Kafka

### 3ï¸âƒ£ Ã‰tape 3 : CrÃ©er le Silver (Cellule 6)

**Objectif** : Lire les donnÃ©es Bronze, les nettoyer et les enrichir pour crÃ©er la couche Silver.

1. **ExÃ©cute la Cellule 6** (Streaming vers Silver)
   - Lit en streaming depuis la table Bronze (Delta Lake)
   - Applique les transformations :
     - Filtre les enregistrements sans `icao24` ou sans coordonnÃ©es GPS
     - Convertit le timestamp Unix en format datetime (`event_timestamp`)
     - Calcule la vitesse en km/h (`velocity_kmh = velocity * 3.6`)
     - SÃ©lectionne et renomme les colonnes pertinentes
   - Ã‰crit en Delta Lake sur S3 : `s3a://datalake/silver/flights`

2. **â³ Patiente 1 Ã  2 minutes**
   - Spark lit tout ce que le Bronze a stockÃ© Ã  l'Ã©tape prÃ©cÃ©dente
   - Les transformations sont appliquÃ©es en streaming

3. **VÃ©rification Visuelle** :
   - Retourne sur le **Garage WebUI**
   - Navigue vers `datalake > silver > flights`
   - Tu devrais voir :
     - Le dossier `_delta_log`
     - Des fichiers `.parquet` avec les donnÃ©es nettoyÃ©es et enrichies

4. **ğŸ›‘ Une fois que tu as vu les fichiers : ARRÃŠTE LA CELLULE 6** (Bouton CarrÃ© Noir â¹ï¸)

### 4ï¸âƒ£ Ã‰tape 4 : VÃ©rification (Cellule 7)

**Objectif** : VÃ©rifier que les donnÃ©es Silver sont bien prÃ©sentes et correctement formatÃ©es.

1. **ExÃ©cute la Cellule 7** (Afficher les donnÃ©es Silver)
   - Lit la table Silver depuis S3
   - Affiche les 5 premiÃ¨res lignes avec `.show(5)`

2. **ğŸ‘€ Tu dois voir** :
   - Un tableau avec les colonnes : `event_timestamp`, `icao24`, `callsign`, `origin_country`, `longitude`, `latitude`, `velocity_kmh`, `altitude_meters`, `on_ground`, `category`
   - Les donnÃ©es doivent Ãªtre propres (pas de nulls pour `icao24`, `latitude`, `longitude`)
   - La vitesse doit Ãªtre en km/h
   - Le timestamp doit Ãªtre au format datetime

### ğŸ”„ Pour relancer le pipeline complet

Si tu veux relancer tout le pipeline :

1. **ArrÃªte le Producer** (optionnel) : ExÃ©cute la Cellule 4 pour arrÃªter le thread Producer
2. **Relance le Producer** : ExÃ©cute la Cellule 3
3. **Relance le Bronze** : ExÃ©cute la Cellule 5 (laisse tourner quelques minutes puis arrÃªte)
4. **Relance le Silver** : ExÃ©cute la Cellule 6 (laisse tourner puis arrÃªte)
5. **VÃ©rifie** : ExÃ©cute la Cellule 7

### âš ï¸ Points d'attention

- **Checkpoints** : Les checkpoints sont stockÃ©s dans `s3a://datalake/checkpoints/`. Si tu veux repartir de zÃ©ro, supprime ces dossiers.
- **Mode "latest" vs "earliest"** : La Cellule 5 utilise `startingOffsets: "latest"` pour ne lire que les nouveaux messages. Si tu veux traiter tout l'historique Kafka, change en `"earliest"`.
- **Ressources** : Les streams Spark consomment de la mÃ©moire. ArrÃªte les cellules de streaming avant de lancer la suivante si tu es limitÃ© en ressources.
- Ne surtout pas oublier d'arrÃªter le Producer pour Ã©viter de cramer les 400 requÃªtes par jour.

