{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 07 - Streaming Gold\n",
    "\n",
    "Agrégations temps réel avec fenêtres temporelles sur les données Silver_ML.\n",
    "\n",
    "**Requêtes implémentées:**\n",
    "1. **Comptage par phase de vol** - Tumbling window 1 minute\n",
    "2. **Alertes anomalies** - Sliding window 5 minutes (slide 1 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, window, count, avg, stddev, max as spark_max, min as spark_min,\n",
    "    when, lit, current_timestamp\n",
    ")\n",
    "from config import get_s3_path, create_spark_session\n",
    "\n",
    "SILVER_ML_PATH = get_s3_path(\"silver\", \"flights_ml\")\n",
    "GOLD_AGGREGATIONS_PATH = get_s3_path(\"gold\", \"streaming_aggregations\", \"flight_phase_counts\")\n",
    "GOLD_ANOMALIES_PATH = get_s3_path(\"gold\", \"streaming_aggregations\", \"anomaly_alerts\")\n",
    "CHECKPOINT_AGGREGATIONS = get_s3_path(\"checkpoints\", \"gold_aggregations\")\n",
    "CHECKPOINT_ANOMALIES = get_s3_path(\"checkpoints\", \"gold_anomalies\")\n",
    "\n",
    "spark = create_spark_session(\"StreamingGold\")\n",
    "\n",
    "print(f\"Input:  {SILVER_ML_PATH}\")\n",
    "print(f\"Output Aggregations: {GOLD_AGGREGATIONS_PATH}\")\n",
    "print(f\"Output Anomalies:    {GOLD_ANOMALIES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Lecture du stream Silver_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver_stream = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(SILVER_ML_PATH)\n",
    "\n",
    "print(f\"Stream Silver_ML initialisé\")\n",
    "print(f\"Colonnes: {df_silver_stream.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Stream 1 : Comptage par phase de vol (Tumbling Window 1 min)\n",
    "\n",
    "Agrégation temps réel du nombre de vols par phase (CLIMB, CRUISE, DESCENT, etc.) avec une fenêtre tumbling de 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase_counts = df_silver_stream \\\n",
    "    .withWatermark(\"event_timestamp\", \"2 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_timestamp\"), \"1 minute\"),\n",
    "        col(\"flight_phase\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"flight_count\"),\n",
    "        avg(\"altitude_meters\").alias(\"avg_altitude\"),\n",
    "        avg(\"velocity_kmh\").alias(\"avg_velocity\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"flight_phase\"),\n",
    "        col(\"flight_count\"),\n",
    "        col(\"avg_altitude\"),\n",
    "        col(\"avg_velocity\")\n",
    "    )\n",
    "\n",
    "print(\"Stream 1: Comptage par phase de vol (Tumbling Window 1 min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_aggregations = df_phase_counts.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_AGGREGATIONS) \\\n",
    "    .start(GOLD_AGGREGATIONS_PATH)\n",
    "\n",
    "print(f\"Stream 1 démarré -> {GOLD_AGGREGATIONS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Stream 2 : Alertes anomalies par pays (Sliding Window 5 min, slide 1 min)\n",
    "\n",
    "Détection de vitesses et altitudes anormales par pays d'origine avec une fenêtre glissante de 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seuils d'anomalie\n",
    "ALTITUDE_MAX_THRESHOLD = 12000  # mètres\n",
    "VELOCITY_MAX_THRESHOLD = 1000   # km/h\n",
    "ALTITUDE_MIN_THRESHOLD = -100   # mètres (sous le niveau de la mer)\n",
    "VELOCITY_MIN_THRESHOLD = 0      # km/h\n",
    "\n",
    "print(f\"Seuils d'anomalie:\")\n",
    "print(f\"  Altitude: {ALTITUDE_MIN_THRESHOLD}m - {ALTITUDE_MAX_THRESHOLD}m\")\n",
    "print(f\"  Vitesse:  {VELOCITY_MIN_THRESHOLD} - {VELOCITY_MAX_THRESHOLD} km/h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relecture du stream pour le second pipeline\n",
    "df_silver_stream_2 = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(SILVER_ML_PATH)\n",
    "\n",
    "df_anomalies = df_silver_stream_2 \\\n",
    "    .withColumn(\n",
    "        \"is_altitude_anomaly\",\n",
    "        when(\n",
    "            (col(\"altitude_meters\") > ALTITUDE_MAX_THRESHOLD) | \n",
    "            (col(\"altitude_meters\") < ALTITUDE_MIN_THRESHOLD),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"is_velocity_anomaly\",\n",
    "        when(\n",
    "            (col(\"velocity_kmh\") > VELOCITY_MAX_THRESHOLD) | \n",
    "            (col(\"velocity_kmh\") < VELOCITY_MIN_THRESHOLD),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    ) \\\n",
    "    .withWatermark(\"event_timestamp\", \"6 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_timestamp\"), \"5 minutes\", \"1 minute\"),\n",
    "        col(\"origin_country\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_observations\"),\n",
    "        count(when(col(\"is_altitude_anomaly\") == 1, 1)).alias(\"altitude_anomalies\"),\n",
    "        count(when(col(\"is_velocity_anomaly\") == 1, 1)).alias(\"velocity_anomalies\"),\n",
    "        spark_max(\"altitude_meters\").alias(\"max_altitude\"),\n",
    "        spark_min(\"altitude_meters\").alias(\"min_altitude\"),\n",
    "        spark_max(\"velocity_kmh\").alias(\"max_velocity\"),\n",
    "        avg(\"altitude_meters\").alias(\"avg_altitude\"),\n",
    "        avg(\"velocity_kmh\").alias(\"avg_velocity\"),\n",
    "        stddev(\"altitude_meters\").alias(\"stddev_altitude\"),\n",
    "        stddev(\"velocity_kmh\").alias(\"stddev_velocity\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"anomaly_rate\",\n",
    "        (col(\"altitude_anomalies\") + col(\"velocity_anomalies\")) / col(\"total_observations\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"origin_country\"),\n",
    "        col(\"total_observations\"),\n",
    "        col(\"altitude_anomalies\"),\n",
    "        col(\"velocity_anomalies\"),\n",
    "        col(\"anomaly_rate\"),\n",
    "        col(\"max_altitude\"),\n",
    "        col(\"min_altitude\"),\n",
    "        col(\"max_velocity\"),\n",
    "        col(\"avg_altitude\"),\n",
    "        col(\"avg_velocity\"),\n",
    "        col(\"stddev_altitude\"),\n",
    "        col(\"stddev_velocity\")\n",
    "    )\n",
    "\n",
    "print(\"Stream 2: Alertes anomalies par pays (Sliding Window 5 min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_anomalies = df_anomalies.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_ANOMALIES) \\\n",
    "    .start(GOLD_ANOMALIES_PATH)\n",
    "\n",
    "print(f\"Stream 2 démarré -> {GOLD_ANOMALIES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Monitoring des streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Monitoring des streams Gold (Ctrl+C pour arrêter)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        print(f\"\\n{time.strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Aggregations: {query_aggregations.status}\")\n",
    "        print(f\"  Anomalies:    {query_anomalies.status}\")\n",
    "        time.sleep(30)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nArrêt demandé...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Arrêt des streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_aggregations.stop()\n",
    "query_anomalies.stop()\n",
    "print(\"Tous les streams Gold arrêtés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Vérification des données Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistiques Gold:\")\n",
    "\n",
    "try:\n",
    "    df_agg = spark.read.format(\"delta\").load(GOLD_AGGREGATIONS_PATH)\n",
    "    print(f\"  Aggregations: {df_agg.count():,} lignes\")\n",
    "    print(\"\\n  Dernières agrégations par phase:\")\n",
    "    df_agg.orderBy(col(\"window_start\").desc()).limit(10).show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"  Aggregations: Table non disponible ({e})\")\n",
    "\n",
    "try:\n",
    "    df_anom = spark.read.format(\"delta\").load(GOLD_ANOMALIES_PATH)\n",
    "    print(f\"\\n  Anomalies: {df_anom.count():,} lignes\")\n",
    "    print(\"\\n  Pays avec le plus d'anomalies:\")\n",
    "    df_anom.filter(col(\"anomaly_rate\") > 0) \\\n",
    "        .orderBy(col(\"anomaly_rate\").desc()) \\\n",
    "        .limit(10).show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"  Anomalies: Table non disponible ({e})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
