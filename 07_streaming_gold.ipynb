{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 07 - Streaming Gold\n",
    "\n",
    "Agrégations temps réel avec fenêtres temporelles sur les données Silver_ML.\n",
    "\n",
    "**Requêtes implémentées:**\n",
    "1. **Comptage par phase de vol** - Tumbling window 1 minute\n",
    "2. **Alertes anomalies** - Sliding window 5 minutes (slide 1 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Session 'StreamingGold' configurée\n",
      "Input:  s3a://datalake/silver/flights_ml\n",
      "Output Aggregations: s3a://datalake/gold/phase_stats\n",
      "Output Anomalies:    s3a://datalake/gold/country_stats\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, window, count, avg, stddev, max as spark_max, min as spark_min,\n",
    "    when, lit, current_timestamp\n",
    ")\n",
    "from config import get_s3_path, create_spark_session\n",
    "\n",
    "SILVER_ML_PATH = get_s3_path(\"silver\", \"flights_ml\")\n",
    "GOLD_AGGREGATIONS_PATH = get_s3_path(\"gold\", \"phase_stats\")\n",
    "GOLD_ANOMALIES_PATH = get_s3_path(\"gold\", \"country_stats\")\n",
    "CHECKPOINT_AGGREGATIONS = get_s3_path(\"checkpoints\", \"gold_aggregations\")\n",
    "CHECKPOINT_ANOMALIES = get_s3_path(\"checkpoints\", \"gold_anomalies\")\n",
    "\n",
    "spark = create_spark_session(\"StreamingGold\")\n",
    "\n",
    "print(f\"Input:  {SILVER_ML_PATH}\")\n",
    "print(f\"Output Aggregations: {GOLD_AGGREGATIONS_PATH}\")\n",
    "print(f\"Output Anomalies:    {GOLD_ANOMALIES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Lecture du stream Silver_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream Silver_ML initialisé\n",
      "Colonnes: ['event_timestamp', 'icao24', 'callsign', 'origin_country', 'longitude', 'latitude', 'velocity_kmh', 'altitude_meters', 'on_ground', 'category', 'prev_altitude', 'prev_velocity', 'altitude_change', 'velocity_change', 'observation_rank', 'airport_icao', 'airport_name', 'airport_country', 'rolling_avg_altitude', 'rolling_std_altitude', 'rolling_avg_velocity', 'flight_phase']\n"
     ]
    }
   ],
   "source": [
    "df_silver_stream = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(SILVER_ML_PATH)\n",
    "\n",
    "print(f\"Stream Silver_ML initialisé\")\n",
    "print(f\"Colonnes: {df_silver_stream.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Stream 1 : Comptage par phase de vol (Tumbling Window 1 min)\n",
    "\n",
    "Agrégation temps réel du nombre de vols par phase (CLIMB, CRUISE, DESCENT, etc.) avec une fenêtre tumbling de 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream 1: Comptage par phase de vol (Tumbling Window 1 min)\n"
     ]
    }
   ],
   "source": [
    "df_phase_counts = df_silver_stream \\\n",
    "    .withWatermark(\"event_timestamp\", \"2 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_timestamp\"), \"1 minute\"),\n",
    "        col(\"flight_phase\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"flight_count\"),\n",
    "        avg(\"altitude_meters\").alias(\"avg_altitude\"),\n",
    "        avg(\"velocity_kmh\").alias(\"avg_velocity\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"flight_phase\"),\n",
    "        col(\"flight_count\"),\n",
    "        col(\"avg_altitude\"),\n",
    "        col(\"avg_velocity\")\n",
    "    )\n",
    "\n",
    "print(\"Stream 1: Comptage par phase de vol (Tumbling Window 1 min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 15:44:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream 1 démarré -> s3a://datalake/gold/phase_stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query_aggregations = df_phase_counts.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_AGGREGATIONS) \\\n",
    "    .start(GOLD_AGGREGATIONS_PATH)\n",
    "\n",
    "print(f\"Stream 1 démarré -> {GOLD_AGGREGATIONS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Stream 2 : Alertes anomalies par pays (Sliding Window 5 min, slide 1 min)\n",
    "\n",
    "Détection de vitesses et altitudes anormales par pays d'origine avec une fenêtre glissante de 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seuils d'anomalie:\n",
      "  Altitude: -100m - 12000m\n",
      "  Vitesse:  0 - 1000 km/h\n"
     ]
    }
   ],
   "source": [
    "# Seuils d'anomalie\n",
    "ALTITUDE_MAX_THRESHOLD = 12000  # mètres\n",
    "VELOCITY_MAX_THRESHOLD = 1000   # km/h\n",
    "ALTITUDE_MIN_THRESHOLD = -100   # mètres (sous le niveau de la mer)\n",
    "VELOCITY_MIN_THRESHOLD = 0      # km/h\n",
    "\n",
    "print(f\"Seuils d'anomalie:\")\n",
    "print(f\"  Altitude: {ALTITUDE_MIN_THRESHOLD}m - {ALTITUDE_MAX_THRESHOLD}m\")\n",
    "print(f\"  Vitesse:  {VELOCITY_MIN_THRESHOLD} - {VELOCITY_MAX_THRESHOLD} km/h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream 2: Alertes anomalies par pays (Sliding Window 5 min)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 15:44:33 WARN HDFSBackedStateStoreProvider: The state for version 10 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 15:44:33 WARN HDFSBackedStateStoreProvider: The state for version 10 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 15:44:33 WARN HDFSBackedStateStoreProvider: The state for version 10 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 15:44:33 WARN HDFSBackedStateStoreProvider: The state for version 10 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 15:44:33 WARN HDFSBackedStateStoreProvider: The state for version 10 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 15:44:33 WARN HDFSBackedStateStoreProvider: The state for version 10 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 15:44:33 WARN HDFSBackedStateStoreProvider: The state for version 10 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 15:44:33 WARN HDFSBackedStateStoreProvider: The state for version 10 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 15:44:35 WARN HDFSBackedStateStoreProvider: The state for version 10 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 15:44:35 WARN HDFSBackedStateStoreProvider: The state for version 10 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 15:44:38 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Relecture du stream pour le second pipeline\n",
    "df_silver_stream_2 = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(SILVER_ML_PATH)\n",
    "\n",
    "df_anomalies = df_silver_stream_2 \\\n",
    "    .withColumn(\n",
    "        \"is_altitude_anomaly\",\n",
    "        when(\n",
    "            (col(\"altitude_meters\") > ALTITUDE_MAX_THRESHOLD) | \n",
    "            (col(\"altitude_meters\") < ALTITUDE_MIN_THRESHOLD),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"is_velocity_anomaly\",\n",
    "        when(\n",
    "            (col(\"velocity_kmh\") > VELOCITY_MAX_THRESHOLD) | \n",
    "            (col(\"velocity_kmh\") < VELOCITY_MIN_THRESHOLD),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    ) \\\n",
    "    .withWatermark(\"event_timestamp\", \"6 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_timestamp\"), \"5 minutes\", \"1 minute\"),\n",
    "        col(\"origin_country\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_observations\"),\n",
    "        count(when(col(\"is_altitude_anomaly\") == 1, 1)).alias(\"altitude_anomalies\"),\n",
    "        count(when(col(\"is_velocity_anomaly\") == 1, 1)).alias(\"velocity_anomalies\"),\n",
    "        spark_max(\"altitude_meters\").alias(\"max_altitude\"),\n",
    "        spark_min(\"altitude_meters\").alias(\"min_altitude\"),\n",
    "        spark_max(\"velocity_kmh\").alias(\"max_velocity\"),\n",
    "        avg(\"altitude_meters\").alias(\"avg_altitude\"),\n",
    "        avg(\"velocity_kmh\").alias(\"avg_velocity\"),\n",
    "        stddev(\"altitude_meters\").alias(\"stddev_altitude\"),\n",
    "        stddev(\"velocity_kmh\").alias(\"stddev_velocity\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"anomaly_rate\",\n",
    "        (col(\"altitude_anomalies\") + col(\"velocity_anomalies\")) / col(\"total_observations\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"origin_country\"),\n",
    "        col(\"total_observations\"),\n",
    "        col(\"altitude_anomalies\"),\n",
    "        col(\"velocity_anomalies\"),\n",
    "        col(\"anomaly_rate\"),\n",
    "        col(\"max_altitude\"),\n",
    "        col(\"min_altitude\"),\n",
    "        col(\"max_velocity\"),\n",
    "        col(\"avg_altitude\"),\n",
    "        col(\"avg_velocity\"),\n",
    "        col(\"stddev_altitude\"),\n",
    "        col(\"stddev_velocity\")\n",
    "    )\n",
    "\n",
    "print(\"Stream 2: Alertes anomalies par pays (Sliding Window 5 min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 15:44:59 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream 2 démarré -> s3a://datalake/gold/country_stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 15:45:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 22:>                (0 + 8) / 10][Stage 25:>                (0 + 0) / 10]\r"
     ]
    }
   ],
   "source": [
    "query_anomalies = df_anomalies.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_ANOMALIES) \\\n",
    "    .start(GOLD_ANOMALIES_PATH)\n",
    "\n",
    "print(f\"Stream 2 démarré -> {GOLD_ANOMALIES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Monitoring des streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:=============>  (42 + 8) / 50][Stage 91:>                 (0 + 0) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring des streams Gold (Ctrl+C pour arrêter)\n",
      "============================================================\n",
      "\n",
      "15:46:40\n",
      "  Aggregations: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'No new data but cleaning up state', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "\n",
      "Arrêt demandé...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:===============>(49 + 1) / 50][Stage 91:>                 (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"Monitoring des streams Gold (Ctrl+C pour arrêter)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        print(f\"\\n{time.strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Aggregations: {query_aggregations.status}\")\n",
    "        print(f\"  Anomalies:    {query_anomalies.status}\")\n",
    "        time.sleep(30)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nArrêt demandé...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Arrêt des streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tous les streams Gold arrêtés\n"
     ]
    }
   ],
   "source": [
    "query_aggregations.stop()\n",
    "query_anomalies.stop()\n",
    "print(\"Tous les streams Gold arrêtés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Vérification des données Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiques Gold:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Aggregations: 95 lignes\n",
      "\n",
      "  Dernières agrégations par phase:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+------------------+------------------+\n",
      "|window_start       |window_end         |flight_phase|flight_count|avg_altitude      |avg_velocity      |\n",
      "+-------------------+-------------------+------------+------------+------------------+------------------+\n",
      "|2026-01-23 15:42:00|2026-01-23 15:43:00|TRANSITION  |27118       |6832.430376908976 |609.5445397890696 |\n",
      "|2026-01-23 15:42:00|2026-01-23 15:43:00|GROUND      |64          |233.3625025600195 |14.790625000000002|\n",
      "|2026-01-23 15:41:00|2026-01-23 15:42:00|TRANSITION  |27044       |6834.77202292639  |609.3469730809036 |\n",
      "|2026-01-23 15:41:00|2026-01-23 15:42:00|GROUND      |68          |430.97823266422046|5.1635294117647055|\n",
      "|2026-01-23 15:40:00|2026-01-23 15:41:00|GROUND      |69          |234.01130753669185|5.86376811594203  |\n",
      "|2026-01-23 15:40:00|2026-01-23 15:41:00|TRANSITION  |27045       |6836.639210580109 |609.2889850249575 |\n",
      "|2026-01-23 15:40:00|2026-01-23 15:41:00|CRUISE      |4           |10894.695068359375|895.9875          |\n",
      "|2026-01-23 15:39:00|2026-01-23 15:40:00|GROUND      |97          |267.79979772666064|8.788659793814432 |\n",
      "|2026-01-23 15:39:00|2026-01-23 15:40:00|TRANSITION  |36001       |6837.308891624998 |609.4104544318216 |\n",
      "|2026-01-23 15:38:00|2026-01-23 15:39:00|CLIMB       |1077        |6427.402776691575 |639.412618384401  |\n",
      "+-------------------+-------------------+------------+------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Anomalies: 2,820 lignes\n",
      "\n",
      "  Pays avec le plus d'anomalies:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+------------------+------------------+------------------+------------+------------+------------+------------+------------------+------------------+------------------+------------------+\n",
      "|window_start       |window_end         |origin_country      |total_observations|altitude_anomalies|velocity_anomalies|anomaly_rate|max_altitude|min_altitude|max_velocity|avg_altitude      |avg_velocity      |stddev_altitude   |stddev_velocity   |\n",
      "+-------------------+-------------------+--------------------+------------------+------------------+------------------+------------+------------+------------+------------+------------------+------------------+------------------+------------------+\n",
      "|2026-01-23 14:59:00|2026-01-23 15:04:00|Nigeria             |11                |11                |0                 |1.0         |12192.0     |12184.38    |817.88      |12191.307262073864|813.6290909090909 |2.297551779043264 |2.897169151242152 |\n",
      "|2026-01-23 15:35:00|2026-01-23 15:40:00|Uganda              |9                 |9                 |0                 |1.0         |12496.8     |12496.8     |874.69      |12496.7998046875  |872.0033333333334 |0.0               |1.3148859266111408|\n",
      "|2026-01-23 15:34:00|2026-01-23 15:39:00|Uganda              |5                 |5                 |0                 |1.0         |12496.8     |12496.8     |874.69      |12496.7998046875  |872.5459999999999 |0.0               |1.486902821303377 |\n",
      "|2026-01-23 15:35:00|2026-01-23 15:40:00|Lebanon             |9                 |9                 |0                 |1.0         |12496.8     |12496.8     |960.91      |12496.7998046875  |955.8922222222221 |0.0               |3.9241488815339847|\n",
      "|2026-01-23 15:33:00|2026-01-23 15:38:00|Uganda              |2                 |2                 |0                 |1.0         |12496.8     |12496.8     |872.89      |12496.7998046875  |872.01            |0.0               |1.2445079348883172|\n",
      "|2026-01-23 14:50:00|2026-01-23 14:55:00|Nigeria             |4                 |4                 |0                 |1.0         |12199.62    |12192.0     |787.61      |12193.905029296875|785.205           |3.81005859375     |2.9672040711754626|\n",
      "|2026-01-23 15:06:00|2026-01-23 15:11:00|Angola              |7                 |7                 |0                 |1.0         |12192.0     |12184.38    |844.67      |12190.911411830357|838.9871428571429 |2.8801335770419922|2.5059044560511747|\n",
      "|2026-01-23 14:57:00|2026-01-23 15:02:00|Nigeria             |2                 |2                 |0                 |1.0         |12192.0     |12192.0     |814.75      |12192.0           |814.245           |0.0               |0.7141778489984065|\n",
      "|2026-01-23 14:56:00|2026-01-23 15:01:00|Nigeria             |4                 |4                 |0                 |1.0         |12192.0     |12192.0     |809.21      |12192.0           |804.4575          |0.0               |4.03385155073083  |\n",
      "|2026-01-23 14:10:00|2026-01-23 14:15:00|Syrian Arab Republic|7                 |0                 |7                 |1.0         |9448.8      |8968.74     |1016.21     |9293.134207589286 |1011.2342857142858|205.78014957679787|3.9260236760317526|\n",
      "+-------------------+-------------------+--------------------+------------------+------------------+------------------+------------+------------+------------+------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 15:48:00 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/23 15:49:15 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/23 15:52:39 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "[Stage 285:============>   (8 + 2) / 10][Stage 286:>                (0 + 5) / 5]\r"
     ]
    }
   ],
   "source": [
    "print(\"Statistiques Gold:\")\n",
    "\n",
    "try:\n",
    "    df_agg = spark.read.format(\"delta\").load(GOLD_AGGREGATIONS_PATH)\n",
    "    print(f\"  Aggregations: {df_agg.count():,} lignes\")\n",
    "    print(\"\\n  Dernières agrégations par phase:\")\n",
    "    df_agg.orderBy(col(\"window_start\").desc()).limit(10).show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"  Aggregations: Table non disponible ({e})\")\n",
    "\n",
    "try:\n",
    "    df_anom = spark.read.format(\"delta\").load(GOLD_ANOMALIES_PATH)\n",
    "    print(f\"\\n  Anomalies: {df_anom.count():,} lignes\")\n",
    "    print(\"\\n  Pays avec le plus d'anomalies:\")\n",
    "    df_anom.filter(col(\"anomaly_rate\") > 0) \\\n",
    "        .orderBy(col(\"anomaly_rate\").desc()) \\\n",
    "        .limit(10).show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"  Anomalies: Table non disponible ({e})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
