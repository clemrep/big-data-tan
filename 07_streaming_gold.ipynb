{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 07 - Streaming Gold\n",
    "\n",
    "Agrégations temps réel avec fenêtres temporelles sur les données Silver_ML.\n",
    "\n",
    "**Requêtes implémentées:**\n",
    "1. **Comptage par phase de vol** - Tumbling window 1 minute\n",
    "2. **Alertes anomalies** - Sliding window 5 minutes (slide 1 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration chargée depuis .env\n",
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.spark#spark-hadoop-cloud_2.12 added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3f6c74fb-6da3-4939-a364-6be49204c6c7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.spark#spark-hadoop-cloud_2.12;3.5.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound com.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.14 in central\n",
      "\tfound joda-time#joda-time;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.15.2 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.14 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.16 in central\n",
      "\tfound commons-codec#commons-codec;1.16.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.4 in central\n",
      "\tfound com.microsoft.azure#azure-storage;7.0.1 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.0.0 in central\n",
      "\tfound com.google.guava#guava;14.0.1 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-cloud-storage;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-aliyun;3.3.4 in central\n",
      "\tfound com.aliyun.oss#aliyun-sdk-oss;3.13.0 in central\n",
      "\tfound org.jdom#jdom2;2.0.6 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound stax#stax-api;1.0.1 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-core;4.5.10 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.ini4j#ini4j;0.5.4 in central\n",
      "\tfound io.opentracing#opentracing-api;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-util;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-noop;0.33.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-ram;3.1.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-kms;2.11.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure-datalake;3.3.4 in central\n",
      "\tfound com.microsoft.azure#azure-data-lake-store-sdk;2.3.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-openstack;3.3.4 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.54.v20240208 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.54.v20240208 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 621ms :: artifacts dl 23ms\n",
      "\t:: modules in use:\n",
      "\tcom.aliyun#aliyun-java-sdk-core;4.5.10 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-kms;2.11.0 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-ram;3.1.0 from central in [default]\n",
      "\tcom.aliyun.oss#aliyun-sdk-oss;3.13.0 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.15.2 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.14 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.guava#guava;14.0.1 from central in [default]\n",
      "\tcom.microsoft.azure#azure-data-lake-store-sdk;2.3.9 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.16.1 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\tio.opentracing#opentracing-api;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-noop;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-util;0.33.0 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tjoda-time#joda-time;2.12.5 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aliyun;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure-datalake;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-cloud-storage;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-openstack;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.14 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.16 from central in [default]\n",
      "\torg.apache.spark#spark-hadoop-cloud_2.12;3.5.3 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.54.v20240208 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.54.v20240208 from central in [default]\n",
      "\torg.ini4j#ini4j;0.5.4 from central in [default]\n",
      "\torg.jdom#jdom2;2.0.6 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\tstax#stax-api;1.0.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 by [org.eclipse.jetty#jetty-util-ajax;9.4.54.v20240208] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   50  |   0   |   0   |   1   ||   49  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3f6c74fb-6da3-4939-a364-6be49204c6c7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 49 already retrieved (0kB/8ms)\n",
      "26/01/23 14:07:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/23 14:07:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "26/01/23 14:07:02 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "26/01/23 14:07:02 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "26/01/23 14:07:02 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "26/01/23 14:07:02 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Session 'StreamingGold' configurée\n",
      "Input:  s3a://datalake/silver/flights_ml\n",
      "Output Aggregations: s3a://datalake/gold/streaming_aggregations/flight_phase_counts\n",
      "Output Anomalies:    s3a://datalake/gold/streaming_aggregations/anomaly_alerts\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, window, count, avg, stddev, max as spark_max, min as spark_min,\n",
    "    when, lit, current_timestamp\n",
    ")\n",
    "from config import get_s3_path, create_spark_session\n",
    "\n",
    "SILVER_ML_PATH = get_s3_path(\"silver\", \"flights_ml\")\n",
    "GOLD_AGGREGATIONS_PATH = get_s3_path(\"gold\", \"streaming_aggregations\", \"flight_phase_counts\")\n",
    "GOLD_ANOMALIES_PATH = get_s3_path(\"gold\", \"streaming_aggregations\", \"anomaly_alerts\")\n",
    "CHECKPOINT_AGGREGATIONS = get_s3_path(\"checkpoints\", \"gold_aggregations\")\n",
    "CHECKPOINT_ANOMALIES = get_s3_path(\"checkpoints\", \"gold_anomalies\")\n",
    "\n",
    "spark = create_spark_session(\"StreamingGold\")\n",
    "\n",
    "print(f\"Input:  {SILVER_ML_PATH}\")\n",
    "print(f\"Output Aggregations: {GOLD_AGGREGATIONS_PATH}\")\n",
    "print(f\"Output Anomalies:    {GOLD_ANOMALIES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Lecture du stream Silver_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 14:07:07 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream Silver_ML initialisé\n",
      "Colonnes: ['event_timestamp', 'icao24', 'callsign', 'origin_country', 'longitude', 'latitude', 'velocity_kmh', 'altitude_meters', 'on_ground', 'category', 'prev_altitude', 'prev_velocity', 'altitude_change', 'velocity_change', 'observation_rank', 'airport_icao', 'airport_name', 'airport_country', 'rolling_avg_altitude', 'rolling_std_altitude', 'rolling_avg_velocity', 'flight_phase']\n"
     ]
    }
   ],
   "source": [
    "df_silver_stream = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(SILVER_ML_PATH)\n",
    "\n",
    "print(f\"Stream Silver_ML initialisé\")\n",
    "print(f\"Colonnes: {df_silver_stream.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Stream 1 : Comptage par phase de vol (Tumbling Window 1 min)\n",
    "\n",
    "Agrégation temps réel du nombre de vols par phase (CLIMB, CRUISE, DESCENT, etc.) avec une fenêtre tumbling de 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream 1: Comptage par phase de vol (Tumbling Window 1 min)\n"
     ]
    }
   ],
   "source": [
    "df_phase_counts = df_silver_stream \\\n",
    "    .withWatermark(\"event_timestamp\", \"2 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_timestamp\"), \"1 minute\"),\n",
    "        col(\"flight_phase\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"flight_count\"),\n",
    "        avg(\"altitude_meters\").alias(\"avg_altitude\"),\n",
    "        avg(\"velocity_kmh\").alias(\"avg_velocity\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"flight_phase\"),\n",
    "        col(\"flight_count\"),\n",
    "        col(\"avg_altitude\"),\n",
    "        col(\"avg_velocity\")\n",
    "    )\n",
    "\n",
    "print(\"Stream 1: Comptage par phase de vol (Tumbling Window 1 min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream 1 démarré -> s3a://datalake/gold/streaming_aggregations/flight_phase_counts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 14:07:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query_aggregations = df_phase_counts.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_AGGREGATIONS) \\\n",
    "    .start(GOLD_AGGREGATIONS_PATH)\n",
    "\n",
    "print(f\"Stream 1 démarré -> {GOLD_AGGREGATIONS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Stream 2 : Alertes anomalies par pays (Sliding Window 5 min, slide 1 min)\n",
    "\n",
    "Détection de vitesses et altitudes anormales par pays d'origine avec une fenêtre glissante de 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seuils d'anomalie:\n",
      "  Altitude: -100m - 12000m\n",
      "  Vitesse:  0 - 1000 km/h\n"
     ]
    }
   ],
   "source": [
    "# Seuils d'anomalie\n",
    "ALTITUDE_MAX_THRESHOLD = 12000  # mètres\n",
    "VELOCITY_MAX_THRESHOLD = 1000   # km/h\n",
    "ALTITUDE_MIN_THRESHOLD = -100   # mètres (sous le niveau de la mer)\n",
    "VELOCITY_MIN_THRESHOLD = 0      # km/h\n",
    "\n",
    "print(f\"Seuils d'anomalie:\")\n",
    "print(f\"  Altitude: {ALTITUDE_MIN_THRESHOLD}m - {ALTITUDE_MAX_THRESHOLD}m\")\n",
    "print(f\"  Vitesse:  {VELOCITY_MIN_THRESHOLD} - {VELOCITY_MAX_THRESHOLD} km/h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream 2: Alertes anomalies par pays (Sliding Window 5 min)\n"
     ]
    }
   ],
   "source": [
    "# Relecture du stream pour le second pipeline\n",
    "df_silver_stream_2 = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(SILVER_ML_PATH)\n",
    "\n",
    "df_anomalies = df_silver_stream_2 \\\n",
    "    .withColumn(\n",
    "        \"is_altitude_anomaly\",\n",
    "        when(\n",
    "            (col(\"altitude_meters\") > ALTITUDE_MAX_THRESHOLD) | \n",
    "            (col(\"altitude_meters\") < ALTITUDE_MIN_THRESHOLD),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"is_velocity_anomaly\",\n",
    "        when(\n",
    "            (col(\"velocity_kmh\") > VELOCITY_MAX_THRESHOLD) | \n",
    "            (col(\"velocity_kmh\") < VELOCITY_MIN_THRESHOLD),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    ) \\\n",
    "    .withWatermark(\"event_timestamp\", \"6 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_timestamp\"), \"5 minutes\", \"1 minute\"),\n",
    "        col(\"origin_country\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_observations\"),\n",
    "        count(when(col(\"is_altitude_anomaly\") == 1, 1)).alias(\"altitude_anomalies\"),\n",
    "        count(when(col(\"is_velocity_anomaly\") == 1, 1)).alias(\"velocity_anomalies\"),\n",
    "        spark_max(\"altitude_meters\").alias(\"max_altitude\"),\n",
    "        spark_min(\"altitude_meters\").alias(\"min_altitude\"),\n",
    "        spark_max(\"velocity_kmh\").alias(\"max_velocity\"),\n",
    "        avg(\"altitude_meters\").alias(\"avg_altitude\"),\n",
    "        avg(\"velocity_kmh\").alias(\"avg_velocity\"),\n",
    "        stddev(\"altitude_meters\").alias(\"stddev_altitude\"),\n",
    "        stddev(\"velocity_kmh\").alias(\"stddev_velocity\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"anomaly_rate\",\n",
    "        (col(\"altitude_anomalies\") + col(\"velocity_anomalies\")) / col(\"total_observations\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"origin_country\"),\n",
    "        col(\"total_observations\"),\n",
    "        col(\"altitude_anomalies\"),\n",
    "        col(\"velocity_anomalies\"),\n",
    "        col(\"anomaly_rate\"),\n",
    "        col(\"max_altitude\"),\n",
    "        col(\"min_altitude\"),\n",
    "        col(\"max_velocity\"),\n",
    "        col(\"avg_altitude\"),\n",
    "        col(\"avg_velocity\"),\n",
    "        col(\"stddev_altitude\"),\n",
    "        col(\"stddev_velocity\")\n",
    "    )\n",
    "\n",
    "print(\"Stream 2: Alertes anomalies par pays (Sliding Window 5 min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream 2 démarré -> s3a://datalake/gold/streaming_aggregations/anomaly_alerts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 14:07:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query_anomalies = df_anomalies.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_ANOMALIES) \\\n",
    "    .start(GOLD_ANOMALIES_PATH)\n",
    "\n",
    "print(f\"Stream 2 démarré -> {GOLD_ANOMALIES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Monitoring des streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring des streams Gold (Ctrl+C pour arrêter)\n",
      "============================================================\n",
      "\n",
      "14:07:11\n",
      "  Aggregations: {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 14:07:11 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14:07:41\n",
      "  Aggregations: {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "\n",
      "14:08:11\n",
      "  Aggregations: {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'Waiting for data to arrive', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14:08:41\n",
      "  Aggregations: {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "\n",
      "14:09:11\n",
      "  Aggregations: {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "\n",
      "14:09:41\n",
      "  Aggregations: {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14:10:11\n",
      "  Aggregations: {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14:10:41\n",
      "  Aggregations: {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14:11:11\n",
      "  Aggregations: {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17045:>              (0 + 5) / 5][Stage 17047:>              (0 + 3) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14:11:42\n",
      "  Aggregations: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 14:11:48 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:48 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:48 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:49 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:49 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:49 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:49 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:49 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:51 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:51 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:51 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:51 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:51 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:52 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:52 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:53 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:53 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:53 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:53 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "26/01/23 14:11:54 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14:12:13\n",
      "  Aggregations: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 14:12:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 17071:>                                                      (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14:12:44\n",
      "  Aggregations: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17088:================================>                     (6 + 4) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14:13:15\n",
      "  Aggregations: {'message': 'No new data but cleaning up state', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14:13:45\n",
      "  Aggregations: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14:14:15\n",
      "  Aggregations: {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "\n",
      "14:14:45\n",
      "  Aggregations: {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Anomalies:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/silver/flights_ml]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "\n",
      "Arrêt demandé...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"Monitoring des streams Gold (Ctrl+C pour arrêter)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        print(f\"\\n{time.strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Aggregations: {query_aggregations.status}\")\n",
    "        print(f\"  Anomalies:    {query_anomalies.status}\")\n",
    "        time.sleep(30)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nArrêt demandé...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Arrêt des streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_aggregations.stop()\n",
    "query_anomalies.stop()\n",
    "print(\"Tous les streams Gold arrêtés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Vérification des données Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiques Gold:\n",
      "  Aggregations: 14 lignes\n",
      "\n",
      "  Dernières agrégations par phase:\n",
      "+-------------------+-------------------+------------+------------+------------------+------------------+\n",
      "|window_start       |window_end         |flight_phase|flight_count|avg_altitude      |avg_velocity      |\n",
      "+-------------------+-------------------+------------+------------+------------------+------------------+\n",
      "|2026-01-23 13:37:00|2026-01-23 13:38:00|TRANSITION  |8008        |2314.071631850777 |349.0629907592404 |\n",
      "|2026-01-23 13:37:00|2026-01-23 13:38:00|TAKEOFF     |677         |1539.0936806423801|399.10004431314627|\n",
      "|2026-01-23 13:37:00|2026-01-23 13:38:00|DESCENT     |3788        |4783.215619616674 |580.8134345300947 |\n",
      "|2026-01-23 13:37:00|2026-01-23 13:38:00|GROUND      |84          |412.3871455873762 |7.297142857142856 |\n",
      "|2026-01-23 13:37:00|2026-01-23 13:38:00|CLIMB       |3361        |6125.106081411548 |648.9094346920566 |\n",
      "|2026-01-23 13:37:00|2026-01-23 13:38:00|CRUISE      |13600       |10789.541237362133|807.982291911763  |\n",
      "|2026-01-23 13:36:00|2026-01-23 13:37:00|TRANSITION  |14039       |2458.0440657100953|363.788012678966  |\n",
      "|2026-01-23 13:36:00|2026-01-23 13:37:00|TAKEOFF     |1358        |1541.1702512536028|388.3564138438878 |\n",
      "|2026-01-23 13:36:00|2026-01-23 13:37:00|GROUND      |152         |627.2463157302454 |16.145921052631586|\n",
      "|2026-01-23 13:36:00|2026-01-23 13:37:00|CRUISE      |23590       |10789.362144024546|807.6348329800735 |\n",
      "+-------------------+-------------------+------------+------------+------------------+------------------+\n",
      "\n",
      "\n",
      "  Anomalies: 699 lignes\n",
      "\n",
      "  Pays avec le plus d'anomalies:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------+------------------+------------------+------------------+------------+------------+------------+------------+------------------+-----------------+------------------+------------------+\n",
      "|window_start       |window_end         |origin_country|total_observations|altitude_anomalies|velocity_anomalies|anomaly_rate|max_altitude|min_altitude|max_velocity|avg_altitude      |avg_velocity     |stddev_altitude   |stddev_velocity   |\n",
      "+-------------------+-------------------+--------------+------------------+------------------+------------------+------------+------------+------------+------------+------------------+-----------------+------------------+------------------+\n",
      "|2026-01-23 13:34:00|2026-01-23 13:39:00|Ethiopia      |48                |12                |12                |0.5         |12001.5     |10355.58    |1120.1      |11304.27001953125 |910.0291666666666|610.1307357183057 |139.1286301024501 |\n",
      "|2026-01-23 13:31:00|2026-01-23 13:36:00|Lebanon       |4                 |2                 |0                 |0.5         |12496.8     |3688.08     |942.73      |10144.125         |875.5025         |4313.374842034954 |89.86651077199637 |\n",
      "|2026-01-23 13:33:00|2026-01-23 13:38:00|Ethiopia      |48                |12                |12                |0.5         |12001.5     |10355.58    |1120.1      |11304.27001953125 |910.0291666666666|610.1307357183057 |139.1286301024501 |\n",
      "|2026-01-23 13:32:00|2026-01-23 13:37:00|Lebanon       |32                |16                |0                 |0.5         |12496.8     |3185.16     |943.52      |10064.591201782227|861.5865625000001|3932.647533484539 |105.19604094136763|\n",
      "|2026-01-23 13:35:00|2026-01-23 13:40:00|Lebanon       |48                |24                |0                 |0.5         |12496.8     |3048.0      |943.52      |10042.524943033854|853.3585416666668|3949.3855781494344|119.74601147339087|\n",
      "|2026-01-23 13:31:00|2026-01-23 13:36:00|Ethiopia      |4                 |1                 |1                 |0.5         |12001.5     |10355.58    |1117.12     |11302.364990234375|909.2075         |697.6185304997011 |158.2212757237997 |\n",
      "|2026-01-23 13:37:00|2026-01-23 13:42:00|Ethiopia      |16                |4                 |4                 |0.5         |12001.5     |10355.58    |1120.1      |11303.793762207031|910.979375       |624.6415464769667 |141.5566437611342 |\n",
      "|2026-01-23 13:36:00|2026-01-23 13:41:00|Ethiopia      |44                |11                |11                |0.5         |12001.5     |10355.58    |1120.1      |11304.443204012785|910.1038636363635|610.6833098294206 |139.32258003633498|\n",
      "|2026-01-23 13:32:00|2026-01-23 13:37:00|Ethiopia      |32                |8                 |8                 |0.5         |12001.5     |10355.58    |1120.1      |11304.50814819336 |909.5540624999999|612.8604488294246 |140.18105092273498|\n",
      "|2026-01-23 13:37:00|2026-01-23 13:42:00|Lebanon       |16                |8                 |0                 |0.5         |12496.8     |3048.0      |942.73      |9998.39242553711  |836.9024999999999|4111.815684182645 |147.05699396265834|\n",
      "+-------------------+-------------------+--------------+------------------+------------------+------------------+------------+------------+------------+------------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Statistiques Gold:\")\n",
    "\n",
    "try:\n",
    "    df_agg = spark.read.format(\"delta\").load(GOLD_AGGREGATIONS_PATH)\n",
    "    print(f\"  Aggregations: {df_agg.count():,} lignes\")\n",
    "    print(\"\\n  Dernières agrégations par phase:\")\n",
    "    df_agg.orderBy(col(\"window_start\").desc()).limit(10).show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"  Aggregations: Table non disponible ({e})\")\n",
    "\n",
    "try:\n",
    "    df_anom = spark.read.format(\"delta\").load(GOLD_ANOMALIES_PATH)\n",
    "    print(f\"\\n  Anomalies: {df_anom.count():,} lignes\")\n",
    "    print(\"\\n  Pays avec le plus d'anomalies:\")\n",
    "    df_anom.filter(col(\"anomaly_rate\") > 0) \\\n",
    "        .orderBy(col(\"anomaly_rate\").desc()) \\\n",
    "        .limit(10).show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"  Anomalies: Table non disponible ({e})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
