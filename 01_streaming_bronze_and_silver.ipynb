{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales - Chargement depuis .env\n",
    "\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from kafka import KafkaProducer\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "KAFKA_BOOTSTRAP = os.getenv(\"KAFKA_BOOTSTRAP\", \"kafka1:9092\")\n",
    "TOPIC_NAME = os.getenv(\"TOPIC_NAME\", \"opensky-data\")\n",
    "\n",
    "GARAGE_ENDPOINT = os.getenv(\"GARAGE_ENDPOINT\", \"http://garage:3900\")\n",
    "GARAGE_HOST = os.getenv(\"GARAGE_HOST\", \"garage\")\n",
    "GARAGE_PORT = os.getenv(\"GARAGE_PORT\", \"3900\")\n",
    "ACCESS_KEY = os.getenv(\"ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\", \"datalake\")\n",
    "\n",
    "API_BASE_URL = os.getenv(\"API_BASE_URL\", \"https://opensky-network.org/api/states/all\")\n",
    "\n",
    "if not ACCESS_KEY or not SECRET_KEY:\n",
    "    raise ValueError(\"‚ùå ACCESS_KEY et SECRET_KEY doivent √™tre d√©finis dans le fichier .env\")\n",
    "\n",
    "print(\"‚úÖ Configuration charg√©e depuis .env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de Spark avec r√©gion\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, BooleanType, LongType\n",
    "\n",
    "# 1. Packages\n",
    "packages = [\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    \"com.amazonaws:aws-java-sdk-bundle:1.12.262\",\n",
    "    \"org.apache.spark:spark-hadoop-cloud_2.12:3.5.3\",\n",
    "    \"io.delta:delta-spark_2.12:3.0.0\",\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\"\n",
    "]\n",
    "\n",
    "# 2. Configuration S3\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OpenSkyFinal\") \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", GARAGE_ENDPOINT) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint.region\", \"garage\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.committer.name\", \"filesystem\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\", \"false\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"‚úÖ Spark Session configur√©e (R√©gion 'garage' forc√©e).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6386c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification du bucket\n",
    "from minio import Minio\n",
    "\n",
    "client = Minio(\n",
    "    f\"{GARAGE_HOST}:{GARAGE_PORT}\",\n",
    "    access_key=ACCESS_KEY,\n",
    "    secret_key=SECRET_KEY,\n",
    "    secure=False,\n",
    "    region=\"garage\"\n",
    ")\n",
    "\n",
    "if not client.bucket_exists(BUCKET_NAME):\n",
    "    print(f\"‚ö†Ô∏è Le bucket '{BUCKET_NAME}' n'existait pas, cr√©ation en cours...\")\n",
    "    client.make_bucket(BUCKET_NAME)\n",
    "else:\n",
    "    print(f\"‚úÖ Le bucket '{BUCKET_NAME}' existe d√©j√†.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread producer qui envoie les donn√©es √† Kafka depuis l'API OpenSky\n",
    "stop_producer = False\n",
    "\n",
    "def run_producer():\n",
    "    print(f\"üöÄ [THREAD PRODUCER] D√©marrage vers {KAFKA_BOOTSTRAP}...\")\n",
    "    \n",
    "    try:\n",
    "        producer = KafkaProducer(\n",
    "            bootstrap_servers=KAFKA_BOOTSTRAP,\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå [THREAD PRODUCER] Erreur connexion Kafka: {e}\")\n",
    "        return\n",
    "\n",
    "    api_url = \"https://opensky-network.org/api/states/all\"\n",
    "\n",
    "    while not stop_producer:\n",
    "        try:\n",
    "            response = requests.get(api_url)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                states = data.get('states') or []\n",
    "                timestamp = data['time']\n",
    "\n",
    "                for s in states:\n",
    "                    record = {\n",
    "                        \"time\": timestamp,\n",
    "                        \"icao24\": s[0],\n",
    "                        \"callsign\": s[1].strip() if s[1] else None,\n",
    "                        \"origin_country\": s[2],\n",
    "                        \"time_position\": s[3],\n",
    "                        \"last_contact\": s[4],\n",
    "                        \"longitude\": s[5],\n",
    "                        \"latitude\": s[6],\n",
    "                        \"baro_altitude\": s[7],\n",
    "                        \"on_ground\": s[8],\n",
    "                        \"velocity\": s[9],\n",
    "                        \"true_track\": s[10],\n",
    "                        \"vertical_rate\": s[11],\n",
    "                        \"geo_altitude\": s[13],\n",
    "                        \"squawk\": s[14],\n",
    "                        \"spi\": s[15],\n",
    "                        \"position_source\": s[16],\n",
    "                        \"category\": s[17] if len(s) > 17 else None\n",
    "                    }\n",
    "                    producer.send(TOPIC_NAME, record)\n",
    "                \n",
    "                producer.flush()\n",
    "                print(f\"üì° [THREAD PRODUCER] {len(states)} vols envoy√©s √† {datetime.now().strftime('%H:%M:%S')}\")\n",
    "            \n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è [THREAD PRODUCER] API Status: {response.status_code}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è [THREAD PRODUCER] Erreur: {e}\")\n",
    "\n",
    "        # Pause de 15s pour l'API\n",
    "        time.sleep(15)\n",
    "    \n",
    "    print(\"üõë [THREAD PRODUCER] Arr√™t√©.\")\n",
    "\n",
    "# Lancement du Thread\n",
    "producer_thread = threading.Thread(target=run_producer, daemon=True)\n",
    "producer_thread.start()\n",
    "print(\"‚úÖ Le Producer tourne en arri√®re-plan ! Passez √† la suite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9aedbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utiliser pour arr√™ter le thread du producer\n",
    "stop_producer = True\n",
    "print(\"Demande d'arr√™t du producer envoy√©e.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a4230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. D√©finition du sch√©ma\n",
    "schema = StructType([\n",
    "    StructField(\"time\", LongType(), True),\n",
    "    StructField(\"icao24\", StringType(), True),\n",
    "    StructField(\"callsign\", StringType(), True),\n",
    "    StructField(\"origin_country\", StringType(), True),\n",
    "    StructField(\"time_position\", LongType(), True),\n",
    "    StructField(\"last_contact\", LongType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"baro_altitude\", FloatType(), True),\n",
    "    StructField(\"on_ground\", BooleanType(), True),\n",
    "    StructField(\"velocity\", FloatType(), True),\n",
    "    StructField(\"true_track\", FloatType(), True),\n",
    "    StructField(\"vertical_rate\", FloatType(), True),\n",
    "    StructField(\"geo_altitude\", FloatType(), True),\n",
    "    StructField(\"squawk\", StringType(), True),\n",
    "    StructField(\"spi\", BooleanType(), True),\n",
    "    StructField(\"position_source\", IntegerType(), True),\n",
    "    StructField(\"category\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# 2. Lecture Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP) \\\n",
    "    .option(\"subscribe\", TOPIC_NAME) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# 3. Parsing\n",
    "parsed_df = kafka_df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "# 4. √âcriture S3 (Bronze)\n",
    "checkpoint_path = f\"s3a://{BUCKET_NAME}/checkpoints/bronze_flights\"\n",
    "output_path = f\"s3a://{BUCKET_NAME}/bronze/flights\"\n",
    "\n",
    "print(f\"üöÄ D√©marrage du Stream Spark vers {output_path}...\")\n",
    "\n",
    "query = parsed_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .start(output_path)\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_unixtime, to_timestamp, when, round\n",
    "\n",
    "INPUT_BRONZE_PATH = f\"s3a://{BUCKET_NAME}/bronze/flights\"\n",
    "OUTPUT_SILVER_PATH = f\"s3a://{BUCKET_NAME}/silver/flights\"\n",
    "CHECKPOINT_SILVER = f\"s3a://{BUCKET_NAME}/checkpoints/silver_flights\"\n",
    "\n",
    "# 1. Lecture en Streaming depuis la table Bronze (Delta)\n",
    "# Spark surveille le dossier Bronze. D√®s qu'un fichier arrive, il le prend.\n",
    "print(\"üìñ Lecture du flux Bronze...\")\n",
    "df_bronze = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(INPUT_BRONZE_PATH)\n",
    "\n",
    "# 2. Transformations (Nettoyage & Enrichissement)\n",
    "df_silver = df_bronze \\\n",
    "    .filter(col(\"icao24\").isNotNull()) \\\n",
    "    .filter(col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull()) \\\n",
    "    .withColumn(\"event_timestamp\", to_timestamp(from_unixtime(col(\"time\")))) \\\n",
    "    .withColumn(\"velocity_kmh\", round(col(\"velocity\") * 3.6, 2)) \\\n",
    "    .withColumn(\"altitude_meters\", col(\"baro_altitude\")) \\\n",
    "    .select(\n",
    "        \"event_timestamp\",\n",
    "        \"icao24\",\n",
    "        \"callsign\",\n",
    "        \"origin_country\",\n",
    "        \"longitude\",\n",
    "        \"latitude\",\n",
    "        \"velocity_kmh\",\n",
    "        \"altitude_meters\",\n",
    "        \"on_ground\",\n",
    "        \"category\"\n",
    "    )\n",
    "\n",
    "# 3. √âcriture en Streaming vers Silver (Delta)\n",
    "print(f\"üöÄ D√©marrage du Stream vers Silver : {OUTPUT_SILVER_PATH}\")\n",
    "query_silver = df_silver.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_SILVER) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(OUTPUT_SILVER_PATH)\n",
    "\n",
    "query_silver.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6869d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les 5 premi√®res lignes de la table Silver\n",
    "print(\"üîç Aper√ßu de la table Silver :\")\n",
    "spark.read.format(\"delta\").load(OUTPUT_SILVER_PATH).show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
