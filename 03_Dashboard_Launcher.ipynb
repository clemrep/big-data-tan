{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31739641",
   "metadata": {},
   "source": [
    "# 03 - Dashboard Launcher\n",
    "\n",
    "**Zero Spark** notebook that launches the production Streamlit dashboard.\n",
    "\n",
    "Responsibilities:\n",
    "- Verify infrastructure readiness (Garage, Kafka, Spark)\n",
    "- Launch Streamlit app on port 8501\n",
    "- Health checks and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8be144",
   "metadata": {},
   "source": [
    "## Run Notes\n",
    "- Start Docker with `docker compose up -d --build` before running this notebook.\n",
    "- Run cells top-to-bottom: config, infra checks, launch, wait, monitor, shutdown.\n",
    "- Infra check now aborts early if Kafka/Spark/Garage stays unreachable after retries.\n",
    "- Streamlit stdout/stderr is piped to notebook logs to troubleshoot missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa324ce5",
   "metadata": {},
   "source": [
    "## Verify Bucket Data\n",
    "Verify Gold tables in bucket before launching dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2eca9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-23 18:13:33,805 - INFO - ==================================================\n",
      "2026-01-23 18:13:33,806 - INFO - BUCKET DATA VERIFICATION\n",
      "2026-01-23 18:13:33,808 - INFO - ==================================================\n",
      "2026-01-23 18:13:33,816 - ERROR - ERROR: Bucket check failed: An error occurred (400) when calling the HeadBucket operation: Bad Request\n",
      "2026-01-23 18:13:33,817 - INFO - \n",
      "Objects in 'datalake':\n",
      "2026-01-23 18:13:33,837 - INFO -   bronze/flights/_delta_log/00000000000000000000.json (      2242 bytes)\n",
      "2026-01-23 18:13:33,838 - INFO -   bronze/flights/_delta_log/00000000000000000001.json (      3091 bytes)\n",
      "2026-01-23 18:13:33,839 - INFO -   bronze/flights/_delta_log/00000000000000000002.json (      3100 bytes)\n",
      "2026-01-23 18:13:33,840 - INFO -   bronze/flights/_delta_log/00000000000000000003.json (      3092 bytes)\n",
      "2026-01-23 18:13:33,840 - INFO -   bronze/flights/_delta_log/00000000000000000004.json (      3101 bytes)\n",
      "2026-01-23 18:13:33,841 - INFO -   bronze/flights/_delta_log/00000000000000000005.json (      3088 bytes)\n",
      "2026-01-23 18:13:33,842 - INFO -   bronze/flights/_delta_log/00000000000000000006.json (      3103 bytes)\n",
      "2026-01-23 18:13:33,842 - INFO -   bronze/flights/_delta_log/00000000000000000007.json (      3085 bytes)\n",
      "2026-01-23 18:13:33,843 - INFO -   bronze/flights/_delta_log/00000000000000000008.json (      3101 bytes)\n",
      "2026-01-23 18:13:33,843 - INFO -   bronze/flights/_delta_log/00000000000000000009.json (      3086 bytes)\n",
      "2026-01-23 18:13:33,844 - INFO -   bronze/flights/_delta_log/00000000000000000010.checkpoint.parquet (     28028 bytes)\n",
      "2026-01-23 18:13:33,845 - INFO -   bronze/flights/_delta_log/00000000000000000010.json (      3104 bytes)\n",
      "2026-01-23 18:13:33,845 - INFO -   bronze/flights/_delta_log/00000000000000000011.json (      3092 bytes)\n",
      "2026-01-23 18:13:33,846 - INFO -   bronze/flights/_delta_log/00000000000000000012.json (      3107 bytes)\n",
      "2026-01-23 18:13:33,846 - INFO -   bronze/flights/_delta_log/00000000000000000013.json (      3087 bytes)\n",
      "2026-01-23 18:13:33,847 - INFO -   bronze/flights/_delta_log/00000000000000000014.json (      3105 bytes)\n",
      "2026-01-23 18:13:33,847 - INFO -   bronze/flights/_delta_log/00000000000000000015.json (      3086 bytes)\n",
      "2026-01-23 18:13:33,847 - INFO -   bronze/flights/_delta_log/00000000000000000016.json (      3105 bytes)\n",
      "2026-01-23 18:13:33,848 - INFO -   bronze/flights/_delta_log/00000000000000000017.json (      1824 bytes)\n",
      "2026-01-23 18:13:33,849 - INFO -   bronze/flights/_delta_log/00000000000000000018.json (      3107 bytes)\n",
      "2026-01-23 18:13:33,849 - INFO -   bronze/flights/_delta_log/00000000000000000019.json (      3086 bytes)\n",
      "2026-01-23 18:13:33,850 - INFO -   bronze/flights/_delta_log/00000000000000000020.checkpoint.parquet (     32713 bytes)\n",
      "2026-01-23 18:13:33,850 - INFO -   bronze/flights/_delta_log/00000000000000000020.json (      3106 bytes)\n",
      "2026-01-23 18:13:33,851 - INFO -   bronze/flights/_delta_log/00000000000000000021.json (      3085 bytes)\n",
      "2026-01-23 18:13:33,851 - INFO -   bronze/flights/_delta_log/00000000000000000022.json (      3107 bytes)\n",
      "2026-01-23 18:13:33,852 - INFO -   bronze/flights/_delta_log/00000000000000000023.json (      3083 bytes)\n",
      "2026-01-23 18:13:33,853 - INFO -   bronze/flights/_delta_log/00000000000000000024.json (      3107 bytes)\n",
      "2026-01-23 18:13:33,853 - INFO -   bronze/flights/_delta_log/00000000000000000025.json (      3104 bytes)\n",
      "2026-01-23 18:13:33,854 - INFO -   bronze/flights/_delta_log/00000000000000000026.json (      3087 bytes)\n",
      "2026-01-23 18:13:33,855 - INFO -   bronze/flights/_delta_log/00000000000000000027.json (      3111 bytes)\n",
      "2026-01-23 18:13:33,856 - INFO -   bronze/flights/_delta_log/00000000000000000028.json (      3087 bytes)\n",
      "2026-01-23 18:13:33,856 - INFO -   bronze/flights/_delta_log/00000000000000000029.json (      3111 bytes)\n",
      "2026-01-23 18:13:33,857 - INFO -   bronze/flights/_delta_log/00000000000000000030.checkpoint.parquet (     37761 bytes)\n",
      "2026-01-23 18:13:33,857 - INFO -   bronze/flights/_delta_log/00000000000000000030.json (      3103 bytes)\n",
      "2026-01-23 18:13:33,858 - INFO -   bronze/flights/_delta_log/00000000000000000031.json (      3088 bytes)\n",
      "2026-01-23 18:13:33,858 - INFO -   bronze/flights/_delta_log/00000000000000000032.json (      3111 bytes)\n",
      "2026-01-23 18:13:33,859 - INFO -   bronze/flights/_delta_log/00000000000000000033.json (      3089 bytes)\n",
      "2026-01-23 18:13:33,859 - INFO -   bronze/flights/_delta_log/00000000000000000034.json (      3106 bytes)\n",
      "2026-01-23 18:13:33,860 - INFO -   bronze/flights/_delta_log/00000000000000000035.json (      3084 bytes)\n",
      "2026-01-23 18:13:33,860 - INFO -   bronze/flights/_delta_log/00000000000000000036.json (      3109 bytes)\n",
      "2026-01-23 18:13:33,862 - INFO -   bronze/flights/_delta_log/00000000000000000037.json (      3098 bytes)\n",
      "2026-01-23 18:13:33,863 - INFO -   bronze/flights/_delta_log/00000000000000000038.json (      3084 bytes)\n",
      "2026-01-23 18:13:33,864 - INFO -   bronze/flights/_delta_log/00000000000000000039.json (      3110 bytes)\n",
      "2026-01-23 18:13:33,864 - INFO -   bronze/flights/_delta_log/00000000000000000040.checkpoint.parquet (     43970 bytes)\n",
      "2026-01-23 18:13:33,865 - INFO -   bronze/flights/_delta_log/00000000000000000040.json (      3101 bytes)\n",
      "2026-01-23 18:13:33,865 - INFO -   bronze/flights/_delta_log/00000000000000000041.json (      3081 bytes)\n",
      "2026-01-23 18:13:33,866 - INFO -   bronze/flights/_delta_log/00000000000000000042.json (      3108 bytes)\n",
      "2026-01-23 18:13:33,866 - INFO -   bronze/flights/_delta_log/00000000000000000043.json (      3082 bytes)\n",
      "2026-01-23 18:13:33,867 - INFO -   bronze/flights/_delta_log/00000000000000000044.json (      3108 bytes)\n",
      "2026-01-23 18:13:33,868 - INFO -   bronze/flights/_delta_log/00000000000000000045.json (      3109 bytes)\n",
      "2026-01-23 18:13:33,868 - INFO -   bronze/flights/_delta_log/00000000000000000046.json (      3083 bytes)\n",
      "2026-01-23 18:13:33,869 - INFO -   bronze/flights/_delta_log/00000000000000000047.json (      3108 bytes)\n",
      "2026-01-23 18:13:33,869 - INFO -   bronze/flights/_delta_log/00000000000000000048.json (      3087 bytes)\n",
      "2026-01-23 18:13:33,871 - INFO -   bronze/flights/_delta_log/00000000000000000049.json (      3108 bytes)\n",
      "2026-01-23 18:13:33,872 - INFO -   bronze/flights/_delta_log/00000000000000000050.checkpoint.parquet (     48973 bytes)\n",
      "2026-01-23 18:13:33,872 - INFO -   bronze/flights/_delta_log/00000000000000000050.json (      3084 bytes)\n",
      "2026-01-23 18:13:33,873 - INFO -   bronze/flights/_delta_log/00000000000000000051.json (      3108 bytes)\n",
      "2026-01-23 18:13:33,873 - INFO -   bronze/flights/_delta_log/00000000000000000052.json (      3085 bytes)\n",
      "2026-01-23 18:13:33,874 - INFO -   bronze/flights/_delta_log/00000000000000000053.json (      3111 bytes)\n",
      "2026-01-23 18:13:33,875 - INFO -   bronze/flights/_delta_log/00000000000000000054.json (      1825 bytes)\n",
      "2026-01-23 18:13:33,876 - INFO -   bronze/flights/_delta_log/00000000000000000055.json (      3108 bytes)\n",
      "2026-01-23 18:13:33,876 - INFO -   bronze/flights/_delta_log/00000000000000000056.json (      3143 bytes)\n",
      "2026-01-23 18:13:33,877 - INFO -   bronze/flights/_delta_log/00000000000000000057.json (      3114 bytes)\n",
      "2026-01-23 18:13:33,877 - INFO -   bronze/flights/_delta_log/00000000000000000058.json (      3084 bytes)\n",
      "2026-01-23 18:13:33,878 - INFO -   bronze/flights/_delta_log/00000000000000000059.json (      3105 bytes)\n",
      "2026-01-23 18:13:33,878 - INFO -   bronze/flights/_delta_log/00000000000000000060.checkpoint.parquet (     54153 bytes)\n",
      "2026-01-23 18:13:33,879 - INFO -   bronze/flights/_delta_log/00000000000000000060.json (      3105 bytes)\n",
      "2026-01-23 18:13:33,879 - INFO -   bronze/flights/_delta_log/00000000000000000061.json (      3089 bytes)\n",
      "2026-01-23 18:13:33,880 - INFO -   bronze/flights/_delta_log/00000000000000000062.json (      3108 bytes)\n",
      "2026-01-23 18:13:33,880 - INFO -   bronze/flights/_delta_log/00000000000000000063.json (      3111 bytes)\n",
      "2026-01-23 18:13:33,881 - INFO -   bronze/flights/_delta_log/00000000000000000064.json (      3083 bytes)\n",
      "2026-01-23 18:13:33,881 - INFO -   bronze/flights/_delta_log/00000000000000000065.json (      3105 bytes)\n",
      "2026-01-23 18:13:33,882 - INFO -   bronze/flights/_delta_log/00000000000000000066.json (      3087 bytes)\n",
      "2026-01-23 18:13:33,882 - INFO -   bronze/flights/_delta_log/00000000000000000067.json (      3106 bytes)\n",
      "2026-01-23 18:13:33,883 - INFO -   bronze/flights/_delta_log/00000000000000000068.json (      3090 bytes)\n",
      "2026-01-23 18:13:33,884 - INFO -   bronze/flights/_delta_log/00000000000000000069.json (      3110 bytes)\n",
      "2026-01-23 18:13:33,884 - INFO -   bronze/flights/_delta_log/00000000000000000070.checkpoint.parquet (     59780 bytes)\n",
      "2026-01-23 18:13:33,885 - INFO -   bronze/flights/_delta_log/00000000000000000070.json (      3085 bytes)\n",
      "2026-01-23 18:13:33,885 - INFO -   bronze/flights/_delta_log/00000000000000000071.json (      3101 bytes)\n",
      "2026-01-23 18:13:33,886 - INFO -   bronze/flights/_delta_log/00000000000000000072.json (      3086 bytes)\n",
      "2026-01-23 18:13:33,887 - INFO -   bronze/flights/_delta_log/00000000000000000073.json (      3107 bytes)\n",
      "2026-01-23 18:13:33,887 - INFO -   bronze/flights/_delta_log/00000000000000000074.json (      3094 bytes)\n",
      "2026-01-23 18:13:33,888 - INFO -   bronze/flights/_delta_log/00000000000000000075.json (      3106 bytes)\n",
      "2026-01-23 18:13:33,889 - INFO -   bronze/flights/_delta_log/00000000000000000076.json (      3089 bytes)\n",
      "2026-01-23 18:13:33,889 - INFO -   bronze/flights/_delta_log/00000000000000000077.json (      3107 bytes)\n",
      "2026-01-23 18:13:33,890 - INFO -   bronze/flights/_delta_log/00000000000000000078.json (      3086 bytes)\n",
      "2026-01-23 18:13:33,890 - INFO -   bronze/flights/_delta_log/00000000000000000079.json (      3104 bytes)\n",
      "2026-01-23 18:13:33,891 - INFO -   bronze/flights/_delta_log/00000000000000000080.checkpoint.parquet (     65343 bytes)\n",
      "2026-01-23 18:13:33,892 - INFO -   bronze/flights/_delta_log/00000000000000000080.json (      3082 bytes)\n",
      "2026-01-23 18:13:33,892 - INFO -   bronze/flights/_delta_log/00000000000000000081.json (      3106 bytes)\n",
      "2026-01-23 18:13:33,893 - INFO -   bronze/flights/_delta_log/00000000000000000082.json (      3091 bytes)\n",
      "2026-01-23 18:13:33,893 - INFO -   bronze/flights/_delta_log/00000000000000000083.json (      3109 bytes)\n",
      "2026-01-23 18:13:33,894 - INFO -   bronze/flights/_delta_log/00000000000000000084.json (      3085 bytes)\n",
      "2026-01-23 18:13:33,894 - INFO -   bronze/flights/_delta_log/00000000000000000085.json (      3103 bytes)\n",
      "2026-01-23 18:13:33,895 - INFO -   bronze/flights/_delta_log/00000000000000000086.json (      3091 bytes)\n",
      "2026-01-23 18:13:33,895 - INFO -   bronze/flights/_delta_log/00000000000000000087.json (      3084 bytes)\n",
      "2026-01-23 18:13:33,896 - INFO -   bronze/flights/_delta_log/00000000000000000088.json (      3104 bytes)\n",
      "2026-01-23 18:13:33,897 - INFO -   bronze/flights/_delta_log/00000000000000000089.json (      1824 bytes)\n",
      "2026-01-23 18:13:33,897 - INFO -   bronze/flights/_delta_log/00000000000000000090.checkpoint.parquet (     70327 bytes)\n",
      "2026-01-23 18:13:33,898 - INFO -   bronze/flights/_delta_log/00000000000000000090.json (      3106 bytes)\n",
      "2026-01-23 18:13:33,899 - INFO - \n",
      "Gold Layer Status:\n",
      "2026-01-23 18:13:34,020 - ERROR -   ERROR - Traffic         | Generic S3 error: Client error with status 400 Bad\n",
      "2026-01-23 18:13:34,037 - ERROR -   ERROR - Metrics         | Generic S3 error: Client error with status 400 Bad\n",
      "2026-01-23 18:13:34,038 - INFO - \n",
      "==================================================\n",
      "2026-01-23 18:13:34,039 - INFO - Verification complete - Ready to launch dashboard\n",
      "2026-01-23 18:13:34,040 - INFO - ==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "from deltalake import DeltaTable\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "# S3/Garage Configuration\n",
    "garage_endpoint = os.getenv(\"GARAGE_ENDPOINT\", \"http://garage:3900\")\n",
    "access_key = os.getenv(\"ACCESS_KEY\", \"minioadmin\")\n",
    "secret_key = os.getenv(\"SECRET_KEY\", \"minioadmin\")\n",
    "bucket_name = os.getenv(\"BUCKET_NAME\", \"datalake\")\n",
    "\n",
    "storage_options = {\n",
    "    \"AWS_ENDPOINT_URL\": garage_endpoint,\n",
    "    \"AWS_ACCESS_KEY_ID\": access_key,\n",
    "    \"AWS_SECRET_ACCESS_KEY\": secret_key,\n",
    "    \"AWS_REGION\": \"us-east-1\",\n",
    "    \"AWS_S3_ALLOW_UNSAFE_SSL\": \"true\",\n",
    "    \"AWS_ALLOW_HTTP\": \"true\",\n",
    "}\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=garage_endpoint,\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key,\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "logger.info(\"=\" * 50)\n",
    "logger.info(\"BUCKET DATA VERIFICATION\")\n",
    "logger.info(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=bucket_name)\n",
    "    logger.info(f\"OK: Bucket '{bucket_name}' exists\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"ERROR: Bucket check failed: {e}\")\n",
    "\n",
    "logger.info(f\"\\nObjects in '{bucket_name}':\")\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, MaxKeys=100)\n",
    "    if \"Contents\" in response:\n",
    "        for obj in response[\"Contents\"]:\n",
    "            key = obj[\"Key\"]\n",
    "            size = obj[\"Size\"]\n",
    "            logger.info(f\"  {key:50} ({size:>10} bytes)\")\n",
    "    else:\n",
    "        logger.warning(\"  (Empty bucket)\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"  Error listing objects: {e}\")\n",
    "\n",
    "logger.info(f\"\\nGold Layer Status:\")\n",
    "gold_paths = {\n",
    "    \"Traffic\": f\"s3a://{bucket_name}/gold/traffic_by_country\",\n",
    "    \"Metrics\": f\"s3a://{bucket_name}/gold/metrics_by_category\",\n",
    "}\n",
    "\n",
    "for table_name, path in gold_paths.items():\n",
    "    try:\n",
    "        dt = DeltaTable(path, storage_options=storage_options)\n",
    "        df = dt.to_pandas()\n",
    "        record_count = len(df)\n",
    "        cols = list(df.columns)\n",
    "        logger.info(f\"  OK - {table_name:15} | {record_count:5} rows | cols: {', '.join(cols[:3])}...\")\n",
    "    except FileNotFoundError:\n",
    "        logger.warning(f\"  PENDING - {table_name:15} | Not yet created (waiting for data)\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"  ERROR - {table_name:15} | {str(e)[:50]}\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\" * 50)\n",
    "logger.info(\"Verification complete - Ready to launch dashboard\")\n",
    "logger.info(\"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f07315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-23 18:13:37,564 - INFO - Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import socket\n",
    "import threading\n",
    "from typing import Dict, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "SERVICES: Dict[str, str] = {\n",
    "    \"Kafka\": \"tcp://kafka1:9092\",\n",
    "    \"Spark Master\": \"http://spark:8080\",\n",
    "    \"Garage S3\": \"tcp://garage:3903\",\n",
    "}\n",
    "\n",
    "DASHBOARD_PORT = \"8501\"\n",
    "DASHBOARD_URL = f\"http://localhost:{DASHBOARD_PORT}\"\n",
    "\n",
    "logger.info(\"Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3533fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-23 18:13:39,727 - INFO - Starting infrastructure checks (max 10 attempts, 15s intervals)\n",
      "2026-01-23 18:13:39,729 - INFO - Note: Kafka typically takes 30-60s to become ready after container start\n",
      "2026-01-23 18:13:39,731 - INFO - \n",
      "2026-01-23 18:13:39,731 - INFO - Infrastructure Health Check\n",
      "2026-01-23 18:13:39,732 - INFO - ----------------------------------------\n",
      "2026-01-23 18:13:39,735 - INFO - Kafka                OK (TCP)\n",
      "2026-01-23 18:13:40,175 - INFO - Spark Master         OK (200)\n",
      "2026-01-23 18:13:40,177 - INFO - Garage S3            OK (TCP)\n",
      "2026-01-23 18:13:40,178 - INFO - All services healthy\n",
      "2026-01-23 18:13:40,178 - INFO - \n",
      "2026-01-23 18:13:40,179 - INFO - ✓ All infrastructure services ready - proceeding with dashboard launch\n"
     ]
    }
   ],
   "source": [
    "def health_check_service(name: str, endpoint: str) -> Tuple[bool, str]:\n",
    "    \"\"\"Check service connectivity via HTTP or TCP.\"\"\"\n",
    "    # Kafka needs more time to accept connections\n",
    "    timeout = 10 if \"Kafka\" in name else 3\n",
    "    \n",
    "    try:\n",
    "        if endpoint.startswith(\"http://\") or endpoint.startswith(\"https://\"):\n",
    "            response = requests.get(endpoint, timeout=timeout)\n",
    "            if 200 <= response.status_code < 300:\n",
    "                return True, f\"OK ({response.status_code})\"\n",
    "            else:\n",
    "                return False, f\"HTTP {response.status_code}\"\n",
    "        elif endpoint.startswith(\"tcp://\"):\n",
    "            host_port = endpoint.replace(\"tcp://\", \"\")\n",
    "            host, port_str = host_port.split(\":\", 1)\n",
    "            port = int(port_str)\n",
    "            with socket.create_connection((host, port), timeout=timeout):\n",
    "                return True, \"OK (TCP)\"\n",
    "        else:\n",
    "            return False, \"Unsupported scheme\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return False, \"Timeout\"\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return False, \"Unreachable\"\n",
    "    except (socket.timeout, socket.error) as e:\n",
    "        return False, f\"Unreachable ({str(e)[:20]})\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Error: {str(e)[:30]}\"\n",
    "\n",
    "def check_infrastructure() -> bool:\n",
    "    \"\"\"Verify all services are operational.\"\"\"\n",
    "    logger.info(\"Infrastructure Health Check\")\n",
    "    logger.info(\"-\" * 40)\n",
    "    \n",
    "    all_ok = True\n",
    "    for service_name, endpoint in SERVICES.items():\n",
    "        ok, status = health_check_service(service_name, endpoint)\n",
    "        logger.info(f\"{service_name:20} {status}\")\n",
    "        if not ok:\n",
    "            all_ok = False\n",
    "    \n",
    "    if not all_ok:\n",
    "        logger.warning(\"Some services not ready\")\n",
    "        return False\n",
    "    \n",
    "    logger.info(\"All services healthy\")\n",
    "    return True\n",
    "\n",
    "# Kafka takes longer to start - increase retries and wait time\n",
    "max_retries = 10\n",
    "wait_time = 15\n",
    "ready = False\n",
    "\n",
    "logger.info(f\"Starting infrastructure checks (max {max_retries} attempts, {wait_time}s intervals)\")\n",
    "logger.info(\"Note: Kafka typically takes 30-60s to become ready after container start\")\n",
    "logger.info(\"\")\n",
    "\n",
    "for attempt in range(max_retries):\n",
    "    if check_infrastructure():\n",
    "        ready = True\n",
    "        break\n",
    "    if attempt < max_retries - 1:\n",
    "        logger.info(f\"Retry in {wait_time}s (attempt {attempt + 1}/{max_retries})\")\n",
    "        logger.info(\"\")\n",
    "        time.sleep(wait_time)\n",
    "    else:\n",
    "        logger.error(\"Infrastructure check failed after all retries\")\n",
    "\n",
    "if not ready:\n",
    "    logger.error(\"\")\n",
    "    logger.error(\"=\" * 60)\n",
    "    logger.error(\"TROUBLESHOOTING:\")\n",
    "    logger.error(\"1. Verify Docker containers are running: docker ps\")\n",
    "    logger.error(\"2. Check Kafka logs: docker logs big-data-tan-kafka1-1\")\n",
    "    logger.error(\"3. Kafka may need more startup time (wait 60s and retry)\")\n",
    "    logger.error(\"4. Verify network: docker network ls\")\n",
    "    logger.error(\"=\" * 60)\n",
    "    raise SystemExit(\"Infrastructure check failed; aborting dashboard launch\")\n",
    "\n",
    "logger.info(\"\")\n",
    "logger.info(\"✓ All infrastructure services ready - proceeding with dashboard launch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa8afdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-23 18:13:43,053 - INFO - Launching Streamlit Dashboard\n",
      "2026-01-23 18:13:43,060 - INFO - Streamlit started (PID: 1499)\n",
      "2026-01-23 18:13:43,061 - INFO - Environment: BUCKET_NAME=datalake, GARAGE_ENDPOINT=http://garage:3900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-23 18:13:43,937 - INFO - [streamlit] \n",
      "2026-01-23 18:13:43,939 - ERROR - [streamlit] 2026-01-23 18:13:43.939 Starting server...\n",
      "2026-01-23 18:13:43,940 - INFO - [streamlit] Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "2026-01-23 18:13:43,942 - ERROR - [streamlit] 2026-01-23 18:13:43.939 Serving static content from /opt/conda/lib/python3.12/site-packages/streamlit/static\n",
      "2026-01-23 18:13:43,942 - INFO - [streamlit] \n",
      "2026-01-23 18:13:43,946 - ERROR - [streamlit] 2026-01-23 18:13:43.945 Server started on port 8501\n",
      "2026-01-23 18:13:43,947 - ERROR - [streamlit] 2026-01-23 18:13:43.946 Runtime state: RuntimeState.INITIAL -> RuntimeState.NO_SESSIONS_CONNECTED\n",
      "2026-01-23 18:13:44,123 - INFO - [streamlit] \n",
      "2026-01-23 18:13:44,124 - INFO - [streamlit]   You can now view your Streamlit app in your browser.\n",
      "2026-01-23 18:13:44,125 - INFO - [streamlit] \n",
      "2026-01-23 18:13:44,125 - ERROR - [streamlit] 2026-01-23 18:13:44.124 Setting up signal handler\n",
      "2026-01-23 18:13:44,126 - INFO - [streamlit]   URL: http://0.0.0.0:8501\n",
      "2026-01-23 18:13:44,128 - INFO - [streamlit] \n",
      "2026-01-23 18:13:58,503 - ERROR - [streamlit] 2026-01-23 18:13:58.503 No singleton. Registering one.\n",
      "2026-01-23 18:14:00,500 - ERROR - [streamlit] 2026-01-23 18:14:00.500 Watcher created for /home/jovyan/work/pages\n",
      "2026-01-23 18:14:00,517 - ERROR - [streamlit] 2026-01-23 18:14:00.516 Watcher created for /home/jovyan/work/app.py\n",
      "2026-01-23 18:14:00,518 - ERROR - [streamlit] 2026-01-23 18:14:00.517 AppSession initialized (id=b3c75297-d796-4b37-8979-2bbac22cc514)\n",
      "2026-01-23 18:14:00,519 - ERROR - [streamlit] 2026-01-23 18:14:00.517 Created new session for client 139743854550176. Session ID: b3c75297-d796-4b37-8979-2bbac22cc514\n",
      "2026-01-23 18:14:00,519 - ERROR - [streamlit] 2026-01-23 18:14:00.517 Runtime state: RuntimeState.NO_SESSIONS_CONNECTED -> RuntimeState.ONE_OR_MORE_SESSIONS_CONNECTED\n",
      "2026-01-23 18:14:00,520 - ERROR - [streamlit] 2026-01-23 18:14:00.517 Received the following back message:\n",
      "2026-01-23 18:14:00,522 - ERROR - [streamlit] rerun_script {\n",
      "2026-01-23 18:14:00,522 - ERROR - [streamlit]   widget_states {\n",
      "2026-01-23 18:14:00,523 - ERROR - [streamlit]   }\n",
      "2026-01-23 18:14:00,524 - ERROR - [streamlit] }\n",
      "2026-01-23 18:14:00,526 - ERROR - [streamlit] \n",
      "2026-01-23 18:14:00,528 - ERROR - [streamlit] 2026-01-23 18:14:00.518 Beginning script thread\n",
      "2026-01-23 18:14:00,530 - ERROR - [streamlit] 2026-01-23 18:14:00.519 Running script RerunData(widget_states=)\n",
      "2026-01-23 18:14:00,531 - ERROR - [streamlit] 2026-01-23 18:14:00.519 Disconnecting files for session with ID b3c75297-d796-4b37-8979-2bbac22cc514\n",
      "2026-01-23 18:14:00,532 - ERROR - [streamlit] 2026-01-23 18:14:00.519 Sessions still active: dict_keys([])\n",
      "2026-01-23 18:14:00,533 - ERROR - [streamlit] 2026-01-23 18:14:00.519 Files: 0; Sessions with files: 0\n",
      "2026-01-23 18:14:17,051 - ERROR - [streamlit] 2026-01-23 18:14:17.050 Removing orphaned files...\n",
      "2026-01-23 18:14:17,114 - ERROR - [streamlit] 2026-01-23 18:14:17.113 Running script RerunData(page_script_hash='3f41e546893dc64b71aaacad12cad815')\n",
      "2026-01-23 18:14:17,116 - ERROR - [streamlit] 2026-01-23 18:14:17.114 Disconnecting files for session with ID b3c75297-d796-4b37-8979-2bbac22cc514\n",
      "2026-01-23 18:14:17,116 - ERROR - [streamlit] 2026-01-23 18:14:17.114 Sessions still active: dict_keys([])\n",
      "2026-01-23 18:14:17,117 - ERROR - [streamlit] 2026-01-23 18:14:17.114 Files: 0; Sessions with files: 0\n",
      "2026-01-23 18:14:32,246 - ERROR - [streamlit] 2026-01-23 18:14:32.245 Removing orphaned files...\n",
      "2026-01-23 18:14:32,315 - ERROR - [streamlit] 2026-01-23 18:14:32.314 Running script RerunData(page_script_hash='3f41e546893dc64b71aaacad12cad815')\n",
      "2026-01-23 18:14:32,316 - ERROR - [streamlit] 2026-01-23 18:14:32.315 Disconnecting files for session with ID b3c75297-d796-4b37-8979-2bbac22cc514\n",
      "2026-01-23 18:14:32,317 - ERROR - [streamlit] 2026-01-23 18:14:32.315 Sessions still active: dict_keys([])\n",
      "2026-01-23 18:14:32,318 - ERROR - [streamlit] 2026-01-23 18:14:32.315 Files: 0; Sessions with files: 0\n",
      "2026-01-23 18:14:47,388 - ERROR - [streamlit] 2026-01-23 18:14:47.387 Removing orphaned files...\n",
      "2026-01-23 18:14:47,455 - ERROR - [streamlit] 2026-01-23 18:14:47.455 Running script RerunData(page_script_hash='3f41e546893dc64b71aaacad12cad815')\n",
      "2026-01-23 18:14:47,456 - ERROR - [streamlit] 2026-01-23 18:14:47.456 Disconnecting files for session with ID b3c75297-d796-4b37-8979-2bbac22cc514\n",
      "2026-01-23 18:14:47,457 - ERROR - [streamlit] 2026-01-23 18:14:47.456 Sessions still active: dict_keys([])\n",
      "2026-01-23 18:14:47,458 - ERROR - [streamlit] 2026-01-23 18:14:47.456 Files: 0; Sessions with files: 0\n",
      "2026-01-23 18:15:02,594 - ERROR - [streamlit] 2026-01-23 18:15:02.593 Removing orphaned files...\n",
      "2026-01-23 18:15:02,648 - ERROR - [streamlit] 2026-01-23 18:15:02.648 Running script RerunData(page_script_hash='3f41e546893dc64b71aaacad12cad815')\n",
      "2026-01-23 18:15:02,650 - ERROR - [streamlit] 2026-01-23 18:15:02.648 Disconnecting files for session with ID b3c75297-d796-4b37-8979-2bbac22cc514\n",
      "2026-01-23 18:15:02,650 - ERROR - [streamlit] 2026-01-23 18:15:02.648 Sessions still active: dict_keys([])\n",
      "2026-01-23 18:15:02,651 - ERROR - [streamlit] 2026-01-23 18:15:02.648 Files: 0; Sessions with files: 0\n",
      "2026-01-23 18:15:17,703 - ERROR - [streamlit] 2026-01-23 18:15:17.703 Removing orphaned files...\n",
      "2026-01-23 18:15:17,764 - ERROR - [streamlit] 2026-01-23 18:15:17.763 Running script RerunData(page_script_hash='3f41e546893dc64b71aaacad12cad815')\n",
      "2026-01-23 18:15:17,765 - ERROR - [streamlit] 2026-01-23 18:15:17.763 Disconnecting files for session with ID b3c75297-d796-4b37-8979-2bbac22cc514\n",
      "2026-01-23 18:15:17,766 - ERROR - [streamlit] 2026-01-23 18:15:17.763 Sessions still active: dict_keys([])\n",
      "2026-01-23 18:15:17,767 - ERROR - [streamlit] 2026-01-23 18:15:17.763 Files: 0; Sessions with files: 0\n",
      "2026-01-23 18:15:32,882 - ERROR - [streamlit] 2026-01-23 18:15:32.882 Removing orphaned files...\n",
      "2026-01-23 18:15:32,940 - ERROR - [streamlit] 2026-01-23 18:15:32.940 Running script RerunData(page_script_hash='3f41e546893dc64b71aaacad12cad815')\n",
      "2026-01-23 18:15:32,942 - ERROR - [streamlit] 2026-01-23 18:15:32.940 Disconnecting files for session with ID b3c75297-d796-4b37-8979-2bbac22cc514\n",
      "2026-01-23 18:15:32,943 - ERROR - [streamlit] 2026-01-23 18:15:32.940 Sessions still active: dict_keys([])\n",
      "2026-01-23 18:15:32,943 - ERROR - [streamlit] 2026-01-23 18:15:32.940 Files: 0; Sessions with files: 0\n",
      "2026-01-23 18:15:47,993 - ERROR - [streamlit] 2026-01-23 18:15:47.992 Removing orphaned files...\n",
      "2026-01-23 18:15:48,127 - ERROR - [streamlit] 2026-01-23 18:15:48.127 Running script RerunData(page_script_hash='3f41e546893dc64b71aaacad12cad815')\n",
      "2026-01-23 18:15:48,128 - ERROR - [streamlit] 2026-01-23 18:15:48.127 Disconnecting files for session with ID b3c75297-d796-4b37-8979-2bbac22cc514\n",
      "2026-01-23 18:15:48,129 - ERROR - [streamlit] 2026-01-23 18:15:48.128 Sessions still active: dict_keys([])\n",
      "2026-01-23 18:15:48,130 - ERROR - [streamlit] 2026-01-23 18:15:48.128 Files: 0; Sessions with files: 0\n",
      "2026-01-23 18:16:03,228 - ERROR - [streamlit] 2026-01-23 18:16:03.228 Removing orphaned files...\n",
      "2026-01-23 18:16:03,322 - ERROR - [streamlit] 2026-01-23 18:16:03.322 Running script RerunData(page_script_hash='3f41e546893dc64b71aaacad12cad815')\n",
      "2026-01-23 18:16:03,324 - ERROR - [streamlit] 2026-01-23 18:16:03.322 Disconnecting files for session with ID b3c75297-d796-4b37-8979-2bbac22cc514\n",
      "2026-01-23 18:16:03,325 - ERROR - [streamlit] 2026-01-23 18:16:03.322 Sessions still active: dict_keys([])\n",
      "2026-01-23 18:16:03,325 - ERROR - [streamlit] 2026-01-23 18:16:03.322 Files: 0; Sessions with files: 0\n",
      "2026-01-23 18:16:18,362 - ERROR - [streamlit] 2026-01-23 18:16:18.362 Removing orphaned files...\n",
      "2026-01-23 18:16:18,464 - ERROR - [streamlit] 2026-01-23 18:16:18.463 Running script RerunData(page_script_hash='3f41e546893dc64b71aaacad12cad815')\n",
      "2026-01-23 18:16:18,466 - ERROR - [streamlit] 2026-01-23 18:16:18.464 Disconnecting files for session with ID b3c75297-d796-4b37-8979-2bbac22cc514\n",
      "2026-01-23 18:16:18,467 - ERROR - [streamlit] 2026-01-23 18:16:18.464 Sessions still active: dict_keys([])\n",
      "2026-01-23 18:16:18,467 - ERROR - [streamlit] 2026-01-23 18:16:18.464 Files: 0; Sessions with files: 0\n",
      "2026-01-23 18:16:33,615 - ERROR - [streamlit] 2026-01-23 18:16:33.614 Removing orphaned files...\n",
      "2026-01-23 18:16:33,732 - ERROR - [streamlit] 2026-01-23 18:16:33.731 Running script RerunData(page_script_hash='3f41e546893dc64b71aaacad12cad815')\n",
      "2026-01-23 18:16:33,733 - ERROR - [streamlit] 2026-01-23 18:16:33.733 Disconnecting files for session with ID b3c75297-d796-4b37-8979-2bbac22cc514\n",
      "2026-01-23 18:16:33,734 - ERROR - [streamlit] 2026-01-23 18:16:33.733 Sessions still active: dict_keys([])\n",
      "2026-01-23 18:16:33,734 - ERROR - [streamlit] 2026-01-23 18:16:33.733 Files: 0; Sessions with files: 0\n",
      "2026-01-23 18:16:46,131 - INFO - [streamlit]   Stopping...\n",
      "2026-01-23 18:16:46,131 - ERROR - [streamlit] 2026-01-23 18:16:46.131 Runtime stopping...\n",
      "2026-01-23 18:16:46,142 - ERROR - [streamlit] 2026-01-23 18:16:46.131 Runtime state: RuntimeState.ONE_OR_MORE_SESSIONS_CONNECTED -> RuntimeState.STOPPING\n",
      "2026-01-23 18:16:46,145 - ERROR - [streamlit] 2026-01-23 18:16:46.131 Shutting down (id=b3c75297-d796-4b37-8979-2bbac22cc514)\n",
      "2026-01-23 18:16:46,149 - ERROR - [streamlit] 2026-01-23 18:16:46.131 Disconnecting files for session with ID b3c75297-d796-4b37-8979-2bbac22cc514\n",
      "2026-01-23 18:16:46,153 - ERROR - [streamlit] 2026-01-23 18:16:46.131 Sessions still active: dict_keys([])\n",
      "2026-01-23 18:16:46,155 - ERROR - [streamlit] 2026-01-23 18:16:46.131 Files: 0; Sessions with files: 0\n",
      "2026-01-23 18:16:46,160 - ERROR - [streamlit] 2026-01-23 18:16:46.131 Removing orphaned files...\n",
      "2026-01-23 18:16:46,161 - ERROR - [streamlit] 2026-01-23 18:16:46.133 Runtime state: RuntimeState.STOPPING -> RuntimeState.STOPPED\n",
      "2026-01-23 18:16:46,168 - INFO - [streamlit]   Stopping...\n",
      "2026-01-23 18:16:46,168 - ERROR - [streamlit] Exception ignored in: <module 'threading' from '/opt/conda/lib/python3.12/threading.py'>\n",
      "2026-01-23 18:16:46,176 - ERROR - [streamlit] Traceback (most recent call last):\n",
      "2026-01-23 18:16:46,177 - ERROR - [streamlit]   File \"/opt/conda/lib/python3.12/threading.py\", line 1624, in _shutdown\n",
      "2026-01-23 18:16:46,179 - ERROR - [streamlit]     lock.acquire()\n",
      "2026-01-23 18:16:46,180 - ERROR - [streamlit]   File \"/opt/conda/lib/python3.12/site-packages/streamlit/web/bootstrap.py\", line 44, in signal_handler\n",
      "2026-01-23 18:16:46,183 - ERROR - [streamlit]     server.stop()\n",
      "2026-01-23 18:16:46,184 - ERROR - [streamlit]   File \"/opt/conda/lib/python3.12/site-packages/streamlit/web/server/server.py\", line 417, in stop\n",
      "2026-01-23 18:16:46,185 - ERROR - [streamlit]     self._runtime.stop()\n",
      "2026-01-23 18:16:46,187 - ERROR - [streamlit]   File \"/opt/conda/lib/python3.12/site-packages/streamlit/runtime/runtime.py\", line 324, in stop\n",
      "2026-01-23 18:16:46,189 - ERROR - [streamlit]     async_objs.eventloop.call_soon_threadsafe(stop_on_eventloop)\n",
      "2026-01-23 18:16:46,193 - ERROR - [streamlit]   File \"/opt/conda/lib/python3.12/asyncio/base_events.py\", line 844, in call_soon_threadsafe\n",
      "2026-01-23 18:16:46,195 - ERROR - [streamlit]     self._check_closed()\n",
      "2026-01-23 18:16:46,196 - ERROR - [streamlit]   File \"/opt/conda/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
      "2026-01-23 18:16:46,198 - ERROR - [streamlit]     raise RuntimeError('Event loop is closed')\n",
      "2026-01-23 18:16:46,200 - ERROR - [streamlit] RuntimeError: Event loop is closed\n"
     ]
    }
   ],
   "source": [
    "def stream_logs(process: subprocess.Popen) -> None:\n",
    "    \"\"\"Pipe Streamlit stdout/stderr into notebook logs.\"\"\"\n",
    "    def _pipe(stream, level: int) -> None:\n",
    "        for line in iter(stream.readline, \"\"):\n",
    "            if line:\n",
    "                logger.log(level, f\"[streamlit] {line.rstrip()}\")\n",
    "    threading.Thread(target=_pipe, args=(process.stdout, logging.INFO), daemon=True).start()\n",
    "    threading.Thread(target=_pipe, args=(process.stderr, logging.ERROR), daemon=True).start()\n",
    "\n",
    "\n",
    "def launch_streamlit() -> subprocess.Popen:\n",
    "    \"\"\"Launch Streamlit application with live bucket access.\"\"\"\n",
    "    logger.info(\"Launching Streamlit Dashboard\")\n",
    "    \n",
    "    # Ensure environment variables are passed to Streamlit\n",
    "    env = os.environ.copy()\n",
    "    env.update({\n",
    "        \"GARAGE_ENDPOINT\": os.getenv(\"GARAGE_ENDPOINT\", \"http://garage:3900\"),\n",
    "        \"BUCKET_NAME\": os.getenv(\"BUCKET_NAME\", \"datalake\"),\n",
    "        \"ACCESS_KEY\": os.getenv(\"ACCESS_KEY\", \"minioadmin\"),\n",
    "        \"SECRET_KEY\": os.getenv(\"SECRET_KEY\", \"minioadmin\"),\n",
    "    })\n",
    "    \n",
    "    cmd = [\n",
    "        \"streamlit\", \"run\", \"app.py\",\n",
    "        \"--server.port\", DASHBOARD_PORT,\n",
    "        \"--server.address\", \"0.0.0.0\",\n",
    "        \"--client.showErrorDetails\", \"true\",\n",
    "        \"--logger.level\", \"debug\",\n",
    "        \"--client.toolbarMode\", \"minimal\",\n",
    "        \"--server.runOnSave\", \"true\",\n",
    "        \"--server.headless\", \"true\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            cmd,\n",
    "            cwd=\"/home/jovyan/work\",\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            env=env\n",
    "        )\n",
    "        stream_logs(process)\n",
    "        logger.info(f\"Streamlit started (PID: {process.pid})\")\n",
    "        logger.info(f\"Environment: BUCKET_NAME={env.get('BUCKET_NAME')}, GARAGE_ENDPOINT={env.get('GARAGE_ENDPOINT')}\")\n",
    "        return process\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to launch Streamlit: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "dashboard_process = launch_streamlit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd23c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-23 18:13:46,947 - INFO - Waiting for Streamlit on http://localhost:8501...\n",
      "2026-01-23 18:13:47,030 - INFO - Dashboard is running\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Waiting for Streamlit on {DASHBOARD_URL}...\")\n",
    "\n",
    "max_wait = 30\n",
    "elapsed = 0\n",
    "\n",
    "while elapsed < max_wait:\n",
    "    try:\n",
    "        response = requests.get(DASHBOARD_URL, timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            logger.info(\"Dashboard is running\")\n",
    "            break\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        pass\n",
    "    \n",
    "    time.sleep(1)\n",
    "    elapsed += 1\n",
    "\n",
    "if elapsed >= max_wait:\n",
    "    logger.warning(\"Dashboard startup timeout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10db6b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-23 18:13:51,395 - INFO - Dashboard Active - URL: http://localhost:8501\n",
      "2026-01-23 18:13:51,396 - INFO - Spark UI: http://localhost:4040\n",
      "2026-01-23 18:13:51,397 - INFO - Data Lake: datalake\n",
      "2026-01-23 18:13:51,399 - INFO - \n",
      "2026-01-23 18:13:51,400 - INFO - Dashboard pulling live data from:\n",
      "2026-01-23 18:13:51,401 - INFO -   s3a://datalake/gold/traffic_by_country\n",
      "2026-01-23 18:13:51,402 - INFO -   s3a://datalake/gold/metrics_by_category\n",
      "2026-01-23 18:13:51,403 - INFO - \n",
      "2026-01-23 18:14:50,550 - INFO - Dashboard running (60s uptime) - pulling live data\n",
      "2026-01-23 18:15:50,736 - INFO - Dashboard running (120s uptime) - pulling live data\n",
      "2026-01-23 18:16:46,130 - INFO - Shutdown requested\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Dashboard Active - URL: {DASHBOARD_URL}\")\n",
    "logger.info(f\"Spark UI: http://localhost:4040\")\n",
    "logger.info(f\"Data Lake: {os.getenv('BUCKET_NAME', 'datalake')}\")\n",
    "logger.info(\"\")\n",
    "logger.info(\"Dashboard pulling live data from:\")\n",
    "logger.info(f\"  s3a://{os.getenv('BUCKET_NAME', 'datalake')}/gold/traffic_by_country\")\n",
    "logger.info(f\"  s3a://{os.getenv('BUCKET_NAME', 'datalake')}/gold/metrics_by_category\")\n",
    "logger.info(\"\")\n",
    "\n",
    "try:\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        \n",
    "        if dashboard_process.poll() is not None:\n",
    "            logger.error(\"Streamlit process terminated\")\n",
    "            break\n",
    "        \n",
    "        if iteration % 60 == 0:\n",
    "            logger.info(f\"Dashboard running ({iteration}s uptime) - pulling live data\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.info(\"Shutdown requested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deafd585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-23 18:06:48,799 - INFO - Shutting down dashboard\n",
      "2026-01-23 18:06:48,801 - INFO - Process already stopped\n",
      "2026-01-23 18:06:48,801 - INFO - Shutdown complete\n"
     ]
    }
   ],
   "source": [
    "def shutdown_dashboard():\n",
    "    \"\"\"Gracefully stop Streamlit process.\"\"\"\n",
    "    logger.info(\"Shutting down dashboard\")\n",
    "    \n",
    "    try:\n",
    "        if dashboard_process.poll() is None:\n",
    "            logger.info(f\"Terminating process (PID: {dashboard_process.pid})\")\n",
    "            dashboard_process.terminate()\n",
    "            \n",
    "            try:\n",
    "                dashboard_process.wait(timeout=5)\n",
    "                logger.info(\"Streamlit terminated\")\n",
    "            except subprocess.TimeoutExpired:\n",
    "                logger.warning(\"Forcing shutdown\")\n",
    "                dashboard_process.kill()\n",
    "                dashboard_process.wait()\n",
    "        else:\n",
    "            logger.info(\"Process already stopped\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Shutdown error: {str(e)}\")\n",
    "    \n",
    "    logger.info(\"Shutdown complete\")\n",
    "\n",
    "shutdown_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
