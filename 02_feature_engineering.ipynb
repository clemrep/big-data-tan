{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Feature Engineering - Silver to Silver_ML\n",
    "\n",
    "Ce notebook transforme les donnÃ©es Silver en features ML-ready avec :\n",
    "- Nettoyage supplÃ©mentaire\n",
    "- Features temporelles avec Window Functions\n",
    "- Features de fenÃªtre glissante (rolling)\n",
    "- Label de classification (flight_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 1 : Configuration\n",
    "%pip install python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "GARAGE_ENDPOINT = os.getenv(\"GARAGE_ENDPOINT\", \"http://garage:3900\")\n",
    "ACCESS_KEY = os.getenv(\"ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\", \"datalake\")\n",
    "\n",
    "if not ACCESS_KEY or not SECRET_KEY:\n",
    "    raise ValueError(\"âŒ ACCESS_KEY et SECRET_KEY doivent Ãªtre dÃ©finis dans le fichier .env\")\n",
    "\n",
    "# DÃ©finir les chemins\n",
    "SILVER_PATH = f\"s3a://{BUCKET_NAME}/silver/flights\"\n",
    "SILVER_ML_PATH = f\"s3a://{BUCKET_NAME}/silver/flights_ml\"\n",
    "\n",
    "print(\"âœ… Configuration chargÃ©e depuis .env\")\n",
    "print(f\"ðŸ“‚ Silver Path: {SILVER_PATH}\")\n",
    "print(f\"ðŸ“‚ Silver ML Path: {SILVER_ML_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de Spark avec configuration S3/Delta\n",
    "\n",
    "# 1. Packages\n",
    "packages = [\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    \"com.amazonaws:aws-java-sdk-bundle:1.12.262\",\n",
    "    \"org.apache.spark:spark-hadoop-cloud_2.12:3.5.3\",\n",
    "    \"io.delta:delta-spark_2.12:3.0.0\"\n",
    "]\n",
    "\n",
    "# 2. Configuration Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FeatureEngineering_SilverML\") \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", GARAGE_ENDPOINT) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint.region\", \"garage\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.committer.name\", \"filesystem\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\", \"false\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"âœ… Spark Session configurÃ©e pour Feature Engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6386c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 2 : Lecture des donnÃ©es Silver\n",
    "\n",
    "print(f\"ðŸ“– Lecture de la table Delta Silver depuis {SILVER_PATH}...\")\n",
    "df_silver = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "\n",
    "# Afficher le schÃ©ma\n",
    "print(\"\\nðŸ“‹ SchÃ©ma des donnÃ©es Silver :\")\n",
    "df_silver.printSchema()\n",
    "\n",
    "# Afficher le nombre de lignes\n",
    "count = df_silver.count()\n",
    "print(f\"\\nðŸ“Š Nombre total de lignes : {count:,}\")\n",
    "\n",
    "# Afficher un aperÃ§u\n",
    "print(\"\\nðŸ” AperÃ§u des donnÃ©es :\")\n",
    "df_silver.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 3 : Nettoyage supplÃ©mentaire\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"ðŸ§¹ Nettoyage des donnÃ©es...\")\n",
    "\n",
    "# Compter avant nettoyage\n",
    "count_before = df_silver.count()\n",
    "print(f\"Lignes avant nettoyage : {count_before:,}\")\n",
    "\n",
    "# Filtrer les donnÃ©es invalides\n",
    "df_cleaned = df_silver \\\n",
    "    .filter(col(\"icao24\").isNotNull()) \\\n",
    "    .filter(col(\"altitude_meters\").between(-500, 15000)) \\\n",
    "    .filter(col(\"velocity_kmh\").between(0, 1200))\n",
    "\n",
    "# Compter aprÃ¨s nettoyage\n",
    "count_after = df_cleaned.count()\n",
    "removed = count_before - count_after\n",
    "print(f\"Lignes aprÃ¨s nettoyage : {count_after:,}\")\n",
    "print(f\"Lignes supprimÃ©es : {removed:,} ({100 * removed / count_before:.2f}%)\")\n",
    "\n",
    "print(\"\\nâœ… Nettoyage terminÃ©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9aedbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 4 : Features temporelles avec Window Functions\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, lead, avg, stddev, col, when, row_number\n",
    "\n",
    "print(\"ðŸªŸ CrÃ©ation des features temporelles avec Window Functions...\")\n",
    "\n",
    "# FenÃªtre par avion, ordonnÃ©e par timestamp\n",
    "window_aircraft = Window.partitionBy(\"icao24\").orderBy(\"event_timestamp\")\n",
    "\n",
    "# CrÃ©er les features temporelles\n",
    "df_temporal = df_cleaned \\\n",
    "    .withColumn(\"prev_altitude\", lag(\"altitude_meters\", 1).over(window_aircraft)) \\\n",
    "    .withColumn(\"prev_velocity\", lag(\"velocity_kmh\", 1).over(window_aircraft)) \\\n",
    "    .withColumn(\"altitude_change\", col(\"altitude_meters\") - col(\"prev_altitude\")) \\\n",
    "    .withColumn(\"velocity_change\", col(\"velocity_kmh\") - col(\"prev_velocity\")) \\\n",
    "    .withColumn(\"observation_rank\", row_number().over(window_aircraft))\n",
    "\n",
    "print(\"âœ… Features temporelles crÃ©Ã©es :\")\n",
    "print(\"   - prev_altitude: Altitude prÃ©cÃ©dente\")\n",
    "print(\"   - prev_velocity: Vitesse prÃ©cÃ©dente\")\n",
    "print(\"   - altitude_change: Variation d'altitude\")\n",
    "print(\"   - velocity_change: Variation de vitesse\")\n",
    "print(\"   - observation_rank: Rang de l'observation\")\n",
    "\n",
    "print(\"\\nðŸ” AperÃ§u des features temporelles :\")\n",
    "df_temporal.select(\n",
    "    \"icao24\", \"event_timestamp\", \"altitude_meters\", \"prev_altitude\", \n",
    "    \"altitude_change\", \"velocity_kmh\", \"velocity_change\", \"observation_rank\"\n",
    ").orderBy(\"icao24\", \"event_timestamp\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a4230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 5 : Features avec fenÃªtre glissante (Rolling Window)\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, stddev\n",
    "\n",
    "print(\"ðŸ“Š CrÃ©ation des features avec fenÃªtre glissante (rolling window)...\")\n",
    "\n",
    "# FenÃªtre des 5 derniÃ¨res observations par avion\n",
    "rolling_window = Window.partitionBy(\"icao24\").orderBy(\"event_timestamp\").rowsBetween(-5, 0)\n",
    "\n",
    "# CrÃ©er les features rolling\n",
    "df_rolling = df_temporal \\\n",
    "    .withColumn(\"rolling_avg_altitude\", avg(\"altitude_meters\").over(rolling_window)) \\\n",
    "    .withColumn(\"rolling_std_altitude\", stddev(\"altitude_meters\").over(rolling_window)) \\\n",
    "    .withColumn(\"rolling_avg_velocity\", avg(\"velocity_kmh\").over(rolling_window))\n",
    "\n",
    "print(\"âœ… Features rolling crÃ©Ã©es :\")\n",
    "print(\"   - rolling_avg_altitude: Moyenne altitude sur 5 observations\")\n",
    "print(\"   - rolling_std_altitude: Ã‰cart-type altitude sur 5 observations\")\n",
    "print(\"   - rolling_avg_velocity: Moyenne vitesse sur 5 observations\")\n",
    "\n",
    "print(\"\\nðŸ” AperÃ§u des features rolling :\")\n",
    "df_rolling.select(\n",
    "    \"icao24\", \"event_timestamp\", \"altitude_meters\", \"rolling_avg_altitude\", \n",
    "    \"rolling_std_altitude\", \"velocity_kmh\", \"rolling_avg_velocity\"\n",
    ").orderBy(\"icao24\", \"event_timestamp\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 6 : CrÃ©ation du label flight_phase\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "print(\"ðŸ·ï¸  CrÃ©ation du label de classification 'flight_phase'...\")\n",
    "\n",
    "# CrÃ©er le label basÃ© sur des rÃ¨gles mÃ©tier\n",
    "df_ml = df_rolling.withColumn(\n",
    "    \"flight_phase\",\n",
    "    when(col(\"on_ground\") == True, \"GROUND\")\n",
    "    .when((col(\"altitude_change\") > 50) & (col(\"altitude_meters\") < 3000), \"TAKEOFF\")\n",
    "    .when(col(\"altitude_change\") > 20, \"CLIMB\")\n",
    "    .when(col(\"altitude_change\").between(-20, 20) & (col(\"altitude_meters\") > 8000), \"CRUISE\")\n",
    "    .when(col(\"altitude_change\") < -20, \"DESCENT\")\n",
    "    .otherwise(\"TRANSITION\")\n",
    ")\n",
    "\n",
    "print(\"âœ… Label 'flight_phase' crÃ©Ã© avec les catÃ©gories :\")\n",
    "print(\"   - GROUND: Avion au sol\")\n",
    "print(\"   - TAKEOFF: DÃ©collage (altitude < 3000m, montÃ©e rapide)\")\n",
    "print(\"   - CLIMB: MontÃ©e (altitude change > 20m)\")\n",
    "print(\"   - CRUISE: CroisiÃ¨re (altitude stable > 8000m)\")\n",
    "print(\"   - DESCENT: Descente (altitude change < -20m)\")\n",
    "print(\"   - TRANSITION: Autre\")\n",
    "\n",
    "print(\"\\nðŸ“Š Distribution des phases de vol :\")\n",
    "df_ml.groupBy(\"flight_phase\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6869d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 7 : Sauvegarder en Delta (Silver_ML)\n",
    "\n",
    "print(f\"ðŸ’¾ Sauvegarde des donnÃ©es ML dans {SILVER_ML_PATH}...\")\n",
    "\n",
    "df_ml.write.format(\"delta\").mode(\"overwrite\").save(SILVER_ML_PATH)\n",
    "\n",
    "print(f\"âœ… DonnÃ©es ML sauvegardÃ©es dans {SILVER_ML_PATH}\")\n",
    "print(f\"ðŸ“¦ Nombre de lignes sauvegardÃ©es : {df_ml.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 8 : VÃ©rification\n",
    "\n",
    "print(\"ðŸ” VÃ©rification des donnÃ©es Silver_ML...\")\n",
    "\n",
    "# Relire les donnÃ©es pour vÃ©rifier\n",
    "df_verify = spark.read.format(\"delta\").load(SILVER_ML_PATH)\n",
    "\n",
    "# Afficher le schÃ©ma final\n",
    "print(\"\\nðŸ“‹ SchÃ©ma final des donnÃ©es ML :\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "# Compter les lignes par flight_phase\n",
    "print(\"\\nðŸ“Š Distribution des phases de vol :\")\n",
    "phase_distribution = df_verify.groupBy(\"flight_phase\").count().orderBy(\"count\", ascending=False)\n",
    "phase_distribution.show()\n",
    "\n",
    "# Statistiques par phase\n",
    "print(\"\\nðŸ“ˆ Statistiques par phase de vol :\")\n",
    "df_verify.groupBy(\"flight_phase\").agg(\n",
    "    avg(\"altitude_meters\").alias(\"avg_altitude\"),\n",
    "    avg(\"velocity_kmh\").alias(\"avg_velocity\"),\n",
    "    avg(\"altitude_change\").alias(\"avg_altitude_change\")\n",
    ").orderBy(\"flight_phase\").show()\n",
    "\n",
    "# Afficher quelques exemples\n",
    "print(\"\\nðŸ” Exemples de donnÃ©es finales :\")\n",
    "df_verify.select(\n",
    "    \"icao24\", \"event_timestamp\", \"altitude_meters\", \"velocity_kmh\",\n",
    "    \"altitude_change\", \"rolling_avg_altitude\", \"flight_phase\"\n",
    ").show(10, truncate=False)\n",
    "\n",
    "print(\"\\nâœ… VÃ©rification terminÃ©e ! Les donnÃ©es sont prÃªtes pour le Machine Learning.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
