{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Streaming Silver & Silver_ML\n",
    "\n",
    "Pipeline de transformation :\n",
    "- **Bronze â†’ Silver** : Nettoyage et enrichissement\n",
    "- **Bronze â†’ Silver_ML** : Feature engineering pour le Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:25:22.327108Z",
     "start_time": "2026-01-19T16:25:22.019412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration chargÃ©e depuis .env\n",
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.spark#spark-hadoop-cloud_2.12 added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-26149457-e0c0-4430-adba-30f7a9b47922;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.spark#spark-hadoop-cloud_2.12;3.5.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound com.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.14 in central\n",
      "\tfound joda-time#joda-time;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.15.2 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.14 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.16 in central\n",
      "\tfound commons-codec#commons-codec;1.16.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.4 in central\n",
      "\tfound com.microsoft.azure#azure-storage;7.0.1 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.0.0 in central\n",
      "\tfound com.google.guava#guava;14.0.1 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-cloud-storage;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-aliyun;3.3.4 in central\n",
      "\tfound com.aliyun.oss#aliyun-sdk-oss;3.13.0 in central\n",
      "\tfound org.jdom#jdom2;2.0.6 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound stax#stax-api;1.0.1 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-core;4.5.10 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.ini4j#ini4j;0.5.4 in central\n",
      "\tfound io.opentracing#opentracing-api;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-util;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-noop;0.33.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-ram;3.1.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-kms;2.11.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure-datalake;3.3.4 in central\n",
      "\tfound com.microsoft.azure#azure-data-lake-store-sdk;2.3.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-openstack;3.3.4 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.54.v20240208 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.54.v20240208 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 5016ms :: artifacts dl 177ms\n",
      "\t:: modules in use:\n",
      "\tcom.aliyun#aliyun-java-sdk-core;4.5.10 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-kms;2.11.0 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-ram;3.1.0 from central in [default]\n",
      "\tcom.aliyun.oss#aliyun-sdk-oss;3.13.0 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.15.2 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.14 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.guava#guava;14.0.1 from central in [default]\n",
      "\tcom.microsoft.azure#azure-data-lake-store-sdk;2.3.9 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.16.1 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\tio.opentracing#opentracing-api;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-noop;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-util;0.33.0 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tjoda-time#joda-time;2.12.5 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aliyun;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure-datalake;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-cloud-storage;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-openstack;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.14 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.16 from central in [default]\n",
      "\torg.apache.spark#spark-hadoop-cloud_2.12;3.5.3 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.54.v20240208 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.54.v20240208 from central in [default]\n",
      "\torg.ini4j#ini4j;0.5.4 from central in [default]\n",
      "\torg.jdom#jdom2;2.0.6 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\tstax#stax-api;1.0.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 by [org.eclipse.jetty#jetty-util-ajax;9.4.54.v20240208] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   50  |   0   |   0   |   1   ||   49  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-26149457-e0c0-4430-adba-30f7a9b47922\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 49 already retrieved (0kB/90ms)\n",
      "26/01/19 16:27:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark Session 'StreamingSilver' configurÃ©e\n",
      "âœ… Input:     s3a://datalake/bronze/flights\n",
      "âœ… Silver:    s3a://datalake/silver/flights\n",
      "âœ… Silver_ML: s3a://datalake/silver/flights_ml\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, from_unixtime, to_timestamp, round,\n",
    "    lag, avg, stddev, row_number, when, sqrt, pow, lit, min as spark_min, broadcast\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from config import get_s3_path, create_spark_session\n",
    "\n",
    "BRONZE_PATH = get_s3_path(\"bronze\", \"flights\")\n",
    "SILVER_PATH = get_s3_path(\"silver\", \"flights\")\n",
    "SILVER_ML_PATH = get_s3_path(\"silver\", \"flights_ml\")\n",
    "CHECKPOINT_SILVER = get_s3_path(\"checkpoints\", \"silver_flights\")\n",
    "CHECKPOINT_SILVER_ML = get_s3_path(\"checkpoints\", \"silver_ml_flights\")\n",
    "AIRPORTS_CSV = \"./data/airports.csv\"\n",
    "\n",
    "spark = create_spark_session(\"StreamingSilver\")\n",
    "\n",
    "print(f\"âœ… Input:     {BRONZE_PATH}\")\n",
    "print(f\"âœ… Silver:    {SILVER_PATH}\")\n",
    "print(f\"âœ… Silver_ML: {SILVER_ML_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des donnÃ©es aÃ©roports (pour Silver_ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:26:09.818721Z",
     "start_time": "2026-01-19T16:25:58.766040Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 5211 aÃ©roports chargÃ©s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_airports = spark.read.option(\"header\", \"true\").csv(AIRPORTS_CSV).select(\n",
    "    col(\"ident\").alias(\"airport_icao\"),\n",
    "    col(\"name\").alias(\"airport_name\"),\n",
    "    col(\"iso_country\").alias(\"airport_country\"),\n",
    "    col(\"latitude_deg\").cast(\"double\").alias(\"airport_lat\"),\n",
    "    col(\"longitude_deg\").cast(\"double\").alias(\"airport_lon\")\n",
    ").filter(col(\"type\").isin(\"large_airport\", \"medium_airport\"))\n",
    "\n",
    "print(f\"âœ… {df_airports.count()} aÃ©roports chargÃ©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream 1 : Bronze â†’ Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:26:10.624729Z",
     "start_time": "2026-01-19T16:26:09.823686Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/19 16:28:04 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Stream 1: Bronze â†’ Silver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/19 16:28:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "df_bronze_stream = spark.readStream.format(\"delta\").load(BRONZE_PATH)\n",
    "\n",
    "df_silver = df_bronze_stream \\\n",
    "    .filter(col(\"icao24\").isNotNull()) \\\n",
    "    .filter(col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull()) \\\n",
    "    .withColumn(\"event_timestamp\", to_timestamp(from_unixtime(col(\"time\")))) \\\n",
    "    .withColumn(\"velocity_kmh\", round(col(\"velocity\") * 3.6, 2)) \\\n",
    "    .withColumn(\"altitude_meters\", col(\"baro_altitude\")) \\\n",
    "    .select(\n",
    "        \"event_timestamp\", \"icao24\", \"callsign\", \"origin_country\",\n",
    "        \"longitude\", \"latitude\", \"velocity_kmh\", \"altitude_meters\",\n",
    "        \"on_ground\", \"category\"\n",
    "    )\n",
    "\n",
    "print(f\"ðŸš€ Stream 1: Bronze â†’ Silver\")\n",
    "\n",
    "query_silver = df_silver.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_SILVER) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(SILVER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream 2 : Bronze â†’ Silver_ML (Feature Engineering)\n",
    "\n",
    "Transformation avec features pour le ML, directement depuis Bronze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:24:55.935373Z",
     "start_time": "2026-01-19T16:24:55.818025Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_ml_batch(batch_df, batch_id):\n",
    "    \"\"\"Traitement d'un micro-batch pour Silver_ML avec feature engineering.\"\"\"\n",
    "    \n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "    \n",
    "    # Transformation Bronze â†’ format Silver\n",
    "    df_base = batch_df \\\n",
    "        .filter(col(\"icao24\").isNotNull()) \\\n",
    "        .filter(col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull()) \\\n",
    "        .withColumn(\"event_timestamp\", to_timestamp(from_unixtime(col(\"time\")))) \\\n",
    "        .withColumn(\"velocity_kmh\", round(col(\"velocity\") * 3.6, 2)) \\\n",
    "        .withColumn(\"altitude_meters\", col(\"baro_altitude\"))\n",
    "    \n",
    "    # Nettoyage ML\n",
    "    df_clean = df_base \\\n",
    "        .filter(col(\"altitude_meters\").between(-500, 15000)) \\\n",
    "        .filter(col(\"velocity_kmh\").between(0, 1200))\n",
    "    \n",
    "    if df_clean.isEmpty():\n",
    "        return\n",
    "    \n",
    "    # Features temporelles\n",
    "    window_aircraft = Window.partitionBy(\"icao24\").orderBy(\"event_timestamp\")\n",
    "    \n",
    "    df_temporal = df_clean \\\n",
    "        .withColumn(\"prev_altitude\", lag(\"altitude_meters\", 1).over(window_aircraft)) \\\n",
    "        .withColumn(\"prev_velocity\", lag(\"velocity_kmh\", 1).over(window_aircraft)) \\\n",
    "        .withColumn(\"altitude_change\", col(\"altitude_meters\") - col(\"prev_altitude\")) \\\n",
    "        .withColumn(\"velocity_change\", col(\"velocity_kmh\") - col(\"prev_velocity\")) \\\n",
    "        .withColumn(\"observation_rank\", row_number().over(window_aircraft))\n",
    "    \n",
    "    # Jointure aÃ©roports\n",
    "    df_on_ground = df_temporal.filter(col(\"on_ground\") == True)\n",
    "    df_in_flight = df_temporal.filter(col(\"on_ground\") == False)\n",
    "    \n",
    "    if df_on_ground.count() > 0:\n",
    "        df_with_airports = df_on_ground.crossJoin(broadcast(df_airports)).withColumn(\n",
    "            \"dist\", sqrt(pow(col(\"latitude\") - col(\"airport_lat\"), 2) + pow(col(\"longitude\") - col(\"airport_lon\"), 2))\n",
    "        )\n",
    "        \n",
    "        w = Window.partitionBy(\"icao24\", \"event_timestamp\")\n",
    "        df_closest = df_with_airports.withColumn(\"min_dist\", spark_min(\"dist\").over(w)) \\\n",
    "            .filter(col(\"dist\") == col(\"min_dist\")) \\\n",
    "            .drop(\"dist\", \"min_dist\", \"airport_lat\", \"airport_lon\")\n",
    "        \n",
    "        df_enriched = df_closest.unionByName(\n",
    "            df_in_flight.withColumn(\"airport_icao\", lit(None))\n",
    "                        .withColumn(\"airport_name\", lit(None))\n",
    "                        .withColumn(\"airport_country\", lit(None)),\n",
    "            allowMissingColumns=True\n",
    "        )\n",
    "    else:\n",
    "        df_enriched = df_in_flight \\\n",
    "            .withColumn(\"airport_icao\", lit(None)) \\\n",
    "            .withColumn(\"airport_name\", lit(None)) \\\n",
    "            .withColumn(\"airport_country\", lit(None))\n",
    "    \n",
    "    # Features rolling window\n",
    "    rolling_window = Window.partitionBy(\"icao24\").orderBy(\"event_timestamp\").rowsBetween(-5, 0)\n",
    "    \n",
    "    df_rolling = df_enriched \\\n",
    "        .withColumn(\"rolling_avg_altitude\", avg(\"altitude_meters\").over(rolling_window)) \\\n",
    "        .withColumn(\"rolling_std_altitude\", stddev(\"altitude_meters\").over(rolling_window)) \\\n",
    "        .withColumn(\"rolling_avg_velocity\", avg(\"velocity_kmh\").over(rolling_window))\n",
    "    \n",
    "    # Label flight_phase\n",
    "    df_ml = df_rolling.withColumn(\n",
    "        \"flight_phase\",\n",
    "        when(col(\"on_ground\") == True, \"GROUND\")\n",
    "        .when((col(\"altitude_change\") > 50) & (col(\"altitude_meters\") < 3000), \"TAKEOFF\")\n",
    "        .when(col(\"altitude_change\") > 20, \"CLIMB\")\n",
    "        .when(col(\"altitude_change\").between(-20, 20) & (col(\"altitude_meters\") > 8000), \"CRUISE\")\n",
    "        .when(col(\"altitude_change\") < -20, \"DESCENT\")\n",
    "        .otherwise(\"TRANSITION\")\n",
    "    )\n",
    "    \n",
    "    # SÃ©lection des colonnes finales\n",
    "    df_final = df_ml.select(\n",
    "        \"event_timestamp\", \"icao24\", \"callsign\", \"origin_country\",\n",
    "        \"longitude\", \"latitude\", \"velocity_kmh\", \"altitude_meters\",\n",
    "        \"on_ground\", \"category\",\n",
    "        \"prev_altitude\", \"prev_velocity\", \"altitude_change\", \"velocity_change\", \"observation_rank\",\n",
    "        \"airport_icao\", \"airport_name\", \"airport_country\",\n",
    "        \"rolling_avg_altitude\", \"rolling_std_altitude\", \"rolling_avg_velocity\",\n",
    "        \"flight_phase\"\n",
    "    )\n",
    "    \n",
    "    # Ã‰criture\n",
    "    df_final.write.format(\"delta\").mode(\"append\").save(SILVER_ML_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:25:12.757910Z",
     "start_time": "2026-01-19T16:25:08.640557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Stream 2: Bronze â†’ Silver_ML (Feature Engineering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/19 16:28:18 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/19 16:28:21 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 6:>                                                          (0 + 3) / 3]\r"
     ]
    }
   ],
   "source": [
    "df_bronze_ml_stream = spark.readStream.format(\"delta\").load(BRONZE_PATH)\n",
    "\n",
    "print(f\"ðŸš€ Stream 2: Bronze â†’ Silver_ML (Feature Engineering)\")\n",
    "\n",
    "query_silver_ml = df_bronze_ml_stream.writeStream \\\n",
    "    .foreachBatch(process_ml_batch) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_SILVER_ML) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring des streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Monitoring des streams (Ctrl+C pour arrÃªter)\n",
      "============================================================\n",
      "\n",
      "â±ï¸  16:28:29\n",
      "  Silver:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/bronze/flights]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Silver_ML: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â±ï¸  16:28:59\n",
      "  Silver:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/bronze/flights]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Silver_ML: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/19 16:29:02 ERROR MicroBatchExecution: Query [id = 4b23c4f3-3d8a-4c6f-a358-6c3ef9772cad, runId = a18281a3-423a-4bd2-9707-6be94fdcb8f4] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/sql/utils.py\", line 120, in call\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/sql/utils.py\", line 117, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_16249/2349631819.py\", line 90, in process_ml_batch\n",
      "    df_final.write.format(\"delta\").mode(\"append\").save(SILVER_ML_PATH)\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/sql/readwriter.py\", line 1463, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: A schema mismatch detected when writing to the Delta table (Table ID: 7adfb981-b1a2-4365-9db9-1bb112d37551).\n",
      "To enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n",
      "'.option(\"mergeSchema\", \"true\")'.\n",
      "For other operations, set the session configuration\n",
      "spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\n",
      "specific to the operation for details.\n",
      "\n",
      "Table schema:\n",
      "root\n",
      "-- event_timestamp: timestamp (nullable = true)\n",
      "-- icao24: string (nullable = true)\n",
      "-- callsign: string (nullable = true)\n",
      "-- origin_country: string (nullable = true)\n",
      "-- longitude: float (nullable = true)\n",
      "-- latitude: float (nullable = true)\n",
      "-- velocity_kmh: double (nullable = true)\n",
      "-- altitude_meters: float (nullable = true)\n",
      "-- on_ground: boolean (nullable = true)\n",
      "-- category: integer (nullable = true)\n",
      "-- prev_altitude: float (nullable = true)\n",
      "-- prev_velocity: double (nullable = true)\n",
      "-- altitude_change: float (nullable = true)\n",
      "-- velocity_change: double (nullable = true)\n",
      "-- observation_rank: integer (nullable = true)\n",
      "-- rolling_avg_altitude: double (nullable = true)\n",
      "-- rolling_std_altitude: double (nullable = true)\n",
      "-- rolling_avg_velocity: double (nullable = true)\n",
      "-- flight_phase: string (nullable = true)\n",
      "\n",
      "\n",
      "Data schema:\n",
      "root\n",
      "-- event_timestamp: timestamp (nullable = true)\n",
      "-- icao24: string (nullable = true)\n",
      "-- callsign: string (nullable = true)\n",
      "-- origin_country: string (nullable = true)\n",
      "-- longitude: float (nullable = true)\n",
      "-- latitude: float (nullable = true)\n",
      "-- velocity_kmh: double (nullable = true)\n",
      "-- altitude_meters: float (nullable = true)\n",
      "-- on_ground: boolean (nullable = true)\n",
      "-- category: integer (nullable = true)\n",
      "-- prev_altitude: float (nullable = true)\n",
      "-- prev_velocity: double (nullable = true)\n",
      "-- altitude_change: float (nullable = true)\n",
      "-- velocity_change: double (nullable = true)\n",
      "-- observation_rank: integer (nullable = true)\n",
      "-- airport_icao: string (nullable = true)\n",
      "-- airport_name: string (nullable = true)\n",
      "-- airport_country: string (nullable = true)\n",
      "-- rolling_avg_altitude: double (nullable = true)\n",
      "-- rolling_std_altitude: double (nullable = true)\n",
      "-- rolling_avg_velocity: double (nullable = true)\n",
      "-- flight_phase: string (nullable = true)\n",
      "\n",
      "         \n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy55.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â±ï¸  16:29:29\n",
      "  Silver:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/bronze/flights]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Silver_ML: {'message': 'Terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\\n  File \"/opt/conda/lib/python3.12/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\\n    return_value = getattr(self.pool[obj_id], method)(*params)\\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/opt/conda/lib/python3.12/site-packages/pyspark/sql/utils.py\", line 120, in call\\n    raise e\\n  File \"/opt/conda/lib/python3.12/site-packages/pyspark/sql/utils.py\", line 117, in call\\n    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\\n  File \"/tmp/ipykernel_16249/2349631819.py\", line 90, in process_ml_batch\\n    df_final.write.format(\"delta\").mode(\"append\").save(SILVER_ML_PATH)\\n  File \"/opt/conda/lib/python3.12/site-packages/pyspark/sql/readwriter.py\", line 1463, in save\\n    self._jwrite.save(path)\\n  File \"/opt/conda/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__\\n    return_value = get_return_value(\\n                   ^^^^^^^^^^^^^^^^^\\n  File \"/opt/conda/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 185, in deco\\n    raise converted from None\\npyspark.errors.exceptions.captured.AnalysisException: A schema mismatch detected when writing to the Delta table (Table ID: 7adfb981-b1a2-4365-9db9-1bb112d37551).\\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\\n\\'.option(\"mergeSchema\", \"true\")\\'.\\nFor other operations, set the session configuration\\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\\nspecific to the operation for details.\\n\\nTable schema:\\nroot\\n-- event_timestamp: timestamp (nullable = true)\\n-- icao24: string (nullable = true)\\n-- callsign: string (nullable = true)\\n-- origin_country: string (nullable = true)\\n-- longitude: float (nullable = true)\\n-- latitude: float (nullable = true)\\n-- velocity_kmh: double (nullable = true)\\n-- altitude_meters: float (nullable = true)\\n-- on_ground: boolean (nullable = true)\\n-- category: integer (nullable = true)\\n-- prev_altitude: float (nullable = true)\\n-- prev_velocity: double (nullable = true)\\n-- altitude_change: float (nullable = true)\\n-- velocity_change: double (nullable = true)\\n-- observation_rank: integer (nullable = true)\\n-- rolling_avg_altitude: double (nullable = true)\\n-- rolling_std_altitude: double (nullable = true)\\n-- rolling_avg_velocity: double (nullable = true)\\n-- flight_phase: string (nullable = true)\\n\\n\\nData schema:\\nroot\\n-- event_timestamp: timestamp (nullable = true)\\n-- icao24: string (nullable = true)\\n-- callsign: string (nullable = true)\\n-- origin_country: string (nullable = true)\\n-- longitude: float (nullable = true)\\n-- latitude: float (nullable = true)\\n-- velocity_kmh: double (nullable = true)\\n-- altitude_meters: float (nullable = true)\\n-- on_ground: boolean (nullable = true)\\n-- category: integer (nullable = true)\\n-- prev_altitude: float (nullable = true)\\n-- prev_velocity: double (nullable = true)\\n-- altitude_change: float (nullable = true)\\n-- velocity_change: double (nullable = true)\\n-- observation_rank: integer (nullable = true)\\n-- airport_icao: string (nullable = true)\\n-- airport_name: string (nullable = true)\\n-- airport_country: string (nullable = true)\\n-- rolling_avg_altitude: double (nullable = true)\\n-- rolling_std_altitude: double (nullable = true)\\n-- rolling_avg_velocity: double (nullable = true)\\n-- flight_phase: string (nullable = true)\\n\\n         \\n', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "\n",
      "â¹ï¸  ArrÃªt demandÃ©...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"ðŸ“Š Monitoring des streams (Ctrl+C pour arrÃªter)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        print(f\"\\nâ±ï¸  {time.strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Silver:    {query_silver.status}\")\n",
    "        print(f\"  Silver_ML: {query_silver_ml.status}\")\n",
    "        time.sleep(30)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâ¹ï¸  ArrÃªt demandÃ©...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArrÃªt des streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tous les streams arrÃªtÃ©s\n"
     ]
    }
   ],
   "source": [
    "query_silver.stop()\n",
    "query_silver_ml.stop()\n",
    "print(\"âœ… Tous les streams arrÃªtÃ©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VÃ©rification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Statistiques :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Bronze:    27,719 lignes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Silver:    27,513 lignes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Silver_ML: 25,506 lignes\n",
      "\n",
      "ðŸ“Š Distribution flight_phase (Silver_ML) :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.==================>    (46 + 4) / 50]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o339.showString",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Silver_ML: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark.read.format(\u001b[33m'\u001b[39m\u001b[33mdelta\u001b[39m\u001b[33m'\u001b[39m).load(SILVER_ML_PATH).count()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m lignes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ“Š Distribution flight_phase (Silver_ML) :\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdelta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSILVER_ML_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mflight_phase\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcount\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyspark/sql/dataframe.py:947\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    888\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[32m    889\u001b[39m \n\u001b[32m    890\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    945\u001b[39m \u001b[33;03m    name | Bob\u001b[39;00m\n\u001b[32m    946\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyspark/sql/dataframe.py:965\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    959\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    960\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    961\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    962\u001b[39m     )\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/py4j/protocol.py:334\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    330\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    335\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    336\u001b[39m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name))\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28mtype\u001b[39m = answer[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mPy4JError\u001b[39m: An error occurred while calling o339.showString"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Statistiques :\")\n",
    "print(f\"  Bronze:    {spark.read.format('delta').load(BRONZE_PATH).count():,} lignes\")\n",
    "print(f\"  Silver:    {spark.read.format('delta').load(SILVER_PATH).count():,} lignes\")\n",
    "print(f\"  Silver_ML: {spark.read.format('delta').load(SILVER_ML_PATH).count():,} lignes\")\n",
    "\n",
    "print(\"\\nðŸ“Š Distribution flight_phase (Silver_ML) :\")\n",
    "spark.read.format(\"delta\").load(SILVER_ML_PATH).groupBy(\"flight_phase\").count().orderBy(\"count\", ascending=False).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
