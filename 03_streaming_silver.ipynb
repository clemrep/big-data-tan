{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Streaming Silver & Silver_ML\n",
    "\n",
    "Pipeline de transformation :\n",
    "- **Bronze ‚Üí Silver** : Nettoyage et enrichissement\n",
    "- **Bronze ‚Üí Silver_ML** : Feature engineering pour le Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:25:22.327108Z",
     "start_time": "2026-01-19T16:25:22.019412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration charg√©e depuis .env\n",
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.spark#spark-hadoop-cloud_2.12 added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5c42e1d1-0066-4ffb-af93-677331a368ec;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.spark#spark-hadoop-cloud_2.12;3.5.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound com.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.14 in central\n",
      "\tfound joda-time#joda-time;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.15.2 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.14 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.16 in central\n",
      "\tfound commons-codec#commons-codec;1.16.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.4 in central\n",
      "\tfound com.microsoft.azure#azure-storage;7.0.1 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.0.0 in central\n",
      "\tfound com.google.guava#guava;14.0.1 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-cloud-storage;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-aliyun;3.3.4 in central\n",
      "\tfound com.aliyun.oss#aliyun-sdk-oss;3.13.0 in central\n",
      "\tfound org.jdom#jdom2;2.0.6 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound stax#stax-api;1.0.1 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-core;4.5.10 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.ini4j#ini4j;0.5.4 in central\n",
      "\tfound io.opentracing#opentracing-api;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-util;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-noop;0.33.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-ram;3.1.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-kms;2.11.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure-datalake;3.3.4 in central\n",
      "\tfound com.microsoft.azure#azure-data-lake-store-sdk;2.3.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-openstack;3.3.4 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.54.v20240208 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.54.v20240208 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 702ms :: artifacts dl 19ms\n",
      "\t:: modules in use:\n",
      "\tcom.aliyun#aliyun-java-sdk-core;4.5.10 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-kms;2.11.0 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-ram;3.1.0 from central in [default]\n",
      "\tcom.aliyun.oss#aliyun-sdk-oss;3.13.0 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.15.2 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.14 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.guava#guava;14.0.1 from central in [default]\n",
      "\tcom.microsoft.azure#azure-data-lake-store-sdk;2.3.9 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.16.1 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\tio.opentracing#opentracing-api;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-noop;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-util;0.33.0 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tjoda-time#joda-time;2.12.5 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aliyun;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure-datalake;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-cloud-storage;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-openstack;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.14 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.16 from central in [default]\n",
      "\torg.apache.spark#spark-hadoop-cloud_2.12;3.5.3 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.54.v20240208 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.54.v20240208 from central in [default]\n",
      "\torg.ini4j#ini4j;0.5.4 from central in [default]\n",
      "\torg.jdom#jdom2;2.0.6 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\tstax#stax-api;1.0.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 by [org.eclipse.jetty#jetty-util-ajax;9.4.54.v20240208] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   50  |   0   |   0   |   1   ||   49  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5c42e1d1-0066-4ffb-af93-677331a368ec\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 49 already retrieved (0kB/7ms)\n",
      "26/01/23 13:39:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/23 13:39:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Session 'StreamingSilver' configur√©e\n",
      "‚úÖ Input:     s3a://datalake/bronze/flights\n",
      "‚úÖ Silver:    s3a://datalake/silver/flights\n",
      "‚úÖ Silver_ML: s3a://datalake/silver/flights_ml\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, from_unixtime, to_timestamp, round,\n",
    "    lag, avg, stddev, row_number, when, sqrt, pow, lit, min as spark_min, broadcast\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from config import get_s3_path, create_spark_session\n",
    "\n",
    "BRONZE_PATH = get_s3_path(\"bronze\", \"flights\")\n",
    "SILVER_PATH = get_s3_path(\"silver\", \"flights\")\n",
    "SILVER_ML_PATH = get_s3_path(\"silver\", \"flights_ml\")\n",
    "CHECKPOINT_SILVER = get_s3_path(\"checkpoints\", \"silver_flights\")\n",
    "CHECKPOINT_SILVER_ML = get_s3_path(\"checkpoints\", \"silver_ml_flights\")\n",
    "AIRPORTS_CSV = \"./data/airports.csv\"\n",
    "\n",
    "spark = create_spark_session(\"StreamingSilver\")\n",
    "\n",
    "print(f\"‚úÖ Input:     {BRONZE_PATH}\")\n",
    "print(f\"‚úÖ Silver:    {SILVER_PATH}\")\n",
    "print(f\"‚úÖ Silver_ML: {SILVER_ML_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des donn√©es a√©roports (pour Silver_ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:26:09.818721Z",
     "start_time": "2026-01-19T16:25:58.766040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 5211 a√©roports charg√©s\n"
     ]
    }
   ],
   "source": [
    "df_airports = spark.read.option(\"header\", \"true\").csv(AIRPORTS_CSV).select(\n",
    "    col(\"ident\").alias(\"airport_icao\"),\n",
    "    col(\"name\").alias(\"airport_name\"),\n",
    "    col(\"iso_country\").alias(\"airport_country\"),\n",
    "    col(\"latitude_deg\").cast(\"double\").alias(\"airport_lat\"),\n",
    "    col(\"longitude_deg\").cast(\"double\").alias(\"airport_lon\")\n",
    ").filter(col(\"type\").isin(\"large_airport\", \"medium_airport\"))\n",
    "\n",
    "print(f\"‚úÖ {df_airports.count()} a√©roports charg√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream 1 : Bronze ‚Üí Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:26:10.624729Z",
     "start_time": "2026-01-19T16:26:09.823686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Stream 1: Bronze ‚Üí Silver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 14:08:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49311:=========================================>           (39 + 8) / 50]\r"
     ]
    }
   ],
   "source": [
    "df_bronze_stream = spark.readStream.format(\"delta\").load(BRONZE_PATH)\n",
    "\n",
    "df_silver = df_bronze_stream \\\n",
    "    .filter(col(\"icao24\").isNotNull()) \\\n",
    "    .filter(col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull()) \\\n",
    "    .withColumn(\"event_timestamp\", to_timestamp(from_unixtime(col(\"time\")))) \\\n",
    "    .withColumn(\"velocity_kmh\", round(col(\"velocity\") * 3.6, 2)) \\\n",
    "    .withColumn(\"altitude_meters\", col(\"baro_altitude\")) \\\n",
    "    .select(\n",
    "        \"event_timestamp\", \"icao24\", \"callsign\", \"origin_country\",\n",
    "        \"longitude\", \"latitude\", \"velocity_kmh\", \"altitude_meters\",\n",
    "        \"on_ground\", \"category\"\n",
    "    )\n",
    "\n",
    "print(f\"üöÄ Stream 1: Bronze ‚Üí Silver\")\n",
    "\n",
    "query_silver = df_silver.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_SILVER) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(SILVER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream 2 : Bronze ‚Üí Silver_ML (Feature Engineering)\n",
    "\n",
    "Transformation avec features pour le ML, directement depuis Bronze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:24:55.935373Z",
     "start_time": "2026-01-19T16:24:55.818025Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_ml_batch(batch_df, batch_id):\n",
    "    \"\"\"Traitement d'un micro-batch pour Silver_ML avec feature engineering.\"\"\"\n",
    "    \n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "    \n",
    "    # Transformation Bronze ‚Üí format Silver\n",
    "    df_base = batch_df \\\n",
    "        .filter(col(\"icao24\").isNotNull()) \\\n",
    "        .filter(col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull()) \\\n",
    "        .withColumn(\"event_timestamp\", to_timestamp(from_unixtime(col(\"time\")))) \\\n",
    "        .withColumn(\"velocity_kmh\", round(col(\"velocity\") * 3.6, 2)) \\\n",
    "        .withColumn(\"altitude_meters\", col(\"baro_altitude\"))\n",
    "    \n",
    "    # Nettoyage ML\n",
    "    df_clean = df_base \\\n",
    "        .filter(col(\"altitude_meters\").between(-500, 15000)) \\\n",
    "        .filter(col(\"velocity_kmh\").between(0, 1200))\n",
    "    \n",
    "    if df_clean.isEmpty():\n",
    "        return\n",
    "    \n",
    "    # Features temporelles\n",
    "    window_aircraft = Window.partitionBy(\"icao24\").orderBy(\"event_timestamp\")\n",
    "    \n",
    "    df_temporal = df_clean \\\n",
    "        .withColumn(\"prev_altitude\", lag(\"altitude_meters\", 1).over(window_aircraft)) \\\n",
    "        .withColumn(\"prev_velocity\", lag(\"velocity_kmh\", 1).over(window_aircraft)) \\\n",
    "        .withColumn(\"altitude_change\", col(\"altitude_meters\") - col(\"prev_altitude\")) \\\n",
    "        .withColumn(\"velocity_change\", col(\"velocity_kmh\") - col(\"prev_velocity\")) \\\n",
    "        .withColumn(\"observation_rank\", row_number().over(window_aircraft))\n",
    "    \n",
    "    # Jointure a√©roports\n",
    "    df_on_ground = df_temporal.filter(col(\"on_ground\") == True)\n",
    "    df_in_flight = df_temporal.filter(col(\"on_ground\") == False)\n",
    "    \n",
    "    if df_on_ground.count() > 0:\n",
    "        df_with_airports = df_on_ground.crossJoin(broadcast(df_airports)).withColumn(\n",
    "            \"dist\", sqrt(pow(col(\"latitude\") - col(\"airport_lat\"), 2) + pow(col(\"longitude\") - col(\"airport_lon\"), 2))\n",
    "        )\n",
    "        \n",
    "        w = Window.partitionBy(\"icao24\", \"event_timestamp\")\n",
    "        df_closest = df_with_airports.withColumn(\"min_dist\", spark_min(\"dist\").over(w)) \\\n",
    "            .filter(col(\"dist\") == col(\"min_dist\")) \\\n",
    "            .drop(\"dist\", \"min_dist\", \"airport_lat\", \"airport_lon\")\n",
    "        \n",
    "        df_enriched = df_closest.unionByName(\n",
    "            df_in_flight.withColumn(\"airport_icao\", lit(None))\n",
    "                        .withColumn(\"airport_name\", lit(None))\n",
    "                        .withColumn(\"airport_country\", lit(None)),\n",
    "            allowMissingColumns=True\n",
    "        )\n",
    "    else:\n",
    "        df_enriched = df_in_flight \\\n",
    "            .withColumn(\"airport_icao\", lit(None)) \\\n",
    "            .withColumn(\"airport_name\", lit(None)) \\\n",
    "            .withColumn(\"airport_country\", lit(None))\n",
    "    \n",
    "    # Features rolling window\n",
    "    rolling_window = Window.partitionBy(\"icao24\").orderBy(\"event_timestamp\").rowsBetween(-5, 0)\n",
    "    \n",
    "    df_rolling = df_enriched \\\n",
    "        .withColumn(\"rolling_avg_altitude\", avg(\"altitude_meters\").over(rolling_window)) \\\n",
    "        .withColumn(\"rolling_std_altitude\", stddev(\"altitude_meters\").over(rolling_window)) \\\n",
    "        .withColumn(\"rolling_avg_velocity\", avg(\"velocity_kmh\").over(rolling_window))\n",
    "    \n",
    "    # Label flight_phase\n",
    "    df_ml = df_rolling.withColumn(\n",
    "        \"flight_phase\",\n",
    "        when(col(\"on_ground\") == True, \"GROUND\")\n",
    "        .when((col(\"altitude_change\") > 50) & (col(\"altitude_meters\") < 3000), \"TAKEOFF\")\n",
    "        .when(col(\"altitude_change\") > 20, \"CLIMB\")\n",
    "        .when(col(\"altitude_change\").between(-20, 20) & (col(\"altitude_meters\") > 8000), \"CRUISE\")\n",
    "        .when(col(\"altitude_change\") < -20, \"DESCENT\")\n",
    "        .otherwise(\"TRANSITION\")\n",
    "    )\n",
    "    \n",
    "    # S√©lection des colonnes finales\n",
    "    df_final = df_ml.select(\n",
    "        \"event_timestamp\", \"icao24\", \"callsign\", \"origin_country\",\n",
    "        \"longitude\", \"latitude\", \"velocity_kmh\", \"altitude_meters\",\n",
    "        \"on_ground\", \"category\",\n",
    "        \"prev_altitude\", \"prev_velocity\", \"altitude_change\", \"velocity_change\", \"observation_rank\",\n",
    "        \"airport_icao\", \"airport_name\", \"airport_country\",\n",
    "        \"rolling_avg_altitude\", \"rolling_std_altitude\", \"rolling_avg_velocity\",\n",
    "        \"flight_phase\"\n",
    "    )\n",
    "    \n",
    "    # √âcriture\n",
    "    df_final.write.format(\"delta\").mode(\"append\").save(SILVER_ML_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:25:12.757910Z",
     "start_time": "2026-01-19T16:25:08.640557Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Stream 2: Bronze ‚Üí Silver_ML (Feature Engineering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 14:08:22 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49314:==============================>                      (25 + 8) / 44]\r"
     ]
    }
   ],
   "source": [
    "df_bronze_ml_stream = spark.readStream.format(\"delta\").load(BRONZE_PATH)\n",
    "\n",
    "print(f\"üöÄ Stream 2: Bronze ‚Üí Silver_ML (Feature Engineering)\")\n",
    "\n",
    "query_silver_ml = df_bronze_ml_stream.writeStream \\\n",
    "    .foreachBatch(process_ml_batch) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_SILVER_ML) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring des streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49314:===========> (40 + 4) / 44][Stage 49315:>             (0 + 4) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Monitoring des streams (Ctrl+C pour arr√™ter)\n",
      "============================================================\n",
      "\n",
      "‚è±Ô∏è  14:08:25\n",
      "  Silver:    {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Silver_ML: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è  14:08:55\n",
      "  Silver:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/bronze/flights]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Silver_ML: {'message': 'Getting offsets from DeltaSource[s3a://datalake/bronze/flights]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "\n",
      "‚è±Ô∏è  14:09:25\n",
      "  Silver:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/bronze/flights]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Silver_ML: {'message': 'Getting offsets from DeltaSource[s3a://datalake/bronze/flights]', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è  14:09:55\n",
      "  Silver:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/bronze/flights]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Silver_ML: {'message': 'Getting offsets from DeltaSource[s3a://datalake/bronze/flights]', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è  14:10:25\n",
      "  Silver:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/bronze/flights]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Silver_ML: {'message': 'Getting offsets from DeltaSource[s3a://datalake/bronze/flights]', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è  14:10:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56930:>              (0 + 2) / 2][Stage 56933:>              (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Silver:    {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
      "  Silver_ML: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 14:11:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 56955:=========================================>           (39 + 8) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è  14:11:27\n",
      "  Silver:    {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
      "  Silver_ML: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 14:11:39 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è  14:11:57\n",
      "  Silver:    {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
      "  Silver_ML: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 14:12:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 56987:===========> (44 + 6) / 50][Stage 56990:>             (0 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è  14:12:27\n",
      "  Silver:    {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
      "  Silver_ML: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 14:12:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 57007:=====================================>               (35 + 8) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è  14:12:57\n",
      "  Silver:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/bronze/flights]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Silver_ML: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57024:==============>                                      (14 + 8) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è  14:13:27\n",
      "  Silver:    {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
      "  Silver_ML: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 14:13:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è  14:13:58\n",
      "  Silver:    {'message': 'Getting offsets from DeltaSource[s3a://datalake/bronze/flights]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "  Silver_ML: {'message': 'Getting offsets from DeltaSource[s3a://datalake/bronze/flights]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "\n",
      "‚è±Ô∏è  14:14:28\n",
      "  Silver:    {'message': 'Waiting for data to arrive', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "  Silver_ML: {'message': 'Getting offsets from DeltaSource[s3a://datalake/bronze/flights]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "\n",
      "‚è±Ô∏è  14:14:58\n",
      "  Silver:    {'message': 'Waiting for data to arrive', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "  Silver_ML: {'message': 'Getting offsets from DeltaSource[s3a://datalake/bronze/flights]', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"üìä Monitoring des streams (Ctrl+C pour arr√™ter)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        print(f\"\\n‚è±Ô∏è  {time.strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Silver:    {query_silver.status}\")\n",
    "        print(f\"  Silver_ML: {query_silver_ml.status}\")\n",
    "        time.sleep(30)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚èπÔ∏è  Arr√™t demand√©...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arr√™t des streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tous les streams arr√™t√©s\n"
     ]
    }
   ],
   "source": [
    "query_silver.stop()\n",
    "query_silver_ml.stop()\n",
    "print(\"‚úÖ Tous les streams arr√™t√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V√©rification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Statistiques :\n",
      "  Bronze:    98,840 lignes\n",
      "  Silver:    97,950 lignes\n",
      "  Silver_ML: 88,432 lignes\n",
      "\n",
      "üìä Distribution flight_phase (Silver_ML) :\n",
      "+------------+-----+\n",
      "|flight_phase|count|\n",
      "+------------+-----+\n",
      "|      CRUISE|37190|\n",
      "|  TRANSITION|29374|\n",
      "|     DESCENT|10699|\n",
      "|       CLIMB| 8877|\n",
      "|     TAKEOFF| 2035|\n",
      "|      GROUND|  257|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Statistiques :\")\n",
    "print(f\"  Bronze:    {spark.read.format('delta').load(BRONZE_PATH).count():,} lignes\")\n",
    "print(f\"  Silver:    {spark.read.format('delta').load(SILVER_PATH).count():,} lignes\")\n",
    "print(f\"  Silver_ML: {spark.read.format('delta').load(SILVER_ML_PATH).count():,} lignes\")\n",
    "\n",
    "print(\"\\nüìä Distribution flight_phase (Silver_ML) :\")\n",
    "spark.read.format(\"delta\").load(SILVER_ML_PATH).groupBy(\"flight_phase\").count().orderBy(\"count\", ascending=False).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
