{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Streaming Silver & Silver_ML\n",
    "\n",
    "Pipeline de transformation :\n",
    "- **Bronze â†’ Silver** : Nettoyage et enrichissement\n",
    "- **Bronze â†’ Silver_ML** : Feature engineering pour le Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:25:22.327108Z",
     "start_time": "2026-01-19T16:25:22.019412Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, from_unixtime, to_timestamp, round,\n",
    "    lag, avg, stddev, row_number, when, sqrt, pow, lit, min as spark_min, broadcast\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from config import get_s3_path, create_spark_session\n",
    "\n",
    "BRONZE_PATH = get_s3_path(\"bronze\", \"flights\")\n",
    "SILVER_PATH = get_s3_path(\"silver\", \"flights\")\n",
    "SILVER_ML_PATH = get_s3_path(\"silver\", \"flights_ml\")\n",
    "CHECKPOINT_SILVER = get_s3_path(\"checkpoints\", \"silver_flights\")\n",
    "CHECKPOINT_SILVER_ML = get_s3_path(\"checkpoints\", \"silver_ml_flights\")\n",
    "AIRPORTS_CSV = \"./data/airports.csv\"\n",
    "\n",
    "spark = create_spark_session(\"StreamingSilver\")\n",
    "\n",
    "print(f\"âœ… Input:     {BRONZE_PATH}\")\n",
    "print(f\"âœ… Silver:    {SILVER_PATH}\")\n",
    "print(f\"âœ… Silver_ML: {SILVER_ML_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des donnÃ©es aÃ©roports (pour Silver_ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:26:09.818721Z",
     "start_time": "2026-01-19T16:25:58.766040Z"
    }
   },
   "outputs": [],
   "source": [
    "df_airports = spark.read.option(\"header\", \"true\").csv(AIRPORTS_CSV).select(\n",
    "    col(\"ident\").alias(\"airport_icao\"),\n",
    "    col(\"name\").alias(\"airport_name\"),\n",
    "    col(\"iso_country\").alias(\"airport_country\"),\n",
    "    col(\"latitude_deg\").cast(\"double\").alias(\"airport_lat\"),\n",
    "    col(\"longitude_deg\").cast(\"double\").alias(\"airport_lon\")\n",
    ").filter(col(\"type\").isin(\"large_airport\", \"medium_airport\"))\n",
    "\n",
    "print(f\"âœ… {df_airports.count()} aÃ©roports chargÃ©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream 1 : Bronze â†’ Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:26:10.624729Z",
     "start_time": "2026-01-19T16:26:09.823686Z"
    }
   },
   "outputs": [],
   "source": [
    "df_bronze_stream = spark.readStream.format(\"delta\").load(BRONZE_PATH)\n",
    "\n",
    "df_silver = df_bronze_stream \\\n",
    "    .filter(col(\"icao24\").isNotNull()) \\\n",
    "    .filter(col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull()) \\\n",
    "    .withColumn(\"event_timestamp\", to_timestamp(from_unixtime(col(\"time\")))) \\\n",
    "    .withColumn(\"velocity_kmh\", round(col(\"velocity\") * 3.6, 2)) \\\n",
    "    .withColumn(\"altitude_meters\", col(\"baro_altitude\")) \\\n",
    "    .select(\n",
    "        \"event_timestamp\", \"icao24\", \"callsign\", \"origin_country\",\n",
    "        \"longitude\", \"latitude\", \"velocity_kmh\", \"altitude_meters\",\n",
    "        \"on_ground\", \"category\"\n",
    "    )\n",
    "\n",
    "print(f\"ðŸš€ Stream 1: Bronze â†’ Silver\")\n",
    "\n",
    "query_silver = df_silver.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_SILVER) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(SILVER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream 2 : Bronze â†’ Silver_ML (Feature Engineering)\n",
    "\n",
    "Transformation avec features pour le ML, directement depuis Bronze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:24:55.935373Z",
     "start_time": "2026-01-19T16:24:55.818025Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/20 15:48:14 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "26/01/20 15:48:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/20 15:48:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "26/01/20 15:48:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "26/01/20 15:48:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "26/01/20 15:48:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "26/01/20 15:48:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "26/01/20 15:48:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "26/01/20 15:48:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "26/01/20 15:48:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "26/01/20 15:48:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "26/01/20 15:48:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "26/01/20 15:48:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "26/01/20 15:48:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "26/01/20 15:48:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "26/01/20 15:48:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/20 15:48:54 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/20 15:49:35 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/20 15:50:14 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/20 15:51:02 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/20 15:51:45 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/20 15:52:19 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/20 15:53:01 ERROR NonFateSharingFuture: Failed to get result from future  \n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/20 15:53:52 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/20 15:54:32 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/20 15:55:12 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/20 15:55:46 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n",
      "26/01/20 15:56:20 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n"
     ]
    }
   ],
   "source": [
    "def process_ml_batch(batch_df, batch_id):\n",
    "    \"\"\"Traitement d'un micro-batch pour Silver_ML avec feature engineering.\"\"\"\n",
    "    \n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "    \n",
    "    # Transformation Bronze â†’ format Silver\n",
    "    df_base = batch_df \\\n",
    "        .filter(col(\"icao24\").isNotNull()) \\\n",
    "        .filter(col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull()) \\\n",
    "        .withColumn(\"event_timestamp\", to_timestamp(from_unixtime(col(\"time\")))) \\\n",
    "        .withColumn(\"velocity_kmh\", round(col(\"velocity\") * 3.6, 2)) \\\n",
    "        .withColumn(\"altitude_meters\", col(\"baro_altitude\"))\n",
    "    \n",
    "    # Nettoyage ML\n",
    "    df_clean = df_base \\\n",
    "        .filter(col(\"altitude_meters\").between(-500, 15000)) \\\n",
    "        .filter(col(\"velocity_kmh\").between(0, 1200))\n",
    "    \n",
    "    if df_clean.isEmpty():\n",
    "        return\n",
    "    \n",
    "    # Features temporelles\n",
    "    window_aircraft = Window.partitionBy(\"icao24\").orderBy(\"event_timestamp\")\n",
    "    \n",
    "    df_temporal = df_clean \\\n",
    "        .withColumn(\"prev_altitude\", lag(\"altitude_meters\", 1).over(window_aircraft)) \\\n",
    "        .withColumn(\"prev_velocity\", lag(\"velocity_kmh\", 1).over(window_aircraft)) \\\n",
    "        .withColumn(\"altitude_change\", col(\"altitude_meters\") - col(\"prev_altitude\")) \\\n",
    "        .withColumn(\"velocity_change\", col(\"velocity_kmh\") - col(\"prev_velocity\")) \\\n",
    "        .withColumn(\"observation_rank\", row_number().over(window_aircraft))\n",
    "    \n",
    "    # Jointure aÃ©roports\n",
    "    df_on_ground = df_temporal.filter(col(\"on_ground\") == True)\n",
    "    df_in_flight = df_temporal.filter(col(\"on_ground\") == False)\n",
    "    \n",
    "    if df_on_ground.count() > 0:\n",
    "        df_with_airports = df_on_ground.crossJoin(broadcast(df_airports)).withColumn(\n",
    "            \"dist\", sqrt(pow(col(\"latitude\") - col(\"airport_lat\"), 2) + pow(col(\"longitude\") - col(\"airport_lon\"), 2))\n",
    "        )\n",
    "        \n",
    "        w = Window.partitionBy(\"icao24\", \"event_timestamp\")\n",
    "        df_closest = df_with_airports.withColumn(\"min_dist\", spark_min(\"dist\").over(w)) \\\n",
    "            .filter(col(\"dist\") == col(\"min_dist\")) \\\n",
    "            .drop(\"dist\", \"min_dist\", \"airport_lat\", \"airport_lon\")\n",
    "        \n",
    "        df_enriched = df_closest.unionByName(\n",
    "            df_in_flight.withColumn(\"airport_icao\", lit(None))\n",
    "                        .withColumn(\"airport_name\", lit(None))\n",
    "                        .withColumn(\"airport_country\", lit(None)),\n",
    "            allowMissingColumns=True\n",
    "        )\n",
    "    else:\n",
    "        df_enriched = df_in_flight \\\n",
    "            .withColumn(\"airport_icao\", lit(None)) \\\n",
    "            .withColumn(\"airport_name\", lit(None)) \\\n",
    "            .withColumn(\"airport_country\", lit(None))\n",
    "    \n",
    "    # Features rolling window\n",
    "    rolling_window = Window.partitionBy(\"icao24\").orderBy(\"event_timestamp\").rowsBetween(-5, 0)\n",
    "    \n",
    "    df_rolling = df_enriched \\\n",
    "        .withColumn(\"rolling_avg_altitude\", avg(\"altitude_meters\").over(rolling_window)) \\\n",
    "        .withColumn(\"rolling_std_altitude\", stddev(\"altitude_meters\").over(rolling_window)) \\\n",
    "        .withColumn(\"rolling_avg_velocity\", avg(\"velocity_kmh\").over(rolling_window))\n",
    "    \n",
    "    # Label flight_phase\n",
    "    df_ml = df_rolling.withColumn(\n",
    "        \"flight_phase\",\n",
    "        when(col(\"on_ground\") == True, \"GROUND\")\n",
    "        .when((col(\"altitude_change\") > 50) & (col(\"altitude_meters\") < 3000), \"TAKEOFF\")\n",
    "        .when(col(\"altitude_change\") > 20, \"CLIMB\")\n",
    "        .when(col(\"altitude_change\").between(-20, 20) & (col(\"altitude_meters\") > 8000), \"CRUISE\")\n",
    "        .when(col(\"altitude_change\") < -20, \"DESCENT\")\n",
    "        .otherwise(\"TRANSITION\")\n",
    "    )\n",
    "    \n",
    "    # SÃ©lection des colonnes finales\n",
    "    df_final = df_ml.select(\n",
    "        \"event_timestamp\", \"icao24\", \"callsign\", \"origin_country\",\n",
    "        \"longitude\", \"latitude\", \"velocity_kmh\", \"altitude_meters\",\n",
    "        \"on_ground\", \"category\",\n",
    "        \"prev_altitude\", \"prev_velocity\", \"altitude_change\", \"velocity_change\", \"observation_rank\",\n",
    "        \"airport_icao\", \"airport_name\", \"airport_country\",\n",
    "        \"rolling_avg_altitude\", \"rolling_std_altitude\", \"rolling_avg_velocity\",\n",
    "        \"flight_phase\"\n",
    "    )\n",
    "    \n",
    "    # Ã‰criture\n",
    "    df_final.write.format(\"delta\").mode(\"append\").save(SILVER_ML_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:25:12.757910Z",
     "start_time": "2026-01-19T16:25:08.640557Z"
    }
   },
   "outputs": [],
   "source": [
    "df_bronze_ml_stream = spark.readStream.format(\"delta\").load(BRONZE_PATH)\n",
    "\n",
    "print(f\"ðŸš€ Stream 2: Bronze â†’ Silver_ML (Feature Engineering)\")\n",
    "\n",
    "query_silver_ml = df_bronze_ml_stream.writeStream \\\n",
    "    .foreachBatch(process_ml_batch) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_SILVER_ML) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring des streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"ðŸ“Š Monitoring des streams (Ctrl+C pour arrÃªter)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        print(f\"\\nâ±ï¸  {time.strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Silver:    {query_silver.status}\")\n",
    "        print(f\"  Silver_ML: {query_silver_ml.status}\")\n",
    "        time.sleep(30)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâ¹ï¸  ArrÃªt demandÃ©...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArrÃªt des streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_silver.stop()\n",
    "query_silver_ml.stop()\n",
    "print(\"âœ… Tous les streams arrÃªtÃ©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VÃ©rification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š Statistiques :\")\n",
    "print(f\"  Bronze:    {spark.read.format('delta').load(BRONZE_PATH).count():,} lignes\")\n",
    "print(f\"  Silver:    {spark.read.format('delta').load(SILVER_PATH).count():,} lignes\")\n",
    "print(f\"  Silver_ML: {spark.read.format('delta').load(SILVER_ML_PATH).count():,} lignes\")\n",
    "\n",
    "print(\"\\nðŸ“Š Distribution flight_phase (Silver_ML) :\")\n",
    "spark.read.format(\"delta\").load(SILVER_ML_PATH).groupBy(\"flight_phase\").count().orderBy(\"count\", ascending=False).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
